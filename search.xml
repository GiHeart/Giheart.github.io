<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Flask上传文件]]></title>
    <url>%2F2018%2F11%2F22%2FFlask%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[介绍上传文件在web中经常用到，本文就介绍在Flask中怎么上传文件！ 方法新建一个Flask项目 在app.py中写路由： 1234567891011121314@app.route('/upload', methods=['GET', 'POST'])def upload(): if request.method == 'GET': return render_template('upload.html') elif request.method == 'POST': file = request.files['file'] print(file.filename) base_path = os.path.dirname(__file__) save_path = os.path.join(base_path, 'static/files/', secure_filename(file.filename)) print(save_path) file.save(save_path) return '上传完成' else: pass 在templates下新建upload.html写一个上传表单 1234&lt;form action="&#123;&#123; url_for('upload') &#125;&#125;" method="post" enctype="multipart/form-data"&gt; &lt;input type="file" name="file"&gt; &lt;input type="submit" value="上传"&gt;&lt;/form&gt; 代码解释app.py:在app.py中写了一个路由用来处理upload的请求，在路由中指明了接收那些请求类型：GET和POST。下面使用了if、else来处理这两个类型的请求，GET请求返回upload.html。post请求就处理上传的文件（保存文件）。 这个POST请求，有很多需要注意的地方。 首先使用了request对象获取用户上传的文件。并赋值给变量file。 然后使用os包来构造一个文件保存路径,base_path使用了os.path.dirname来获取当前文件所在的路径，后面的参数123456789101112131415161718os.path.join的第三个参数本来可以直接使用file.name，但是因为文件名可能会包含特殊字符，所以使用werzeug.uril中的secure_name方法来对文件特殊字符进行转义。最后使用file.save()方法保存用户上传的文件到刚才构造的文件路径中。并返回一个上传完成的信息给用户。## upload.html这个页面就是一个普通的上传文件表单需要注意的就一点，注意需要在表单中声明上传地址、提交请求类型和enctype=”multipart/form-data”。enctype=”multipart/form-data”，上传文件时必须声明，指明了这个表单submit时会上传二进制文件。# 遇到的错误我在学习这个示例的时候，也遇到了很多错误。这些错误大部分都是因为粗心大意，没有深入理解它的原理造成的。下面记录几个容易出错的地方```base_path = os.path.dirname(__file__) 这行代码是为了获取当前文件所在的路径，后面的__file__表示当前这个app.py文件，然后去掉这个文件，就获得了一个基础路径 save_path = os.path.join(base_path, &#39;static/files/&#39;, secure_filename(file.filename)) 这行代码拼装了一个路径来保存文件，需要注意secure_filename(file.filename)使用secure_filename来对文件名转义。 file.filename本质上是request.files[file].filename使用了请求上下文request对象来获取用户上传的文件名。]]></content>
      <categories>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask入门]]></title>
    <url>%2F2018%2F11%2F21%2FFlask%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[介绍Flask是一个使用python编写的轻量级web应用框架。其http部分来自Werkzeug,模板引擎使用jinja2。flask使用简单的核心，用扩展来正价其它功能。flask没有默认使用的数据库、表单工具。 众多的扩展提供了数据库集成、表单验证、上传处理、开放认证技术等功能。flask也许是微小的，但少即是多，它已准备好在需求繁杂的生产环境中投入使用。 其它框架： Django：大而全，最出名的python框架，最出名的是它全自动化的管理后台：只需要使用ORM，做简单的对象定义，他就能自动生成数据库结构、以及全功能的管理后台 web2py：是一个为python语言提供的全功能web应用框架，旨在敏捷快速的开发web应用，具有快速安全以及可一直的数据库驱动的应用。 Tornado：特点是异步web框架，适合做长连接，他不仅是web框架，还是一个web server，同时提供了异步库。 Sanic：他是一个类似flask的异步web框架，基于Python3.5的 async/await 原生异步语法实现的。根据官方文档所说，性能非常高 选择： 综合考虑，我们应该学习的是flask和django框架。一个小而微，一个大而全。但它们两者原理是非常相似的。flask足够流行，通过添加扩展能做复杂的项目，因为其核心简单，适合web学习入门和进阶。掌握了flask在学习其他框架也不是问题。 环境准备1pip install flask 安装flask包 虽然只install flask，但还出现了其它几个包。它们是flask的依赖包，flask的一些功能基于它们。它们的大概作用是： Flask：flask的核心组成 Jinja2：前端模板渲染 MarkupSafe、itsdangerous：安全相关，防CSRF攻击等 pycryptodome: 加密 Werzeug：http封装 flask项目Hello World1234567891011from flask import Flaskapp = Flask(__name__)@app.route('/')def hello() return 'hello world'if __name__ == '__main__' app.run() 这就是最简单的hello world 第一行引入了Flask，第二行实例化Flask并赋值给app，接下来使用装饰器写了一个路由，这个路由表明了处理那些url，路由以’/‘开头。 下面定义了一个视图函数，用来处理URL这个视图函数就是返回hello world。 模板渲染当视图函数需要返回很多内容时，比如返回一个完整的网页，如果再在返回值里面写的话，会很麻烦，这时候就需要引入模板渲染了 123456789from flask import Flask, render_templatesapp = Flask(__name__)@app.route('/')def index(): return render_templates('index.html')if __name__ == '__main__' app.run() render_templates就是引入模板函数，参数是html文件名，html文件放在flask项目目录下的templates文件夹下 这是一个完整的flask项目，static文件夹下放css、js等文件，templates文件夹下放html模板。 传入变量修改上面的代码在函数return后面添加一个变量： 1return render_templates('index.html',user=小花) 在index.html文件中 123&lt;body&gt; &lt;h1&gt;你好：&#123;&#123; user &#125;&#125;&lt;/h1&gt;&lt;/body&gt; 这样前端html就可以使用后端的变量了。运行后会在浏览器上看到：你好：小花 写路由flask提供了url_for函数构造地址，首先在app.py中引入url_for 12345678910111213141516from flask import Flask, url_for, render_template, requestapp = Flask(__name__)@app.route('/')# @app.route('/test') # 多个url指向一个路由def index(): return render_template('index.html')@app.route('/test/&lt;no&gt;')@app.route('/test/', defaults=&#123;'no': '小花'&#125;) # 参数默认值def test1(no): print(no) return f'&lt;h1&gt;HELLO,&#123;no&#125;&lt;h1&gt;' 这里多写了一个路由，在前端页面index 1&lt;h1&gt;&lt;a href="&#123;&#123; 'test' &#125;&#125;"&gt;学习&lt;/a&gt;&lt;/h1&gt; 这里的a标签链接指向了路由test，这时，这个完整的链接是http://127.0.0.1:5000/test(假如flask只监听本地，运行在5000端口)，视图函数test会处理这个请求，返回hello小花，这里是一个变量，是一个可变的url，当输入/test/xxx时就会返回hello,xxx。在视图函数中给了它一个默认值小花，当输入/test/会默认返回hello，小花。 引入css等文件html页面当然少不了css，那么怎么引入css呢？ 12345&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;学习&lt;/title&gt; &lt;link rel="stylesheet" href="&#123;&#123; url_for('static', filename='test.css') &#125;&#125;"&gt;&lt;/head&gt; 方法是在头部引入css这和我们平时引入css不太一样，这里使用了url_for函数来引入，第一个参数是文件夹名字，第二个参数是css文件键值对形式。这样写，当收到请求的时候，他就会在static文件夹中找到css文件。 总结url_forurl_for函数， 构造url:endpoint端点参数，填写方法名。注意参数对应的是函数名，跟路由的url没关系。 当ip、port发生变化时，不用前端页面 引用css js之类的静态资源。flask框架会对url进行预处理，前端html页面引用资源时不能写成相对路径。前端url_for()返回结果/static/index,css,flask框架内置相关路由。 url_for引用方法时候，参数写方法名，引用css js之类的资源时有两个参数，第一个参数是文件夹名，第二个是filename=文件名 eg. url_for(‘static’, filename=test) 请求上下文、POST请求Flask从客户端收到请求时，要让视图函数能访问一些对象，这样才能处理请求。请求对象就是一个很好的例子，它封装了客户端发送的HTTP请求。要想让视图函数能够访问请求对象，一种直截了当的方式是将其作为参数传入视图函数，不过这会导致应用中的每个视图函数都多出一个参数。除了访问请求对象，如果视图函数在处理请求时还要访问其他对象，情况会变得更糟。为了避免大量可有可无的参数把视图函数弄得一团糟，Flask使用上下文临时把某些对象变为全局可访问。有了上下文，便可以像下面这样编写视图函数： 12345from flask import request@app.route('/')def index():user_agent = request.headers.get('User-Agent')return '&lt;p&gt;Your browser is &#123;&#125;&lt;/p&gt;'.format(user_agent) 注意，在这个视图函数中我们把request当作全局变量使用。事实上，request不可能是全局变量。试想，在多线程服务器中，多个线程同时处理不同客户端发送的不同请求时，每个线程看到的request对象必然不同。Flask使用上下文让特定的变量在一个线程中全局可访问，与此同时却不会干扰其他线程。 在Flask中有两种上下文：应用上下文和请求上下文: 应用上下文：current_app g 请求上下文：request session 这4个变量很有用，今天先研究request这个对象。 request是一个请求对象，它封装了客户端发出的HTTP请求中的内容： 有了这些属性和方法，我们就能做很多事情了。 eg. 写一个路由处理表单和POST请求 app.py 12345678910111213141516171819@app.route('/post', methods=['GET', 'POST'])def post(): if request.method == 'GET': return render_template('post.html') elif request.method == 'POST': args = request.args form = request.form username = form['user_name'] password = form['phone_number'] var = form['var'] print(username, password, var) print(type(form)) print(args) page = args['page'] print(page) # print(args['cat']) return "&lt;h1'&gt;你好，崽种&lt;/h1&gt;" else: pass post.html 1234567891011121314body&gt; &lt;div class="form1"&gt; &lt;form action="&#123;&#123; url_for('post',page=1,cag='sport') &#125;&#125;" method="post"&gt; &lt;p&gt;姓名:&lt;/p&gt; &lt;input type="text" name="user_name" placeholder="Input your name"&gt; &lt;p&gt;手机号:&lt;/p&gt; &lt;input type="text" name="phone_number" placeholder="Input your phone number"&gt; &lt;p&gt;验证码:&lt;/p&gt; &lt;input type="text" name="var" placeholder="Input validation code"&gt; &lt;input type="submit" value="提交"&gt; &lt;input type="reset" value="重置"&gt; &lt;/form&gt; &lt;/div&gt;&lt;/body&gt; 首先，定义了这个表单提交方式method是POST定义了action提交地址是post路由，并且在这里我设置了一个带参数的url。然后在路由中指明了接收什么样的请求，如果不指明接收POST请求，在表单提交时会出错。 然后，使用了request这个对象的method属性获取请求方式，如果是GET就返回post.html如果是POST请求就返回“你好，崽种”。 在这个POST请求中我使用了request.args来获取请求url中的参数，就是我在上面设置的带参数的url； 然后使用了request.form来获取提交的表单数据。 这里在后台就打印出了url的参数和我提交的数据。 flask的这些上下文对象非常有用，正式有了request这样的上下文对象，我们才能想上面那样写路由和视图函数。 今天仅仅是认识了这些对象，在接下来的学习中，我们还会更加深刻的理解这些对象并掌握它们的用法。 容易出错的地方带参数的url url的？号是参数，匹配的不是路由，在浏览器中输入url在？后面输入参数，参数是键值对的形式，在视图函数中使用request.args可以捕获到这些参数。这点容易迷惑 带参数的路由： 12345@app.route('/x/&lt;test&gt;')@app.route('/x/', defaults=&#123;'test': 'hello'&#125;)def test(test): print(test) return f'&#123;test&#125;:nihao' 这两者很容易迷惑，路由匹配的是url，上面那个匹配的是参数。 带参数的路由在视图函数中必须要接收这个参数。 上面的代码中路由匹配到了127.0.0.1/x/，然后后面的test是路由的参数，下面给了这个参数一个默认值，默认进入到x这个页面携带参数test=hello。 路由的参数和url的参数不一样，url的参数写在？后面，路由的参数些在/后面。 本文参考： 杨铮老师的博客 《Flask Web开发：基于Python的Web应用开发实践(第二版)》——人民邮电出版社]]></content>
      <categories>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
        <tag>WEB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python闭包和装饰器]]></title>
    <url>%2F2018%2F11%2F19%2Fpython%E9%97%AD%E5%8C%85%E5%92%8C%E8%A3%85%E9%A5%B0%E5%99%A8%2F</url>
    <content type="text"><![CDATA[闭包返回函数函数作为返回值 告诫函数除了可以接受函数作为参数外，还可以吧函数作为结果值返回。 定义一个可变参数的求和： 12345def calc_sum(*args): ax = 0 for n in args: ax = ax + n return ax 但是如果不需要立即求和，而是在后面的代码中，根据需要在计算怎么办？可以不返回求和的结果，而是返回求和的函数： 1234567def lazy_sum(*args): def sum(): ax = 0 for n in args: ax = ax +n return ax return sum 当我们调用lazy_sum时，返回的并不是求和结果，而是求和函数： 123&gt;&gt;&gt; f = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f&lt;function lazy_sum.&lt;locals&gt;.sum at 0x101c6ed90&gt; 调用函数f时，才真正计算求和的结果： 12&gt;&gt;&gt; f()25 在这个例子中，我们在lazy_sum中又定义了函数sum，并且，内部函数sum可以引用外部函数lazy_sum的参数和局部变量，当lazy_sum函数返回sum时，相关函数和变量都保存在函数中，这种称为‘’闭包“的程序结构拥有极大的为例。 请再注意一点，当我们调用lazy_sum()时，每次调用都会返回一个新的函数，即使传入相同的参数： 1234&gt;&gt;&gt; f1 = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f2 = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f1==f2False f1()和f2()的调用结果互不影响。 返回的函数在其定义内部引用了局部变量args，所以，当一个函数反悔了一个函数后，其内部的变量还能被新函数引用，所以，闭包用起来简单，实现起来可不容易。 另一个需要注意的问题是，返回的函数并没有被立刻执行，而是直到调用了f()才执行。我们来看一个例子： 123456789def count(): fs = [] for i in range(1,4): def f(): return i*i fs.append(f) return fs f1,f2,f3 = count() 直接打印f1/f2/f3得到的是一个返回函数对象，需要调用这个函数才能够获取到值。 还有一点，按理来说这个函数最终的返回值应该是1,4,9但实际结果却是9。 原因就在于返回的函数引用了变量 i，但并没l立即执行。等到3个函数都返回时，他们所引用的变量 i 已经变成了3，因此最终结果为9。 返回闭包时牢记一点：返回函数不要引用任何循环变量，或者后续会发生变化的变量。 如果一定要引用循环变量怎么办？方法是再创建一个函数，用该函数的参数绑定循环变量当前的值，无论该循环变量后续如何更改，已绑定到函数参数的值不变： 123456789def conut(): def f(j): def g(): return j*j return g fs = [] for i in range(1,4): fs.append(i) return fs 匿名函数当我们再传入函数式，有些时候，不需要显式地定义函数，直接传入匿名函数更方便。 匿名函数关键字lambda，匿名函数有个限制，就是只能有一个表达式，不用写return，返回值就是该表达式的结果。 12def f(x): return x*x 匿名函数lambda x:x * x就相当于f(x)函数 用匿名函数有个好处，因为函数没有名字，不必担心函数名冲突。此外，匿名函数也是一个函数对象，也可以把匿名函数赋值给一个变量，再利用变量来调用该函数: 123f = lambda x :x * xf(5)→25 同样的也可以把匿名函数作为返回值返回 12def build(x,y): return lambda: x * x + y * y 装饰器由于函数也是一个对象，而且函数对象可以被赋值给其它变量，所以，通过变量也能调用该函数。 12345def now(): print('2018-3-25')f = nowf()→2018-3-25 函数对象有一个1234```pythonnow.__name__&apos;now 现在假设我们要增强now()函数的功能，比如，在函数调用前后自动打印日志，但又不希望修改now()函数的定义，这种在代码运行期间动态增加功能的方式，称之为装饰器 本质上，装饰器就是一个返回函数的高阶函数。所以，我们要定义一个能打印日志的装饰器，可以定义如下 123456789def log(func): def wrapper(*args,**kw): print('call %s():' % func.__name__ ) return func(*args,**kw) return wrapper # 终于明白了func(*args, **kw)是什么意思了：# log函数接受一个函数作为参数，然后在内部调用这个函数，并添加想应功能# func(*args, **kw)其实相当于now()，之所以这样写，是因为函数有可能是会变的，这样写，这个函数就可以接收有任何参数的函数了。 理解装饰器12345678# 理解：函数的装饰器其实就是在一个函数外又写了一个函数，比如上面的now函# 数的装饰器，我们需要对这个函数添加功能()，而又不想修改now()函数内部，# 这时，我们就可以写一个装饰器，装饰器的内部是wrapper()，这个函数可以# 接受任何参数，然后在wrapper()函数内部添加我们想要实现的新功能，即打印# 函数日志，然后这个函数的返回值调用now()函数。# 为了下面调用方便，我们把它又加上了一层函数封装log()，这样在下面的函数# 定义时前面加上@log在调用的时候即默认调用了装饰器，实际上我们在调用now# 函数的时候调用的是log()函数的内部。 观察上面的log，因为它是一个装饰器，所以接受一个函数作为参数，并返回一个函数。我们要借助python的@语法，把装饰器至于函数的定义处： 123@logdef now(): print('2018-3-25') 嗲用now()函数，不仅会运行函数本身，还会在运行now()函数前打印一行日志： 123&gt;&gt;&gt; now()call now():2015-3-25 把@log放到now()函数的定义处，相当于执行了语句： 1now = log(now) 由于log()是一个装饰器，返回一个函数，所以，原来的now()函数仍然存在，只是现在同名的now变量指向了新的函数，于是调用now()将执行新函数，即在log()函数中返回的wrapper()函数。 wrapper()函数的参数定义是(*args,**kw)，因此，wrapper()函数可以接受任意参数的调用。在wrapper()函数内，首先打印日志，再接着调用原始函数。 如果装饰器本身需要传入参数，那就需要一个返回解释器的高阶函数，写出来会更复杂。比如要自定义log的文本： 1234567def log(text): def decorator(func): def wrapper(*args, **kw): print('%s %s():' % (text, func.__name__)) return func(*args, **kw) return wrapper return decorator 这个3层嵌套的decorator用法如下： 123@log('execute')def now(): print('2015-3-25') 执行结果如下： 123&gt;&gt;&gt; now()execute now():2015-3-25 和两层嵌套的decorator相比，3层嵌套的效果是这样的： 1&gt;&gt;&gt; now = log('execute')(now) 我们来剖析上面的语句，首先执行log(‘execute’)，返回的是decorator()函数，再调用返回的函数，参数是now函数，返回值最终是wrapper函数。 以上两种装饰器的定义都没有问题，但还差最后一步。因为我们讲了函数也是对象，它有1234```python&gt;&gt;&gt; now.__name__&apos;wrapper&apos; 因为返回的那个wrapper()函数名字就是wrapper，所以，需要把原始函数的__name__等属性复制到wrapper()函数中，否则，有些依赖函数签名的代码执行就会出错。 不需要编写wrapper.__name__ = func.__name__这样的代码，Python内置的functools.wraps就是干这个事的，所以，一个完整的decorator的写法如下： 12345678import functoolsdef log(func): @functools.wraps(func) def wrapper(*args, **kw): print('call %s():' % func.__name__) return func(*args, **kw) return wrapper 或者针对带参数的decorator： 12345678910import functoolsdef log(text): def decorator(func): @functools.wraps(func) def wrapper(*args, **kw): print('%s %s():' % (text, func.__name__)) return func(*args, **kw) return wrapper return decorator]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>学习</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo重新部署]]></title>
    <url>%2F2018%2F11%2F15%2Fhexo%E9%87%8D%E6%96%B0%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[前言：昨天换了电脑，原电脑里很多重要的文件资料都要重新弄，很烦！最重要的是我的博客啊，苦心经营了那么久的博客，难道就因为换了电脑没有了？然后我就在百度上找怎么重新部署、继续维护博客的方法。找了半天都没有找到有效可行的方法，大部分都是说从github上下载项目，创建分支什么的，我觉得还是很麻烦。终于，我在简书上面找到了最佳答案： 具体操作复制原博客为了方便，直接把原博客的所有文件直接复制。 环境部署 安装git，可以从git官网上下载git 打开git bash，设置用户名称和邮件地址 12$ git config --global user.name &quot;username&quot;$ git config --global user.email &quot;username@example.com&quot; 在用户主目录下运行：ssh-keygen -t rsa -C “youremail@example.com” 把其中的邮件地址换成自己的邮件地址，然后一路回车(这里的主目录一般是指Administrator目录) 最后完成后，会在用户主目录下生成.ssh目录，里面有id_rsa和id_rsa.pub两个文件，这两个就是SSH key密钥对，id_rsa是私钥，千万不能泄露出去，id_rsa.pub是公钥，可以放心地告诉任何人。 登陆GitHub，打开「Settings」-&gt;「SSH and GPG keys」，然后点击「new SSH key」，填上任意Title，在Key文本框里粘贴公钥id_rsa.pub文件的内容（千万不要粘贴成私钥了！），最后点击「Add SSH Key」，你就应该看到已经添加的Key。 安装node.js(从官网上下载)这里有两个版本，一般选择LTS 安装hexo：打开git bash客户端，输入 npm install hexo-cli -g，开始安装hexo 更新博客环境都设置好了之后，到这里一般就没问题了。 把原博客文件复制到新电脑里，然后在这个目录下运行git bash 其它操作都和以前一样了，第一次更新hexo g -d的时候可能会出现一个提示框 输入yes点ok就完事啦！！！]]></content>
      <categories>
        <category>HEXO</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F11%2F15%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Hello,this is my blog,i’m Yu deqiang. nice to meet you!]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL语法]]></title>
    <url>%2F2018%2F11%2F12%2FSQL%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[SQL语句对数据库进行操作，我们主要使用的是SQL语句。SQL语句包括最基本的增删改查和高级功能比如各种约束、高级查询等。 创建数据库、表CREATE DATABASE 数据库名 用来创建数据库 CREATE TABLE 表名(字段1 定义类型(长度),字段2 定义类型(长度)) eg.：创建学生库，学生表，有id和name两个字段，类型都为varchar，长度为20，id为主键 12CREATE DATABASE studentsCREATE TABLE students(id VARCHAR(20) PRIMARY KEY,name VARCHAR(20)) 增删改查查询语句：SELECT 字段 FROM 表名 比如从学生表中查询所有学生信息： SELECT * FROM students 在python中，查询语句执行完成后需要使用fetchall或者fetchone来接收这个查询结果，fetchall返回所有数据，fetchone返回一条数据。eg： cursor.excute(“SELECT * FROM students”) data = cursor.fetchall() 这两句话也可以合成一句话： data = cursor.excute(‘SELECT * FROM students’).fetchall() 返回的数据类型是列表嵌套元组形式，可以通过下标访问取值。 SQL SELECT DISTINCT在表中，可能会包含重复值。有时，我们只需要列出不同的值。这是可以使用 SELECT DISTINCT语句。 eg：查询学生表中所有不同的行。 1SELECT DISTINCT * FROM students WHERE 语句如果需要有条件的从表中选取数据，可将WHERE子句添加到SELECT语句： 1SELECT 列名称 FROM 表名称 WHERE 列 运算符 值 下面这些运算符可以在WHERE子句中使用。 操作符 描述 = 等于 &lt;&gt; 不等于 &gt; 大于 &lt; 小于 &gt;= 大于等于 &lt;= 小于等于 BETWEEN 在某个范围内 LIKE 搜索某种模式 注释：在某些版本的 SQL 中，操作符 &lt;&gt; 可以写为 !=。 eg：假如学生列表中有id、年龄、姓名和性别列。我们希望选择性别为男的所有行 1SELECT * FROM students WHERE sex='男' eg:选择所有年龄&gt;10的学生 1SELECT * FROM students WHERE age&gt;10 这里需要提醒一下：当字段值为数字型时，不要使用引号，是字符串是，要使用引号，尽量都使用单引号！和我们学习python的str和int规则一样。 SQL AND&amp;OR 运算符AND和OR运算符用于一个以上的条件对记录进行过滤。 AND和OR可在WHERE子语句中把两个或多个条件结合起来。 如果第一个条件和第二个条件都成立，则AND运算符显示一条记录 如果第一个和第二个条件只要有一个城里，则OR运算符显示一条记录。 AND eg：选择id为1并且name为m4a1的人 1SELECT * FROM students WHERE id=1 AND name='m4a1' OR eg：选择id为1或者name为m4a1的人 1SELECT * FROM students WHERE id=1 OR name='m4a1' AND和OR也可以结合起来组成复杂的表达式： eg：选择id为1或者name为m4a1并且age=10的人 1SELECT * FROM students WHERE (id=1 OR name='m4a1')AND age=10 SQL ORDER BY 语句有时候我们需要对查询返回结果进行排序，在MongoDB中有sort方法，那么在SQL中使用什么呢？ 答案是使用ORDER BY。 ORDER BY语句用于根据指定的列对结果进行排序，默认按照升序进行排序；如果希望按照降序排序，可以使用DESC关键字。 eg：查询所有学生行并以id顺序排序 123SELECT * FROM students ORDER BY id#降序排列可以添加DESC关键字SELECT * FROM students ORDER BY id DESC 增加INSERT INTO 表名(列名1,列名2) VALUES(‘值1’,’值2’) 比如在学生列表中添加一条记录 1INSERT INTO student(id,name) VALUES (1,'aK47') 修改修改使用的是update语句： UPDATE 表名 SET 列名 = 新值 WHERE 列名称 = 某值 列的概念： eg:将学生表中id为1的name改为m4a1 12UPDATE students SET name = 'm4a1' WHERE id = 1UPDATE students SET name = 'scar',id=2 WHERE id = 1#修改多列注意使用,号分割 删除DELETE语句用于删除表中的行： DELETE FROM 表名称 WHERE 列名称 = 值 eg:删除student表id为1的那一行 1DELETE FROM student WHERE id = 1 删除所有行 123DELETE FROM student#或者DELETE * FROM student 高级语法SQL TopTOP子句用于规定要返回的记录的数目。对于拥有很多条记录的大型表来说，TOP子句是非常有用的。 MySQL中的top语法:从学生表中返回10条信息 1SELECT * FROM student LIMIT 10 eg：从students表中选取前两条记录 1SELECT TOP 2 * FROM students eg：从students表中选取50%的记录 1SELECT TOP 50 PERCENT * FROM students SQL LIKELIKE操作符用于在WHERE字句中搜索列中的指定模式。有点像是使用正则筛选数据 eg：我们希望从学生表中选取name以X开始的人： 12SELECT * FROM students WHERE name LIKE 'X%'#%号用于定义通配符（模式中缺少的字母） eg：从学生表中选取name以X结尾的人 1SELECT * FROM students WHERE neme LIKE '%X' eg：从学生表中选取name包含x的人 1SELECT * FROM students WHERE name LIKE '%X%' eg:使用NOT关键字，可以从学生表中选择name不包含x的人 1SELECT * FROM students WHERE name NOT LIKE '%X%' SQL通配符就像是正则表达式的匹配符，在搜索数据库中的数据时，SQL通配符可以替代一个或者多个字符。SQL通配符必须与LIKE运算符一起使用。 在SQL中，可以使用以下通配符： 通配符 描述 % 替代一个或多个字符 _ 仅替代一个字符 [charlist] 字符列中的任何单一字符 [^charlist]或者[!charlist] 不在字符列中的任何单一字符 eg：从学生表中选择name以A或B或C开头的人 1SELECT * FROM students WHERE name LIKE '[ABC]%' eg：从学生表中选择name不以A或B或C开头的人 1SELECT * FROM students WHERE name LIKE '[!ABC]%' SQL IN 操作符IN操作符允许我们在WHERE字句中规定多个值 语法： SELECT * FROM 表名 WHERE 列名 IN (value1,value2,…) eg:从students表中选取name为ak和m4的人 1SELECT * FROM students WHERE name IN ('ak','m4') SQL BETWEEN 操作符BETWEEN 操作符在WHERE子句中使用，作用是选取介于两个值之间的数据范围。这些值可以是数值、文本或者日期。 语法： SELECT * FROM 表名 WHERE 列名 BETWEEN value1 AND value2 eg：从students表中选择id介于5和10之间的人： 1SELECT * FROM students WHERE id BETWEEN 5 AND 10 注意：返回的结果包括5但不包括10 eg：如果要选择上面范围之外的人，可以使用NOT操作符 1SELECT * FROM students WHERE id NOT BETWENN 5 AND 10 SQL Alias（别名）通过使用SQL，可以为列名称和表名称指定别名。 语法：表的别名 1SELECT * FROM 表名 AS 别名 列的别名 1SELECT 列名 AS 别名 FROM 表名 SQL JOINSQL join用于根据两个或多个表中的列之间的关系，从这些表中查询数据。 join和key：有时为了得到完成的结果，我们需要从两个或更多的表中获取结果。我们就需要执行join。 数据库中的表可通过键将彼此联系起来。主键是一个列，在这个列中每一行都是唯一的。在表中，每个主键的值都是唯一的。这样做的目的是在不重复每个表中的所有数据的情况下，把表间的数据交叉捆绑在一起。 请看 “Persons” 表： Id_P LastName FirstName Address City 1 Adams John Oxford Street London 2 Bush George Fifth Avenue New York 3 Carter Thomas Changan Street Beijing 请注意，”Id_P” 列是 Persons 表中的的主键。这意味着没有两行能够拥有相同的 Id_P。即使两个人的姓名完全相同，Id_P 也可以区分他们。 接下来请看 “Orders” 表： Id_O OrderNo Id_P 1 77895 3 2 44678 3 3 22456 1 4 24562 1 5 34764 65 请注意，”Id_O” 列是 Orders 表中的的主键，同时，”Orders” 表中的 “Id_P” 列用于引用 “Persons” 表中的人，而无需使用他们的确切姓名。 请留意，”Id_P” 列把上面的两个表联系了起来。 引用两个表：我们可以通过引用两个表的方式，从两个表中获取数据： eg：上面两张表，加入person是客户表，order是订单表，我们需要查询哪个客户订购了什么： 1SELECT Persons.Lastname,Persons.FirstName,Orders.OrderNo FROM Persons,Orders WHERE Persons.ID_P = Orders.ID_P 结果： LastName FirstName OrderNo Adams John 22456 Adams John 24562 Carter Thomas 77895 Carter Thomas 44678 除了上面的方法，我们也可以使用关键字JOIN来从两个表中获取数据。 如果我们希望列出所有客户的订单，可以使用下面的语句： 1SELECT Persons.LastName,Persons.FirstName,Orders.OrderNo FROM Persons INNER JOIN Orders IN Persons.ID_P ORDER BY Persons.LastName 不同的SQL JOIN:除了我们在上面的例子中实用的INNER JOIN(内连接)，我们还可以使用其他几种链接。 下面列出了您可以使用的 JOIN 类型，以及它们之间的差异。 JOIN: 如果表中有至少一个匹配，则返回行 LEFT JOIN: 即使右表中没有匹配，也从左表返回所有的行 RIGHT JOIN: 即使左表中没有匹配，也从右表返回所有的行 FULL JOIN: 只要其中一个表中存在匹配，就返回行 INNER JOIN在表中存在至少一个匹配时，INNER JOIN关键字返回行 语法： 1SELECT 列名1，列名2 FROM 表名1 INNER JOIN 表名2 ON 表名1.列名1=表名2.列名2 注释：INNER JOIN 和 JOIN 是相同的，SELECT 后面的列名可以是多个来自不同两张表的列名。 LEFT JOINLEFT JOIN 关键字会从左表（表1）那里返回所有航，即使在右表（表2）中没有匹配的行。 语法： 1SELECT 列名1，列名2 FROM 表名1 LEFT JOIN 表名2 ON 表名1.列名1=表名2.列名2 RIGHT JOINRIGHT JOIN 关键字会右表 (table_name2) 那里返回所有的行，即使在左表 (table_name1) 中没有匹配的行。 语法和LEFT JOIN 等一样。 FULL JOIN只要某个表中存在匹配，FULL JOIN 关键字就会返回行。 1234SELECT column_name(s)FROM table_name1FULL JOIN table_name2 ON table_name1.column_name=table_name2.column_name 注释：在某些数据库中， FULL JOIN 称为 FULL OUTER JOIN。 SQL UNION 和 UNION ALL 操作符SQL UNION操作符用于合并两个或多个SELECT语句的结果集。 请注意，UNION内部的SELECT语句必须拥有相同数量的列。列也必须拥有相似的数据类型。同时，每条SELECT语句中的列的顺序必须相同。 语法： 123SELECT 列名 FROM 表1UNIONSELECT 列名 FROM 表2 注释：默认地，UNION 操作符选取不同的值。如果允许重复的值，请使用 UNION ALL。 SQL UNION ALL语法： 123SELECT column_name(s) FROM table_name1UNION ALLSELECT column_name(s) FROM table_name2 复习时请看：W3SCHOOL SELECT INTOSELECT INTO 语句可用于创建表的备份文件。 SELECT INTO 语句从一个表中选取数据，然后把数据插入另一个表中。 SELECT INTO 语句常用于创建表的备份复件或者用于对记录进行存档。 语法：把所有列插入新表： 1SELECT * INTO new_table_name[IN externaldatabase] FROM old_table_name 或者只把希望的列插入新表： 123SELECT column_name(s)INTO new_table_name [IN externaldatabase] FROM old_tablename eg：制作Persons表的备份 1SELECT * INTO Persons_copy FROM Persons IN子句可以用于向另一个数据库表中拷贝表： 1SELECT * INTO Persons IN 'backup.mdb' FROM 如果我们希望拷贝某些列，可以在 SELECT 语句后列出这些域： 123SELECT LastName,FirstNameINTO Persons_backupFROM Persons SQL SELECT INTO实例-带有WHERE子句 我们也可以添加 WHERE 子句。 下面的例子通过从 “Persons” 表中提取居住在 “Beijing” 的人的信息，创建了一个带有两个列的名为 “Persons_backup” 的表： 1234SELECT LastName,FirstnameINTO Persons_backupFROM PersonsWHERE City='Beijing' SQL SELECT INTO 实例-被连接的表 从一个以上的白哦中选取数据也是可以做到的。 下面的例子会创建一个名为 “Persons_Order_Backup” 的新表，其中包含了从 Persons 和 Orders 两个表中取得的信息： 12345SELECT Persons.LastName,Orders.OrderNoINTO Persons_Order_BackupFROM PersonsINNER JOIN OrdersON Persons.Id_P=Orders.Id_P SQL CTEATE DATABASECREATE DATABASE用于创建数据库。 语法： 1CREATE DATABASE 数据库名 SQL CREATE TABLECREATE TABLE 用于创建数据库中的表 语法： 1CREATE TABLE 表名(字段1 数据类型，字段2 数据类型，字段3 数据类型，...) 数据类型（data_type）规定了列可容纳何种数据类型。下面的表格包含了SQL中最常用的数据类型： 数据类型 描述 integer(size)int(size)smallint(size)tinyint(size) 仅容纳整数。在括号内规定数字的最大位数。 decimal(size,d)numeric(size,d) 容纳带有小数的数字。”size” 规定数字的最大位数。”d” 规定小数点右侧的最大位数。 char(size) 容纳固定长度的字符串（可容纳字母、数字以及特殊字符）。在括号中规定字符串的长度。 varchar(size) 容纳可变长度的字符串（可容纳字母、数字以及特殊的字符）。在括号中规定字符串的最大长度。 date(yyyymmdd) 容纳日期。 SQL 约束约束用于限制加入表的数据的类型。 可以在创建表示规定约束（通过CREATE TABLE语句），或者在表创建之后通过（ALERT TABLE语句） 我们将主要探讨以下几种约束： NOT NULL UNIQUE PRIMARY KEY FOREIGN KEY CHECK DEFAULT NOT NULL(非空约束)NOT NULL 约束强制列不接受 NULL 值。 NOT NULL 约束强制字段始终包含值。这意味着，如果不向字段添加值，就无法插入新记录或者更新记录。 eg：创建students表，有3个字段（id,name,sex）并且id，name添加非空约束 1CREATE TABLE students(id INT NOT NULL,name VARCHAR(30) NOT NULL,sex VARCHAR(10) ) UNIQUE(唯一约束)UNIQUE约束唯一标识数据库中的每条记录。 UNIQUE 和 PRIMARY KEY 约束均为列或列集合提供了唯一性的保证，PRIMARY KEY 拥有自动定义的UNIQUE约束。每张表只能有一个PRIMARY KEY约束，但可以有多个UNIQUE约束。 下面的 SQL 在 “Persons” 表创建时在 “Id_P” 列创建 UNIQUE 约束： 123456789CREATE TABLE Persons(Id_P int NOT NULL,LastName varchar(255) NOT NULL,FirstName varchar(255),Address varchar(255),City varchar(255),UNIQUE (Id_P)) 如果需要命名 UNIQUE 约束，以及为多个列定义 UNIQUE 约束，请使用下面的 SQL 语法： 123456789CREATE TABLE Persons(Id_P int NOT NULL,LastName varchar(255) NOT NULL,FirstName varchar(255),Address varchar(255),City varchar(255),CONSTRAINT uc_PersonID UNIQUE (Id_P,LastName)) 当表已被创建时，如需在 “Id_P” 列创建 UNIQUE 约束，请使用下列 SQL： 12ALTER TABLE PersonsADD UNIQUE (Id_P) 如需命名 UNIQUE 约束，并定义多个列的 UNIQUE 约束，请使用下面的 SQL 语法： 12ALTER TABLE PersonsADD CONSTRAINT uc_PersonID UNIQUE (Id_P,LastName) 如果需要撤销UNIQUE约束，使用下面的SQL 1ALTER TABLE Persons DROP INDEX 列名 SQL PRIMARY KEY (主键约束)PRIMARY KEY 约束唯一标识数据库表中的每条记录。 主键必须包含唯一的值。 主键列不能包含 NULL 值。 每个表都应该有一个主键，并且每个表只能有一个主键。 再创建表的时候定义主键约束eg： 123456789CREATE TABLE Persons(Id_P int NOT NULL,LastName varchar(255) NOT NULL,FirstName varchar(255),Address varchar(255),City varchar(255),PRIMARY KEY (Id_P)) 如果需要命名 PRIMARY KEY 约束，以及为多个列定义 PRIMARY KEY 约束，请使用下面的 SQL 语法： 123456789CREATE TABLE Persons(Id_P int NOT NULL,LastName varchar(255) NOT NULL,FirstName varchar(255),Address varchar(255),City varchar(255),CONSTRAINT pk_PersonID PRIMARY KEY (Id_P,LastName)) 如果在表已存在的情况下为 “Id_P” 列创建 PRIMARY KEY 约束，请使用下面的 SQL： 12ALTER TABLE PersonsADD PRIMARY KEY (Id_P) 如果需要命名 PRIMARY KEY 约束，以及为多个列定义 PRIMARY KEY 约束，请使用下面的 SQL 语法： 12ALTER TABLE PersonsADD CONSTRAINT pk_PersonID PRIMARY KEY (Id_P,LastName) 注释：如果您使用 ALTER TABLE 语句添加主键，必须把主键列声明为不包含 NULL 值（在表首次创建时）。 如需撤销 PRIMARY KEY 约束，请使用下面的 SQL： 12ALTER TABLE PersonsDROP PRIMARY KEY FOREIGN KEY(外键约束)一个表中的FOREIGN KEY指向另一个表中的PRIMARY KEY。 让我们通过一个例子来解释外键。请看下面两个表： “Persons” 表： Id_P LastName FirstName Address City 1 Adams John Oxford Street London 2 Bush George Fifth Avenue New York 3 Carter Thomas Changan Street Beijing “Orders” 表： Id_O OrderNo Id_P 1 77895 3 2 44678 3 3 22456 1 4 24562 1 请注意，”Orders” 中的 “Id_P” 列指向 “Persons” 表中的 “Id_P” 列。 “Persons” 表中的 “Id_P” 列是 “Persons” 表中的 PRIMARY KEY。 “Orders” 表中的 “Id_P” 列是 “Orders” 表中的 FOREIGN KEY。 FOREIGN KEY 约束用于预防破坏表之间连接的动作。 FOREIGN KEY 约束也能防止非法数据插入外键列，因为它必须是它指向的那个表中的值之一。 eg:下面的 SQL 在 “Orders” 表创建时为 “Id_P” 列创建 FOREIGN KEY： 12345678CREATE TABLE Orders(Id_O int NOT NULL,OrderNo int NOT NULL,Id_P int,PRIMARY KEY (Id_O),FOREIGN KEY (Id_P) REFERENCES Persons(Id_P)) FOREIGN KEY (列名) REFERENCES 表名(列名) 一个表中的外键是指向另一个表中的主键。 如果需要命名 FOREIGN KEY 约束，以及为多个列定义 FOREIGN KEY 约束，请使用下面的 SQL 语法： 123456789CREATE TABLE Orders(Id_O int NOT NULL,OrderNo int NOT NULL,Id_P int,PRIMARY KEY (Id_O),CONSTRAINT fk_PerOrders FOREIGN KEY (Id_P)REFERENCES Persons(Id_P)) 如果在 “Orders” 表已存在的情况下为 “Id_P” 列创建 FOREIGN KEY 约束，请使用下面的 SQL： 123ALTER TABLE OrdersADD FOREIGN KEY (Id_P)REFERENCES Persons(Id_P) 如果需要命名 FOREIGN KEY 约束，以及为多个列定义 FOREIGN KEY 约束，请使用下面的 SQL 语法： 1234ALTER TABLE OrdersADD CONSTRAINT fk_PerOrdersFOREIGN KEY (Id_P)REFERENCES Persons(Id_P) 如需撤销 FOREIGN KEY 约束，请使用下面的 SQL： 1ALTER TABLE Orders DROP FORENGN KEY fk_perOrders CHECK(范围约束)CHECK约束用于限制列中的值的范围。 如果对单个列定义CHECK约束，那么该列只允许特定的值。 如果对一个表定义CHECK约束，那么此约束会在特定的乐众对值进行限制。 eg：下面的 SQL 在 “Persons” 表创建时为 “Id_P” 列创建 CHECK 约束。CHECK 约束规定 “Id_P” 列必须只包含大于 0 的整数。 123456789CREATE TABLE Persons(Id_P int NOT NULL,LastName varchar(255) NOT NULL,FirstName varchar(255),Address varchar(255),City varchar(255),CHECK (Id_P&gt;0)) 如果需要命名 CHECK 约束，以及为多个列定义 CHECK 约束，请使用下面的 SQL 语法： 123456789CREATE TABLE Persons(Id_P int NOT NULL,LastName varchar(255) NOT NULL,FirstName varchar(255),Address varchar(255),City varchar(255),CONSTRAINT chk_Person CHECK (Id_P&gt;0 AND City='Sandnes')) 如果在表已存在的情况下为 “Id_P” 列创建 CHECK 约束，请使用下面的 SQL： 12ALTER TABLE PersonsADD CHECK (Id_P&gt;0) 如果需要命名 CHECK 约束，以及为多个列定义 CHECK 约束，请使用下面的 SQL 语法： 12ALTER TABLE PersonsADD CONSTRAINT chk_Person CHECK (Id_P&gt;0 AND City='Sandnes') 如需撤销 CHECK 约束，请使用下面的 SQL： 12ALTER TABLE PersonsDROP CHECK chk_Person DEFAULT(默认值约束)DEFAULT 约束用于向列中插入默认值。 如果没有规定其他的值，那么会将默认值添加到所有的新记录。 下面的 SQL 在 “Persons” 表创建时为 “City” 列创建 DEFAULT 约束： 12345678CREATE TABLE Persons(Id_P int NOT NULL,LastName varchar(255) NOT NULL,FirstName varchar(255),Address varchar(255),City varchar(255) DEFAULT 'Sandnes') 通过使用类似 GETDATE() 这样的函数，DEFAULT 约束也可以用于插入系统值： 1234567CREATE TABLE Orders(Id_O int NOT NULL,OrderNo int NOT NULL,Id_P int,OrderDate date DEFAULT GETDATE()) 如果在表已存在的情况下为 “City” 列创建 DEFAULT 约束，请使用下面的 SQL： 12ALTER TABLE PersonsALTER City SET DEFAULT 'SANDNES' 如需撤销 DEFAULT 约束，请使用下面的 SQL： 12ALTER TABLE PersonsALTER City DROP DEFAULT SQL CREATE INDEX 语句CREATE INDEX 语句用于在表中创建索引。在不读取整个表的情况下，索引使数据库应用程序可以更快的查找数据。 您可以在表中创建索引，以便更加快速高效地查询数据。 用户无法看到索引，它们只能被用来加速搜索/查询。 注释：更新一个包含索引的表需要比更新一个没有索引的表更多的时间，这是由于索引本身也需要更新。因此，理想的做法是仅仅在常常被搜索的列（以及表）上面创建索引。 语法： 1CREATE INDEX index_name ON table_name(column_name) 注释：”column_name” 规定需要索引的列。 SQL CREATE UNIQUE INDEX 语法: 在表上创建一个唯一的索引。唯一的索引意味着两个行不能拥有相同的索引值。 12CREATE UNIQUE INDEX index_nameON table_name (column_name) CREATE INDEX 实例 本例会创建一个简单的索引，名为 “PersonIndex”，在 Person 表的 LastName 列： 12CREATE INDEX PersonIndexON Person (LastName) 如果您希望以降序索引某个列中的值，您可以在列名称之后添加保留字 DESC： 12CREATE INDEX PersonIndexON Person (LastName DESC) 假如您希望索引不止一个列，您可以在括号中列出这些列的名称，用逗号隔开： 12CREATE INDEX PersonIndexON Person (LastName, FirstName) SQL 撤销索引、表以及数据库通过使用DROP语句，可以轻松地删除索引、表和数据库。 我们可以使用DROP INDEX命令删除表格中的索引。语法： 1ALTER TABLE table_name DROP INDEX index_name DROP TABLE 语句用于删除表（表的结构、属性以及索引也会被删除）： 1DROP TABLE 表名称 DROP DATABASE 语句用于删除数据库： 1DROP DATABASE 数据库名称 如果我们仅仅需要除去表内的数据，但并不删除表本身，那么我们该如何做呢？ 请使用 TRUNCATE TABLE 命令（仅仅删除表格中的数据）： 1TRUNCATE TABLE 表名称 SQL ALTER TABLE(修改表)ALTER TABLE 语句用于在已有的表中添加、修改或删除列。 添加列如果需要在表中添加列，请使用下列语法： 1ALTER TABLE 表名 ADD 列名 数据类型 也可以在添加列时给定一个默认值 1ALTER TABLE 表名 ADD 列名 数据类型 DEFAULT 'test' 要删除表中的列，请使用下列语法： 12ALTER TABLE 表名 DROP COLUMN 列名 注释：某些数据库系统不允许这种在数据库表中删除列的方式 (DROP COLUMN column_name)。 要改变表中列的数据类型，请使用下列语法： 12ALTER TABLE table_nameALTER COLUMN column_name datatype Persons 表: Id LastName FirstName Address City 1 Adams John Oxford Street London 2 Bush George Fifth Avenue New York 3 Carter Thomas Changan Street Beijing SQL ALTER TABLE 实例现在，我们希望在表 “Persons” 中添加一个名为 “Birthday” 的新列。 我们使用下列 SQL 语句： 12ALTER TABLE PersonsADD Birthday date 请注意，新列 “Birthday” 的类型是 date，可以存放日期。数据类型规定列中可以存放的数据的类型。 新的 “Persons” 表类似这样： Id LastName FirstName Address City Birthday 1 Adams John Oxford Street London 2 Bush George Fifth Avenue New York 3 Carter Thomas Changan Street Beijing 改变数据类型实例现在我们希望改变 “Persons” 表中 “Birthday” 列的数据类型。 我们使用下列 SQL 语句： 12ALTER TABLE PersonsALTER COLUMN Birthday year 请注意，”Birthday” 列的数据类型是 year，可以存放 2 位或 4 位格式的年份。 DROP COLUMN 实例接下来，我们删除 “Person” 表中的 “Birthday” 列： 12ALTER TABLE PersonDROP COLUMN Birthday Persons 表会成为这样: Id LastName FirstName Address City 1 Adams John Oxford Street London 2 Bush George Fifth Avenue New York 3 Carter Thomas Changan Street Beijing SQL AUTO INCREMENT 字段AUTO INCREMENT 会在新记录插入表中时生成一个唯一的数字。 我们通常希望在每次插入新记录时，自动的创建主键字段的值。 我们可以在表中创建一个auto-increment 字段。 eg：下列 SQL 语句把 “Persons” 表中的 “P_Id” 列定义为 auto-increment 主键： 123456789CREATE TABLE Persons(P_Id int NOT NULL AUTO_INCREMENT,LastName varchar(255) NOT NULL,FirstName varchar(255),Address varchar(255),City varchar(255),PRIMARY KEY (P_Id)) MySQL 使用 AUTO_INCREMENT 关键字来执行 auto-increment 任务。 默认地，AUTO_INCREMENT 的开始值是 1，每条新记录递增 1。 要让 AUTO_INCREMENT 序列以其他的值起始，请使用下列 SQL 语法： 1ALTER TABLE Persons AUTO_INCREMENT=100 要在 “Persons” 表中插入新记录，我们不必为 “P_Id” 列规定值（会自动添加一个唯一的值）： 12INSERT INTO Persons (FirstName,LastName)VALUES (&apos;Bill&apos;,&apos;Gates&apos;) 上面的 SQL 语句会在 “Persons” 表中插入一条新记录。”P_Id” 会被赋予一个唯一的值。”FirstName” 会被设置为 “Bill”，”LastName” 列会被设置为 “Gates”。 SQL VIEW（视图）视图是可视化的表。 在SQL中，视图是基于SQL语句的结果集的可视化的表。 就像是通过基于某些条件创建了一张新表，这张新表保存在数据库的view子目录下 视图包含行和列，就像一个真实的表。视图中的字段就是来自一个或多个数据库中的表的真实字段。我们可以向视图添加SQL函数、WHERE以及JOIN语句，我们也可以提交数据，就像这些来自于某个单一的表。数据库的设计和结构不会受到视图中的函数、where或join语句的影响。 SQL CREATE VIEW 语法12CREATE VIEW view_name AS SELECT column_name(s) FROM table_name WHERE condition 视图总是显示最近的数据。每当用户查询视图是，数据库引擎通过使用SQL语句来重建数据。 实例：可以从某个查询内部、某个存储过程内部，或者从另一个视图内部来使用视图。通过向视图添加函数、join等等，我们可以向用户精确的提交我们希望提交的数据。 样本数据库Northwind 拥有一些被默认安装的视图。视图 “Current Product List” 会从 Products 表列出所有正在使用的产品。这个视图使用下列 SQL 创建： 1234CREATE VIEW [Current Product List] ASSELECT ProductID,ProductNameFROM ProductsWHERE Discontinued=No 我们可以查询上面这个视图： 1SELECT * FROM [Current Prodect List] Northwind 样本数据库的另一个视图会选取 Products 表中所有单位价格高于平均单位价格的产品： 1234CREATE VIEW [Products Above Average Price] ASSELECT ProductName,UnitPriceFROM ProductsWHERE UnitPrice&gt;(SELECT AVG(UnitPrice) FROM Products) 我们可以像这样查询上面这个视图： 1SELECT * FROM [Products Above Average Price] 另一个来自 Northwind 数据库的视图实例会计算在 1997 年每个种类的销售总数。请注意，这个视图会从另一个名为 “Product Sales for 1997” 的视图那里选取数据： 1234CREATE VIEW [Category Sales For 1997] ASSELECT DISTINCT CategoryName,Sum(ProductSales) AS CategorySalesFROM [Product Sales for 1997]GROUP BY CategoryName 我们可以像这样查询上面这个视图： 1SELECT * FROM [Category Sales For 1997] 我们也可以向查询添加条件。现在，我们仅仅需要查看 “Beverages” 类的全部销量： 12SELECT * FROM [Category Sales For 1997]WHERE CategoryName='Beverages' 语法讲解：CREATE VIEW 创建视图 AS 根据条件 上面的条件是从Products表中查询ProductID,ProductName字段其实就是一个SQL查询语句，这个AS后面可以跟很多条件，比如我们可以使用join语句从两张表中查询某些字段： 12CREATE VIEW test AS SELECT 列名1,列名2 FROM 表名1 INNER JOIN 表名2 ON 条件#条件比如可以是表1.列名1=表2.列名2 SQL 更新视图(好像没什么用)可以使用下面的语法来更新视图： 12SQL CREATE OR REPLACE VIEW SyntaxCREATE OR REPLACE VIEW 视图名 AS SELECT column_name(s) FROM table_name WHERE condition 现在，我们希望向 “Current Product List” 视图添加 “Category” 列。我们将通过下列 SQL 更新视图： 1234CREATE VIEW [Current Product List] ASSELECT ProductID,ProductName,CategoryFROM ProductsWHERE Discontinued=No SQL 撤销视图您可以通过 DROP VIEW 命令来删除视图。 12SQL DROP VIEW SyntaxDROP VIEW view_name SQL DATE(时间、日期)当我们处理日期时，最难的任务恐怕是确保所插入的日期的格式，与数据库中日期列的格式相匹配。 只要数据包含的只是日期部分，运行查询就不会出问题。但是，如果涉及时间，情况就有点复杂了。 在讨论日期查询的复杂性之前，我们先来看看最重要的内建日期处理函数。 Date函数下面的表格列出了 MySQL 中最重要的内建日期函数： 函数 描述 NOW() 返回当前的日期和时间 CURDATE() 返回当前的日期 CURTIME() 返回当前的时间 DATE() 提取日期或日期/时间表达式的日期部分 EXTRACT() 返回日期/时间按的单独部分 DATE_ADD() 给日期添加指定的时间间隔 DATE_SUB() 从日期减去指定的时间间隔 DATEDIFF() 返回两个日期之间的天数 DATE_FORMAT() 用不同的格式显示日期/时间 SQL Date数据类型MySQL 使用下列数据类型在数据库中存储日期或日期/时间值： DATE - 格式 YYYY-MM-DD DATETIME - 格式: YYYY-MM-DD HH:MM:SS TIMESTAMP - 格式: YYYY-MM-DD HH:MM:SS YEAR - 格式 YYYY 或 YY SQL 日期处理如果不涉及时间部分，那么我们可以轻松地比较两个日期！ 假设我们有下面这个 “Orders” 表： OrderId ProductName OrderDate 1 computer 2008-12-26 2 printer 2008-12-26 3 electrograph 2008-11-12 4 telephone 2008-10-19 现在，我们希望从上表中选取 OrderDate 为 “2008-12-26” 的记录。 我们使用如下 SELECT 语句： 1SELECT * FROM Orders WHERE OrderDate=&apos;2008-12-26&apos; 结果集： OrderId ProductName OrderDate 1 computer 2008-12-26 3 electrograph 2008-12-26 现在假设 “Orders” 类似这样（请注意 “OrderDate” 列中的时间部分）： OrderId ProductName OrderDate 1 computer 2008-12-26 16:23:55 2 printer 2008-12-26 10:45:26 3 electrograph 2008-11-12 14:12:08 4 telephone 2008-10-19 12:56:10 如果我们使用上面的 SELECT 语句： 1SELECT * FROM Orders WHERE OrderDate=&apos;2008-12-26&apos; 那么我们得不到结果。这是由于该查询不含有时间部分的日期。 提示：如果您希望使查询简单且更易维护，那么请不要在日期中使用时间部分！ SQL NULLNULL值是遗漏的未知数据。默认的表的列可以存放NULL值。 NULL如果表中的某个列是可选的，那么我们可以在不向该列添加值的情况下插入新纪录或更新现有的记录。这意味着该字段将以NULL值保存。 NULL值得处理方式与其他值不同。 NULL用作未知的或不是用的值得占位符。 注意：NULL值无法和0比较；他们是不等价的。 SQL 的 NULL值处理： 请看下面的 “Persons” 表： Id LastName FirstName Address City 1 Adams John London 2 Bush George Fifth Avenue New York 3 Carter Thomas Beijing 假如 “Persons” 表中的 “Address” 列是可选的。这意味着如果在 “Address” 列插入一条不带值的记录，”Address” 列会使用 NULL 值保存。 那么我们如何测试 NULL 值呢？ 无法使用比较运算符来测试 NULL 值，比如 =, &lt;, 或者 &lt;&gt;。 我们必须使用 IS NULL 和 IS NOT NULL 操作符。 IS NULL我们如何仅仅选取在 “Address” 列中带有 NULL 值的记录呢？ 我们必须使用 IS NULL 操作符： 12SELECT LastName,FirstName,Address FROM PersonsWHERE Address IS NULL 结果集： LastName FirstName Address Adams John Carter Thomas 提示：请始终使用 IS NULL 来查找 NULL 值。 IS NOT NULL我们如何选取在 “Address” 列中不带有 NULL 值的记录呢？ 我们必须使用 IS NOT NULL 操作符： 12SELECT LastName,FirstName,Address FROM PersonsWHERE Address IS NOT NULL 结果集： LastName FirstName Address Bush George Fifth Avenue SQL NULL函数SQL ISNULL()、NVL()、IFNULL()和COALESCE()函数请看下面的 “Products” 表： P_Id ProductName UnitPrice UnitsInStock UnitsOnOrder 1 computer 699 25 15 2 printer 365 36 3 telephone 280 159 57 假如 “UnitsOnOrder” 是可选的，而且可以包含 NULL 值。 我们使用如下 SELECT 语句： 12SELECT ProductName,UnitPrice*(UnitsInStock+UnitsOnOrder)FROM Products 在上面的例子中，如果有 “UnitsOnOrder” 值是 NULL，那么结果是 NULL。 微软的 ISNULL() 函数用于规定如何处理 NULL 值。 NVL(), IFNULL() 和 COALESCE() 函数也可以达到相同的结果。 在这里，我们希望 NULL 值为 0。 下面，如果 “UnitsOnOrder” 是 NULL，则不利于计算，因此如果值是 NULL 则 ISNULL() 返回 0。 SQL Server / MS Access 12SELECT ProductName,UnitPrice*(UnitsInStock+ISNULL(UnitsOnOrder,0))FROM Products Oracle Oracle 没有 ISNULL() 函数。不过，我们可以使用 NVL() 函数达到相同的结果： 12SELECT ProductName,UnitPrice*(UnitsInStock+NVL(UnitsOnOrder,0))FROM Products MySQL MySQL 也拥有类似 ISNULL() 的函数。不过它的工作方式与微软的 ISNULL() 函数有点不同。 在 MySQL 中，我们可以使用 IFNULL() 函数，就像这样： 12SELECT ProductName,UnitPrice*(UnitsInStock+IFNULL(UnitsOnOrder,0))FROM Products 或者我们可以使用 COALESCE() 函数，就像这样： 12SELECT ProductName,UnitPrice*(UnitsInStock+COALESCE(UnitsOnOrder,0))FROM Products MySQL 数据类型在 MySQL 中，有三种主要的类型：文本、数字和日期/时间类型。 Text 类型： 数据类型 描述 CHAR(size) 保存固定长度的字符串（可包含字母、数字以及特殊字符）。在括号中指定字符串的长度。最多 255 个字符。 VARCHAR(size) 保存可变长度的字符串（可包含字母、数字以及特殊字符）。在括号中指定字符串的最大长度。最多 255 个字符。注释：如果值的长度大于 255，则被转换为 TEXT 类型。 TINYTEXT 存放最大长度为 255 个字符的字符串。 TEXT 存放最大长度为 65,535 个字符的字符串。 BLOB 用于 BLOBs (Binary Large OBjects)。存放最多 65,535 字节的数据。 MEDIUMTEXT 存放最大长度为 16,777,215 个字符的字符串。 MEDIUMBLOB 用于 BLOBs (Binary Large OBjects)。存放最多 16,777,215 字节的数据。 LONGTEXT 存放最大长度为 4,294,967,295 个字符的字符串。 LONGBLOB 用于 BLOBs (Binary Large OBjects)。存放最多 4,294,967,295 字节的数据。 ENUM(x,y,z,etc.) 允许你输入可能值的列表。可以在 ENUM 列表中列出最大 65535 个值。如果列表中不存在插入的值，则插入空值。注释：这些值是按照你输入的顺序存储的。可以按照此格式输入可能的值：ENUM(‘X’,’Y’,’Z’) SET 与 ENUM 类似，SET 最多只能包含 64 个列表项，不过 SET 可存储一个以上的值。 Number 类型： 数据类型 描述 TINYINT(size) -128 到 127 常规。0 到 255 无符号*。在括号中规定最大位数。 SMALLINT(size) -32768 到 32767 常规。0 到 65535 无符号*。在括号中规定最大位数。 MEDIUMINT(size) -8388608 到 8388607 普通。0 to 16777215 无符号*。在括号中规定最大位数。 INT(size) -2147483648 到 2147483647 常规。0 到 4294967295 无符号*。在括号中规定最大位数。 BIGINT(size) -9223372036854775808 到 9223372036854775807 常规。0 到 18446744073709551615 无符号*。在括号中规定最大位数。 FLOAT(size,d) 带有浮动小数点的小数字。在括号中规定最大位数。在 d 参数中规定小数点右侧的最大位数。 DOUBLE(size,d) 带有浮动小数点的大数字。在括号中规定最大位数。在 d 参数中规定小数点右侧的最大位数。 DECIMAL(size,d) 作为字符串存储的 DOUBLE 类型，允许固定的小数点。 * 这些整数类型拥有额外的选项 UNSIGNED。通常，整数可以是负数或正数。如果添加 UNSIGNED 属性，那么范围将从 0 开始，而不是某个负数。 Date 类型： 数据类型 描述 DATE() 日期。格式：YYYY-MM-DD注释：支持的范围是从 ‘1000-01-01’ 到 ‘9999-12-31’ DATETIME() *日期和时间的组合。格式：YYYY-MM-DD HH:MM:SS注释：支持的范围是从 ‘1000-01-01 00:00:00’ 到 ‘9999-12-31 23:59:59’ TIMESTAMP() *时间戳。TIMESTAMP 值使用 Unix 纪元(‘1970-01-01 00:00:00’ UTC) 至今的描述来存储。格式：YYYY-MM-DD HH:MM:SS注释：支持的范围是从 ‘1970-01-01 00:00:01’ UTC 到 ‘2038-01-09 03:14:07’ UTC TIME() 时间。格式：HH:MM:SS 注释：支持的范围是从 ‘-838:59:59’ 到 ‘838:59:59’ YEAR() 2 位或 4 位格式的年。注释：4 位格式所允许的值：1901 到 2155。2 位格式所允许的值：70 到 69，表示从 1970 到 2069。 * 即便 DATETIME 和 TIMESTAMP 返回相同的格式，它们的工作方式很不同。在 INSERT 或 UPDATE 查询中，TIMESTAMP 自动把自身设置为当前的日期和时间。TIMESTAMP 也接受不同的格式，比如 YYYYMMDDHHMMSS、YYMMDDHHMMSS、YYYYMMDD 或 YYMMDD。 暂时先学习到这儿…… 学习]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL使用记录]]></title>
    <url>%2F2018%2F11%2F12%2FMySQL%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[前言之前已经讲过MySQL的安装，这篇文章主要记录我在使用MySQL时遇到的各种问题和解决方法。 可视化工具的使用Navicat for MySQL是一套管理和开发MySQL的工具，实现了数据库可视化操作，对于新手来说安装一个可视化工具对于学习数据库有很大帮助。 Navicat for MySQL中文版，使用起来很方便。缺点就是这个软件收费，在网上看了一下正版1500；但是，早已经有人破解了这个软件。网上破解的方法有很多，我在这里就介绍一个最简单的方法： 下载链接：https://pan.baidu.com/s/1-6htt3CDzVlEIsurq8_fRw提取码：l6ho使用百度网盘下载上面文件，这里包含了Navicat安装包和破解程序，安装完成Navicat后运行PatchNavicat.exe选择navicat.exe 打开后破解就完成了。 使用安装完成后按照下图连接自己的MySQL 设置好了之后右键刚才新建的连接，然后点击打开连接。 我在做到这一步时出现了错误1251，大致是说不支持什么什么的，然后上网上查了一下，很快找到了解决方法： 在cmd登录到mysql然后执行： 123ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;password&apos; PASSWORD EXPIRE NEVER; #修改加密规则 ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED WITH mysql_native_password BY &apos;password&apos;; #更新一下用户的密码 FLUSH PRIVILEGES; #刷新权限 password可以修改成任意密码 还有一点要注意的是，在mysql命令行下，这些命令都要以;号结尾。 完成了之后再去Navicat打开连接就可以了 python连接mysql使用python连接mysql要首先下载mysql驱动： 1pip install mysql-connector 然后引入驱动操作mysql吧 12345678910# coding:utf-8__autor__ = 'ym'import mysql.connectorconn = mysql.connector.connect(user='root', password='password', database='test')cursor = conn.cursor()cursor.execute('CREATE TABLE student(id VARCHAR (20)PRIMARY KEY ,NAME VARCHAR (20))')cursor.execute("INSERT INTO test1 (id,name) VALUES ('1', '小绿')")cursor.execute("UPDATE student SET NAME='大蓝' WHERE (id='1')")conn.commit()cursor.close() 在上面的代码中，首先引入驱动 然后创建了连接 再创建了cursor 再执行SQL语句 然后和操作sqlite一样，提交事物，关闭游标。 （mysql语法需要多加练习，初次使用，报了很多错误。。。。准备下次专门写一篇记录mysql语法的文章。）]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解HTML和XPath]]></title>
    <url>%2F2018%2F11%2F09%2F%E7%90%86%E8%A7%A3HTML%E5%92%8CXPath%2F</url>
    <content type="text"><![CDATA[HTML我们已经知道HTML是文本标记语言，那么HTML和浏览器有什么样的关系呢？ 当我们在浏览器中输入URL到浏览器显示出页面的过程一般包括四个步骤： 在浏览器输入URL。URL的第一部分(域名，比如baidu.com)用于在网络上找到合适的服务器，而URL以及cookie等其他数据则构成了一个请求，用于发送到那台服务器当中。 服务器端回应，向浏览器发送一个HTML页面。（也有可能发送的是json或者XML格式） 将HTML转换成为浏览器内部的树状表示形式：文档对象模型(Document Object Model,DOM) 基于一些布局规则渲染内部表示，达到我们在浏览器上看到的效果。 下面来看看这些步骤，以及他们所需要的文档表示。 URL对于我们而言，URL分为两个主要部分。第一个部分通过域名系统（DNS）帮助我们在网络上定位合适的服务器。比如挡在浏览器发送https://www.baidu.com/menu/x/xxx/#xx时，将会创建一个对baidu.com的DNS请求，用于确定合适的服务器IP地址，如117.123.42.12。从本质上来看，这个地址被翻译为https://117.123.42.12/menu/x/xxx/#xx。 URL剩余的部分对于服务器端理解请求是什么非常重要。它可能是一张图片、一个文档，或是需要触发某个动作的东西，比如向服务器发送邮件。 HTML文档服务器端读取URL，理解我们的请求是什么，然后回应一个HTML文档。该文档实质上就是一个文本文件，我们可以使用编辑器打开它。和大多数文本文档不同，HTML文档具有万维网联盟指定的格式。当我们在浏览器访问https://www.baidu.com的时候，可以右键选择查看源代码。 树表示法（DOM）每个浏览器都有其复杂的内部数据结构，凭借它来渲染网页。DOM表示法具有跨平台、语言无关性等特点，并且被大多数浏览器支持。 想要在Chrome中查看网页的树表示法，可以选择一个元素然后右键检查。 这个时候我们看到的这个东西就是HTML代码的树表示法。 HTML只是文本，而树表示法是浏览器内存里的对象。 浏览器HTML文本表示和树表示并不是向我们通常在浏览器上看到的那种视图。那么，树表示法是如何映射到我们在浏览器上看到的东西呢？答案就是框模型。正如DOM树元素可以包含其他元素或文本一样，默认情况下，挡在屏幕上渲染时，每个元素的框白哦是同样也都包含其嵌入元素的框表示。从某种意义上来说，我们在浏览器上所看到的是原始HTML文档的二维表示—-树结构也以一种隐藏的方式作为该表示的一部分。 使用XPath选择HTML元素前面已经介绍过XPath的基本语法：链接 在这儿就复习一下： XPath表达式：//标签1[@属性1 = “值”]//标签2[@属性2 = “值”]/…/text() //a[@href]用来选择包含href属性的所有链接， //a[@href = “www.baidu.com&quot;]用来选择href属性为特定值的链接。 不加text()或者@获得的是Element对象，可以对这个对象再次使用XPath，这也就是先抓大再抓小的策略。 提取文本使用text() 提取属性值用@属性 使用*符号来选择指定层级的所有元素 而在Scrapy中，我们在最后还要.extract这个属性才能提取出来元素。 接下来我们学习常见的XPath 重点：前面的XPath学习中我都不知道XPath还有这种语法： 对于大型文档，可能需要编写一个非常大的XPath表达式以访问指定元素。为了避免这一问题，可以使用”//语法”，它可以让你取得某一特定类型的元素，而无需考虑其所在的层次结构。比如，//p将会选择所有的p标签，而//a则会选择所有的链接。 //语法可以在层次结构中的任何地方使用，这非常有用。比如我们想找到div class = “test”下的p标签中的内容，这个div class = “test”下可能还有很多子标签，但我们需要的只是p标签的内容，就可以使用这个语法： //div[@class=”test”]//p/text() 我们也可以选择div test下的子div test1中的所有p标签中的内容 //div[@class=”test”]/div[@class=”test1”]//p/text() 这个语法非常有用，有点相见恨晚的感觉，再写爬虫的时候，经常需要使用XPath来提取有用的信息。这个XPath通常会费好大劲才能分析出来该怎么写，要搞清楚我们需要的信息属于哪个标签，通常在网页源代码中，这种嵌套关系可能有很多层。现在有个这个语法，我们只需要找到包含我们需要的信息的标签，然后直接使用//语法来获得我们需要的标签，这要省不少事。 更加有用的是，它还拥有找到href属性中以一个特定子字符串起始或包含的能力。 //a[starts-with(@href,”http://www&quot;)] //a[contains(@href,”baidu”)] //a[not(contains(@href,”baidu”))] 这些就是XPath的函数，starts-with()函数表示以特定值开头，contains()函数表示属性包含特定值，not(contains())表示不包含某特定值。XPath有很多像这样的函数，可以在百度上找到，不过这些函数不经常使用，记清楚基础的XPath语法才是重中之重。 常见XPath实例有一些XPath表达式，我们将会将常遇到： 获取id为test的h1标签下的span中的text。 //h1[@id=”test”]/span/text() 获取id为toc的div标签内的无序列表中所有的链接 //div[@id=”toc”]/ul//a/@href 获取class属性包含ltr以及class属性包含ltr1的任意元素内所有h1标签下的文本，这两个属性可能在同一个class中，也可能在不同的class中 //*[contains(@class,”ltr”) and contains(@class,”ltr1”)]//h1/text() XPath的contains函数可以让你选择包含指定类的所有元素。 选择class属性值为infobox的表格中的第一张图片的URL。 //table[@class=”infobox”]//img[1]/@src 选择class属性以reflist开头的div标签中所有连接的URL。 //div[starts-with(@class,”reflist”)]//a/@href 选择子元素包含文本test的元素之后的div元素中所有连接的URL //*[text()=”test”]/../following-sibling::div//a 请注意该表达式非常脆弱，并且很容易无法使用，因为他对文档结果做了过多假设。 获取页面中每张图片的URL。 //img/@src 更稳定的XPath我们在抓取网页时，网页经常会发生变化。有时网页结果也会发生变化，因为网站有人在维护。如果他们的HTML已某种方式发生变化时，我们要调整我们的XPath。我们在写XPath时尽量遵循以下原则，帮助我们减少重写XPath的可能性。 避免使用数组索引，例如： //*[@id=”posts”]/article/div/div[1]/p 我们从浏览器中copy出来的XPath中经常会包含大量数字，解决办法是找到一个最接近p标签的标签，找到一个可以使用的包含id或者class属性的元素，如: //div[@class=”post-body”]/p 类并没有那么好用 使用class属性可以更加容易的精确定位元素，不过这些属性一般是用于通过css影响外观的，新词可能会由于网站布局的微小变更而产生变化。 有意义的面向数据的类要比具体的或者面向布局的类更好 我们在使用类选择标签时，首先要选择与内容相关的类，因为这样的类更加可靠。 ID通常是最稳定的 通常情况下，id属性是针对一个目标的最佳选择，因为该属性既有意义又与数据相关。]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>HTML</tag>
        <tag>爬虫</tag>
        <tag>XPath</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日记11.6]]></title>
    <url>%2F2018%2F11%2F06%2F%E6%97%A5%E8%AE%B011-6%2F</url>
    <content type="text"><![CDATA[懒惰和缺乏自控力是我最大的敌人。 我的脑子里只要一有懒惰的想法，那就完蛋啦！！！ 就像上周末一样，周六早上一来，躁动的心就开始无处安放了，就像心里长草了一样。然后周六正好是S8总决赛吗，不知怎么滴，我突然冒出来一个想法：想去找聂辉看总决赛。然后中午还没到吃饭的时候，就下去诳菜市场了。。。然后吃饭的时候，我就觉得今天我在班里坐不下去了。吃完饭就溜了，找个自行车就一路往地铁口奔去。骑一个多小时自行车也丝毫不觉得累，反而觉得很爽！！！回到学校，正好总决赛开始啦，看到IG夺冠那一刻真的挺开心的，怎么说LOL也是陪我度过了那么多时光。虽然有点恨这个游戏，带给我的影响太大了；但还是很开心能看到我们中国赛区的队伍能够拿到S赛总冠军。其实吧，与其说是游戏祸害了我们，不如说我们是自己祸害了自己。游戏没有过错，还是我们自己缺少自制力。原计划是周日晚上按时回寝室的，然后我想周一早上反正九点半才上课，不如周一早上再回去吧，就留了下来。晚上的时候，在425和他们聊天竟然聊到了三点，越聊越有精神，憋了好久突然打开了话匣子，从上这个培训班之后，我好久没有像这样和别人在坐在一起聊天聊这么久了，在培训班里，我估计我给大家的印象就是一个有自闭症的学霸！哈哈哈，我最终变成了自己最讨厌的样子—-在学校的时候，我最讨厌的就是这种人。。。在培训班里，我很少说话，别人跟我说话我也只是随便回答两句，以至于我现在连我右边的同桌是谁都不知道。。。。我佛啦，没想到啊 没想到，我竟然变成了这样子。可能是我真的爱学习吧，哈哈哈！在班里，我现在能叫的上来名字的人不超过十个，我佛了，这都快过去两个月了吧。这也算是一种改变吧，以我的性格，我其实跟什么人都能做朋友，以前每到新环境，总会很快找到新朋友，但是现在，在这里，在我心里，好像暂时还没有能够跟我称得上朋友的人。以前，我吃饭干什么的，甚至上厕所都要别人陪我一起去，现在，我每天都是自己一个人吃饭。我其实觉得这样挺好的，我自己一个人想吃什么就去吃什么，还不用等人，还不用请别人吃饭。哎~扯远了，刚才说到在寝室聊到了三点，大家都聊开心了，我就随便一说：明天上个几把课，不去了！然后。。。。周一早上我跟老师请了个假，又继续睡到了十一点。到了周一晚上我又呆到了八点多，心里差点又萌生出了不去上课的想法。。。。 其实从上次考试之后，我觉得我膨胀了，考了第二名。。。其实我心里觉得我是第一名的，因为我最先交卷，选做题全都作对了，那道最难的编程题好像只有我自己写出来了。结果选择题错了太多，选择题差了十几分…….. 不知怎么的，我真的放松了下来，如果我保持开始的那个劲头继续学的话，估计现在早已把爬虫学完了，这俩星期博客更新的还特别少。我真的不该膨胀，真正的大师永远都怀着一颗学徒的心！真的应该静下心来好好学习啦！争取早点毕业，早点工作，要学的东西还很多，时间真的不是那么充裕，应该争分夺秒来巩固自己所学过的知识，学习新知识。 唉！说起来烦心事真的挺多的，周六那天也是她考试的日子，我在前一天晚上给他打电话，结果人家说不需要。周六那天我发了好几条消息，都不回我，我都不知道发生什么了。那个什么双十一战队也给我退了，说都不说一声。搞得我一脸懵逼，是不是因为我那天没有完成战队任务啊，可是我真的很努力在做了，结果就是找不到，我能有什么办法呢。真想问问他到底怎么了，我一想如果他愿意告诉我的话，早都说了，再说又不是第一次这样对我了，总是突然对我冷落起来，我不都习惯了吗。虽然心里有点生气，有点烦，但是又能怎么办呢。忍着吧，别瞎几把想这些事了，你现在唯一要做的事情就是好好学习，然后找个好工作。可是，我特么要是能随心所欲的控制住自己该干什么不该干什么，还能有这么多烦心事吗。 有时候想想真的觉得这个世界太不公平了，我付出再多可能也不如别人一句话的分量足。心里真的觉得憋屈的慌，唉！没办法没办法没办法啊！ 算了，不想了，努力学习提升自己吧，你若盛开，清风自来！]]></content>
      <categories>
        <category>私密博客</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hornil StylePix]]></title>
    <url>%2F2018%2F11%2F02%2FHornil-StylePix%2F</url>
    <content type="text"><![CDATA[Hornil StylePix 是一个图片编辑软件，跟ps相比他非常轻量，能够满足平时工作学习使用。安装起来也很方便下载地址 下面是教程 节省时间与直观的用户界面Hornil StylePix具有直观的用户界面。它的目的是调整所选功能简单，方便。即使你没有经验，你可以轻松地学习如何编辑图像和修饰您的照片。因此，Hornil StylePix直观的用户界面减少您的时间工作. 为了更好的速度编辑，Hornil StylePix的设计重点在于轻，功能强大。 Hornil StylePix运行在更少的资源，如网络，书籍和笔记本电脑或虚拟机的图像处理功能全(VMware公司虚拟框，虚拟PC等)的环境。我们一直在努力提高Hornil StylePix性能。 便携式支持 Hornil StylePix是一个轻量级的。一种便携式版本的运行Hornil StylePix从可移动存储设备如USB闪存驱动器，闪存卡，或软盘(媒体)。要安装Hornil StylePix便携式，只要下载便携包，然后解压缩。要启动Hornil StylePix便携，只需双击您的便携dirve StylePix.exe文件。 浏览图片和幻灯片浏览图像工具可以让你轻松地探索开放前的影像图像。你也可以打开，复制，删除和重命名的图像或目录。 幻灯片显示了选择的图片系列是在当前工作的全屏幕模式路径中。 支持的文件格式：JPEG，PNG，GIF等，tif格式和TGA，BMP和旅行商。 方便的工作环境有多个文件可以同时打开工作。打开的图像安排在MDI(多文档界面)的容器标签。 MDI的支持级联，瓷砖垂直，水平平铺，设置图标的安排。 快速的图像切换：画布窗口之间切换，按Ctrl+ Tab键。如果你想回去，按Ctrl+ Shift + Tab键。如果按上述键，切换窗口被弹出。然后，如果你想选择下一个画布，按Tab键。 放大/缩小和全屏幕视图和指南，尺子，电网的支持。标尺，网格和引导帮助您编辑。你可以显示/隐藏这些你需要的。缩略图，直方图和电流波形编辑图像的看法。 直方图显示的图像像素为图形的数量。该图的横轴代表语气和垂直轴代表的数量。 波形显示为一个不同于直方图图形图像的像素数量。 多层及分组支持 层是用于Hornil StylePix分开的画布不同的对象。图层就像是在另一个堆放胶片。每一层都可以有不同的对象。 Hornil StylePix支持四个对象类型(图像，文本和路径形状)和组对象。该组对象包含其他对象。此外，本集团可能包含其他组。您可以使用层管理层次。Hornil StylePix支持混合模式是用于确定如何两层互相融合。在StylePix，您可以使用21种混合模式。 # 选择工具 Hornil StylePix支撑区域如以下选择工具： 自动范围选择和色彩范围选择工具 方形，圆形选取工具 多边形，套索选择工具 您可以通过上述工具的区域选择具有以下模式：新，加，减和相交。 现有的区域选择可以进行修改操作：边界，扩展，合同和柔软性。 50种图像过滤器。颜色调节过滤器：自动水平，自动对比度，自动颜色平衡，级别，曲线，色彩平衡，亮度/对比度，色相/饱和度，伽玛校正，去色，反转，灰度，阈值，量化，直方图均化，色调分离。 锐化和模糊过滤器 像素化滤镜 渲染过滤器 噪声滤波器 扭曲过滤器 卷积过滤器 风格过滤器 形态滤波器 照片增强过滤器 绘图工具Hornil StylePix支持各种绘图工具如画笔，橡皮擦，直线，曲线，喷雾，克隆刷，洪水填充，渐变填充，路径和形状。 你可以画几个选项，比如使用大小，抗锯齿，不透明度和混合模式这些工具不同的照片 文字工具文本工具允许你在画布上键入文本。在文本字符串可以被修改，不仅在正常状态，但也不失旋转对象属性的状态。 变换和对齐转换工具允许你改变选择区域或对象。只有当区域选择启用存在。当变换工具被激活，可以旋转和调整大小。 所选择的对象可以被转换或安排如下命令：水平翻转，垂直翻转，旋转左，右旋转，旋转180度，置于顶层，发送到后，布林茨顶，发送至底部，左对齐，水平居中对齐，右对齐，上对齐，垂直对齐中心，底端对齐，水平居中对齐。 加强和还原工具 在提高工具允许您提高基础上的图像变暗，躲闪，模糊和锐化工具。 减轻(道奇)/变暗(燃烧)工具 模糊/锐化工具 还原工具允许你恢复的图像有划伤或红眼等 尘/点除尘工具 红眼移除工具… 裁剪工具 作物工具用于作物或剪辑图像。它适用于所有的形象，有形及无形的层面。 它提供了一些有用的比例，如4 × 3个半卡预设。预置分为横向和纵向。景观包含了有关设备，预置比如HDTV，宽屏等相反的水平轴垂直轴长，包含肖像肖像中经常使用的预置。如果您选择了预设一，风俗和价值观是汽车setted与高度自动调节或按预定的比例。 多级撤消，重做和行动清单 多级撤消，重做支持：行动清单记得你对我所做的你的形象，让你恢复到任何图像的早期版本。每次你选择，油漆和调整大小等，这些国家的每一个行动中单独列出清单。要恢复到以前的状态的图像，单击操作列表或按Ctrl + z的国家的名称 批处理 批次处理是一个非常有用的工具，过程，改变大小或图像文件类型的重复性等任务。 前缀，后缀和文件名称编号 调整大小，翻转或旋转的多图像 插入一个共同的标志和绘画 一些色彩调整滤镜 以上文章照抄：地址 欢迎抄袭~]]></content>
      <categories>
        <category>日志</category>
      </categories>
      <tags>
        <tag>ps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL安装]]></title>
    <url>%2F2018%2F11%2F01%2FMySQL%E5%85%A5%E5%9D%91%2F</url>
    <content type="text"><![CDATA[介绍MySQL是当今世界上最流行的开源数据库，我在前面已经学习过非关系型数据库MongoDB。MongoDB的优点是快速、高扩展性，json的存储格式，就像python的字典一样，学习起来比较快；但缺点就是它是非关系型数据库，属于文档型数据库，不支持事物。 所以今天闲了没事就来学学MySQL。 安装与使用安装MySQL的强大在安装时就能体现出来（安装很麻烦。。。） 首先在官网上下载mysql（我这里下载的是最新版的8.0.13版本） 下载好了之后是个zip文件，找个地方解压就好了。 然后在解压后的文件夹里创建my.ini使用notepad++或其它编译器打开并输入以下代码： 123456789101112131415161718[mysql]# 设置mysql客户端默认字符集default-character-set=utf8 [mysqld]# 设置3306端口port = 3306# 设置mysql的安装目录basedir=C:/web/mysql-8.0.13-winx64datadir=C:/web/mysql-8.0.13-winx64/data# 设置 mysql数据库的数据的存放目录，MySQL 8+ 不需要以下配置，系统自己生成即可，否则有可能报错# datadir=C:\\web\\sqldata# 允许最大连接数# max_connections=20# 服务端使用的字符集默认为8比特编码的latin1字符集character-set-server=utf8# 创建新表时将使用的默认存储引擎default-storage-engine=INNODB 需要注意的是basedir和datadir要换成自己的安装目录和data目录。 弄好了之后这个文件夹差不长这样。 接下来就是启动数据库啦。 cmd下切换到到数据库安装目录\bin下： 然后输入如下命令初始化数据库。 1mysqld --initialize --console 注意啦，初始化完成之后这里会生成一个用户名和密码，这个很重要，用户名是root密码就是上面用红框标注的地方，这个一定要记下来，下面会用到。这个xx密码我也是佛了，就是这个东西坑了我半天。 然后输入以下命令，将mysql安装为Windows 服务 123mysqld install #如果出错试试下一个命令mysqld.exe -install mysql 再输入以下命令启动mysql服务 1net start mysql 登录MySQL当MySQL服务已经运行时，输入以下命令登录MySQL 可以先试试mysql -u root不使用密码登录，如果不行再使用: mysql -u root -p 然后输入之前生成的密码。 登录完成之后使用alter user &#39;root&#39;@&#39;localhost&#39; identified by &#39;xxx&#39; PASSWORD EXPIRE NEVER account unlock;修改密码(xxx) 登录成功的界面: -u 就是用户的意思 -p 表示用密码登录 还有一个-h属性，由于在这里mysql服务运行在本地，所以可以忽略 我在这个登录的过程中失败了N次，百度了一大堆没用的东西，当出现错误的时候首先检查mysql服务开启了没有，可以在计算机关机→服务 中查看，确定开启了之后，就别管其他的，一次两次没登进去就多试几次。就是这个密码，太麻烦了，一般都是密码没输对。 xxx就是新密码。一定要用这个命令！！！ 使用操作Mysql使用SQL语句，后面再更新。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【持续更新】常用知识点]]></title>
    <url>%2F2018%2F10%2F31%2F%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%E7%AC%94%E8%AE%B0-1%2F</url>
    <content type="text"><![CDATA[在很多时候，往往最基本最简单的知识能帮上大忙，在这儿就记录下经常用到的小知识点。希望以后再碰到问题的时候能够首先想到使用这些简单的方法来解决！ 字符串相关：我们几乎天天都在跟字符串打交道，字符串应该是python中最常见的数据类型了；但是对于字符串的基本知识掌握的还是不太牢固。 字符串切片字符串切片：切片操作（slice）可以从一个字符串中获取子字符串（字符串的一部分）。我们使用一对方括号、起始偏移量start、终止偏移量end 以及可选的步长step 来定义一个分片。 格式： [start：end:step] • [:] 提取从开头（默认位置0）到结尾（默认位置-1）的整个字符串• [start:] 从start 提取到结尾• [:end] 从开头提取到end - 1• [start:end] 从start 提取到end - 1• [start：end:step] 从start 提取到end - 1，每step 个字符提取一个• 左侧第一个字符的位置/偏移量为0，右侧最后一个字符的位置/偏移量为-1 几个特别的examples 如下： 提取最后N个字符： 123&gt;&gt;&gt; letter = 'abcdefghijklmnopqrstuvwxyz'&gt;&gt;&gt; letter[-3:]'xyz' 从开头到结尾，step为N： 12&gt;&gt;&gt; letter[::5]'afkpuz' 将字符串倒转(reverse)， 通过设置步长为负数： 12&gt;&gt;&gt; letter[::-1]'zyxwvutsrqponmlkjihgfedcba' 字符串常用方法分割： 12345str1 = 'safasf.jpg'alist = str1.split('.')print(alist)&gt;&gt;&gt;['safasf', 'jpg']#split方法的参数是一个字符，它以这个字符为分割点，把字符串分割为两部分，返回一个列表 替换： 12345str1 = 'safasf.jpg'str2 = str1.replace('.', '=')print(str2)&gt;&gt;&gt;safasf=jpg#replace方法接收两个参数，第一个想要替换的字符，第二个是新字符。返回的是字符串。 爬虫相关我记得我在写知乎爬虫的时候碰到了问题，就是无论用什么解码得到的都是乱码。后来才知道是Accept-Encoding惹的祸。 Accept-Encoding主要表示浏览器支持的压缩编码有哪些。gzip是压缩编码的一种，deflate是一种无损数据压缩算法。 那浏览器压缩编码跟我们需要的HTML代码有什么关系呢？因为如果这个地段设置成gzip、deflate，那么从服务器返回来的是对应的gzip，deflate压缩的代码，此时没有进行解码，所以会出现乱码的情况，而一些常规浏览器中，从服务器返回对应的gzip、deflate压缩的代码后，浏览器可以自动进行解压缩，忽而不会出现乱码。解决的办法是直接忽略不写此字段或者把此字段值改为”utf-8”，”gbk” 还有一个问题，就是在开启了Fiddler后，所爬取的网址要以具体文件或者“/”结尾，如果没有具体文件，直接写该具体文件的网址即可，比如将要爬取的网址写为“http://news.163.com/16/0825/09/BVA89U500014ESH.html”这种写法是可以的，如果被爬网址是一个文件夹，比如要爬取“http://www.baidu.com”，此时爬取的是一个目录(文件夹)，所以需要以“/”结尾。 在使用requests爬取https网址时，需要添加一个参数： 12import requestshtml = requests.get("https://www.baidu.com" verify=False).content.decode() 就是这个verify，不然会报错。 eval()函数eval()函数十分强大，官方文档解释为：将字符串str当成有效的表达式来求职并返回计算结果。 具体用法请看以下实例 格式化字符串我们平时常用的两种格式化字符串方法： 1234str1 = "test1"print('test %s' % test1)str2 = "test2"print('test&#123;&#125;'.format(str2)) 这两种方法其实在平时使用的时候并没有什么觉得不好，但是当字符串变长，或是变量增加的时候就显得很冗长。好消息是，python3.6开始，有一种新的格式化字符串的方法： 12str3 = "test3"print(f'test&#123;str3&#125;') 这种方法看起来是不是更简洁一点呢。。。赶快用起来吧！ json在写爬虫的时候要经常跟json打交道，json是一种类似python字典的数据。 python中的json模块提供了一种很简单的方式来编码和解码JSON数据。其中两个主要的函数是json.dumps()和json.loads()，要比其他系列化函数库如pickle的接口少得多。下面演示如何将一个Python数据结构转换成 JSON： 123456789import jsondata = &#123; 'name' : 'ACME', 'shares' : 100, 'price' : 542.23&#125;json_str = json.dumps(data) 下面演示如何将一个JSON编码的字符串转回一个python的数据结构： 1data = json.loads(json_str) 如果你要处理的是文件而不是字符串，可以使用json.dump()和json.load()来编码和解码JSON数据，例如： 1234567# Writing JSON datawith open('data.json', 'w') as f: json.dump(data, f)# Reading data backwith open('data.json', 'r') as f: data = json.load(f) 总的来说，python数据转json用json.loads()，数据文件转json用json.load() json转python数据用json.dumps(),json文件转python数据用json.dump() 数据库相关sqlite在sqlite中想要使用变量批量插入数据，可以使用以下方法： 1234567891011121314import sqlite3connect = sqlite3.connect("learn1.db")cursor = connect.cursor()# cursor.execute('CREATE TABLE student1(id VARCHAR (20) PRIMARY KEY ,name VARCHAR(20))')for num1 in range(1, 20): aa = f'test&#123;num1&#125;' cursor.execute("INSERT INTO student1(id,name) VALUES ('&#123;&#125;','&#123;&#125;')".format(num1, aa))# cursor.execute('SELECT name FROM student')x = cursor.fetchall()cursor.close()connect.commit()connect.close() 主要代码就是 123for num1 in range(1, 20): aa = f'test&#123;num1&#125;' cursor.execute("INSERT INTO student1(id,name) VALUES ('&#123;&#125;','&#123;&#125;')".format(num1, aa)) 要注意标点符号的使用，占位符都要加引号，因为占位符也算是一个字符串，不是变量。这里可以使用的方法有两种，一种是%s占位符，一种是{}占位符，还有一种格式化的方法f’{}’在这里不能使用。]]></content>
      <categories>
        <category>日志</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>爬虫</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫开发之requests库]]></title>
    <url>%2F2018%2F10%2F27%2F%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91%E4%B9%8Brequests%E5%BA%93%2F</url>
    <content type="text"><![CDATA[介绍​ 我在前面的博客 爬虫入门链接 中已经介绍了python自带的网络库urllib库的基本用法和简单爬虫的基本流程。但是不知道你有没有觉得urllib库有点麻烦，语法臃肿。我今天就来介绍一个更间接地第三方库–requests库。我在第一次遇到requests库之后就果断弃用了urllib，这俩库一比起来高下立判。我曾经问过一个大佬为什么在写爬虫的时候只用requests库？他回答说“人生苦短，为什么不用高效的工具。” ​ requests是python的一个第三方HTTP库，它比python自带的urllib库更加简单、方便和人性化。requests库的作者这样介绍requests：HTTP for humans，这才是给人用的HTTP库。 安装与使用安装可以直接使用pip安装requests: pip install requests 或者手动安装打开：链接 下载到本地后解压，然后找到包含setup.py的文件夹，在此处打开命令行窗口后输入以下命令： python setup.py install 使用​ 使用浏览器来访问网页，看起来只需要输入网址就可以。经过前面的学习，我们知道网页有多种打开方式，最常见的是GET和POST方式。在浏览器里面可以直接通过输入网址访问的页面，就是使用了GET方式。还有一些页面，只能通过从另一个页面单击某个链接或者某个按钮以后跳过来，不能直接通过浏览器输入网址访问，这种网页就使用了POST方式。 GET 方式​ 对于使用GET方式访问的网页，在Python里面可以使用requests的get()方法获取网页的源代码： 1234import requestshtml = requests.get("网址")html_bytes = html.contenthtml_str = html_bytes.decode() ​ 在这四行代码中，第一行导入了requests库，这样代码才能使用。 第二行使用GET方法获取了网页，得到一个Response对象 第三行使用.content属性来显示bytes(字节)类型的网页源代码 第四行使用decode来解码字节。 大部分情况下，这四行代码我一般都合成两行来写： 12import requesthtml_str = request.get("网址").content.decode() 其实在解码的时候，我们会经常遇到很多错误。这个时候首先需要查看源网页的编码方式，一般在网页的头部会表明编码方式。 有时，即使解码方式和源网页保持了一致，还是会出错，比如网页源代码中包含了无法是别的字符。这时可以在decode()时加上第二个参数“ignore”，强制忽略无法识别的字符。 但是，问题往往没有这么简单，有的时候，你解码方式也对了，ignore参数也加了，但解码出来的网页还是一堆乱码。我就遇到过，郁闷了半天，然后一个大佬给我解决了。是因为在设置请求头的时候，请求头里面有一个Accept Encoding属性，他定义了网站接受什么解码方式，把这个属性给删除就行了。注意，这个Accept Encoding属性并不是每个网站都遵守的，所以在出现错误的时候在吧这个属性给删除了。 POST方式网页的访问方式除了GET方式以外，还有POST方式。有一些网页，使用GET和POST方式访问同样的网址，得到的结果是不一样的。还有一些网页，只能使用POST方式访问，如果使用GET方式访问，网站会直接返回错误信息。 POST方法的格式如下： 1234import requestsdata = &#123;'key':'value1', 'key2':'value2'&#125;html_formdata = request.post('网址'.data=data).content.decode() data这个字典的内容需要根据实际情况修改，key和value在不同的网站是不一样的。而做爬虫，构造这个字典是任务之一。 还有一些网址，提交的内容是JSON格式的，因此post()方法的参数需要进行一些修改： 1html_json = requests.post('网址',json=data).content.decode() 这样写代码，request可以自动将字典转换为JSON格式。 关于网页是什么访问方式，可以使用浏览器开发者工具，或者使用Fiddler分析出来。具体方法暂时不讲。 关于请求头​ 我们知道有些网站添加了反爬虫机制，请求头过滤就是最常见的一种发爬虫机制。我在前面的文章中已经介绍过urllib库添加请求头的方法，那么requests库怎么添加请求头呢？很简单，只需要加一行代码： 123headers = &#123;'user-agent':' Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36'&#125;html = requerst.get('网址', headers=headers).content.decode()#可以看到headers是python的字典形式 我在前面的博客中忘了说，我们在写爬虫的时候最好吧浏览器里面的headers全都添加进去，因为很多网站不是设置一个user-agent就能蒙混过关的。 看到这么多条属性要一个一个手动转换成python的字典形式是不是感到头皮发麻，还好我有神器！ 具体方法在我的:另一篇博客 requests库很强大，我先简单写这么多，等以后用到了再更新！]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>requests</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTML总结]]></title>
    <url>%2F2018%2F10%2F26%2FHTML%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本篇博客介绍HTML的语法，还包括CSS等。 HTML介绍HTML：超文本标记语言，除了文本，还包括图片、视频、下载文件等内容。 标记语言： 变成有逻辑、变量、ifalse。标记语言相比比较简单，由信息和结构语法组成。 历史和版本xhtml：类似html但语法要求非常严格。 html4：就是我们要学习的内容。 html5：在html4的基础上添加了新功能和新语法。html5兼容html4目前最流行的就是html5 基本语法12345678910&lt;html&gt;&lt;head&gt; &lt;title&gt;helloworld &lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;p&gt;hello&lt;/p&gt; &lt;p&gt;world&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; ## 常用标签： 标签名 用法 html 根标签 head 头部，元信息，引用css、jss和title title 标题 h1-h6 网页内容里的标题。六个级别。数字越大，标题越小 p 段落，输出完会换行。 br /br br/ 都表示换行 b 加粗 i 斜体 strong 加重语气 small 字体变小变细 hr 水平分割线 a 超链接标签 table 表格 img 图片 ul 无序列表 li 列表中的每一项 ol 有序列表 form 表单 select 下拉菜单 option 下拉菜单的每一项 textarea 文本域 submit 提交。会将数据传递到后台 标签嵌套：html标签是可以嵌套的 空元素：标签的内容可以为空。 ！DOCTYPE html：声明html类型，不容易出错，可以不写 meta charset=”UTF-8”元信息，作者，编码提、网页描述 属性和值为了定义标签的功能。 属性attribate:标签里的键值对里的键。 值 value: 标签里的键值对的值。只需要用双引号括起来。 常用属性 属性名 用法 algin 对齐。center,left,right style 样式，值是css语句 title 鼠标在热点区域停留时出现的提示信息。 name 一个标签的别名，可以用来描述标签，name可以重复 id 一个标签的识别符或别名，跟name的区别是不可重复 href 链接地址 target 链接打开方式，默认是self当前标签页打开 blank新标签页打开 src 图片资源地址 width 宽 height 高 action 定义请求地址 input下的 属性type： text 普通的文本输入框 password 密码输入框 radio 单选 checkbox 多选框 placeholder 占位符输出一些提示性信息 label 标签，写在input前面作为信息提示。 CSScss:层叠样式表。对html布局、字体、颜色进行精确的外观控制 语法css表达式实例: header{background-color:”black”} 上面的header是选择器，大括号包住语句块。每一句语句由声明和值构成，值用括号括住，冒号分割，分号结尾。 选择器：选择这句css声明具体作用在html文件的哪个部分。 引用样式的方式： 行内样式：直接写在html标签内的style属性上。 内部像是：css语句写在head标签下的style标签中。 外部样式：css语句写在xxx.css文件当中。link标签，rel目标文件夹种类，href目标文件地址，type文件类型。 优先级：不同引用样式方式作用于同一个标签。 行内样式优先级最高， 其次是内部样式，最后是外部样式 可读性易维护性：行内样式多之后html文件显得混乱 外部样式最易于维护，其次是内部样式，最后是行内样式。 css选择器： 派生选择器。外层的标签包含里层的标签，通过空格表示层级关系。 eg: li strong{color:red;} ID选择器。标签里实现定义好ID值，语法#开头跟ID名字。 eg: secondp{color:red;} 类class选择器。标签里定义好class值。css中语法，开头跟class值。 css常用标签： background 背景相关background-color 定义背景颜色的background-img:url(“”) 背景图片background-repeat:no-repeat; 背景图片是否可重复background-position:top;background-attachment:fixed;background-size:200px 300px;背景图宽高，长度可以是像素px 也可以用百分比 继承： 外层标签如果定义了一个样式，外层标签所包含的标签都会继承这个样式。 子标签重写一个相同的样式，子标签里的定义优先级更高。 盒子模型： padding:内容和边缘的距离 border:边缘，可以定义style、width、color margin:div块距离其他div块边缘的距离。两个div的外边距重合时，以大的为准。]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>HTML</tag>
        <tag>前端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python自动安装第三方包]]></title>
    <url>%2F2018%2F10%2F26%2FPython%E8%87%AA%E5%8A%A8%E5%AE%89%E8%A3%85%E7%AC%AC%E4%B8%89%E6%96%B9%E5%8C%85%2F</url>
    <content type="text"><![CDATA[介绍昨天突然心血来潮想利用我目前学过的知识一个能自动安装python第三方包的小程序。你肯定想问了，我明明有pip可以实现自动安装第三方包了，为什么还要写这么个看似毫无卵用的东西呢？ 我也不知道，就是心血来潮吧。 但是这个小程序还是有好处的: 他首先是从第三方网站下载了第三方包到本地，然后在使用pip install xxxxx.whl在本地安装第三方包，所以更快，成功率更高。 过程步骤 使用爬虫爬取python第三方包网站， 从网站源代码中提取出所有包名字并存入数据库（使用MongoDB） 根据需求从数据库中查询关键字（也就是需要的包名） 根据包名构建get请求 发送get请求，得到文件 引入os包，执行命令os.system(“pip install 包名”) 遇到的问题 在写爬虫的时候，对XPath的使用还不是很清楚，导致我卡在爬虫这一环节很久。 就是提取包名这一行，想了很久。开始我以为像这样写XPath只能获取到网页源代码中第一个包名。原来，在没有指定特殊属性的时候，XPath会返回所有符合条件的信息，就像上面这个XPath，它返回的就是div class=”pylibs”标签下的li标签下的ul标签下的li标签下的a标签的所有文本信息。这句话虽然有点绕，但是没毛病哦铁汁。 在插入数据库的时候，首先要构造一个列表，然后使用for循环把上面提取到的信息（也就是提取到的包名）展开，然后构造字典，再然后将这些字典添加到列表中。最后使用collection.insert_many()插入到数据库中。 从数据库中查询包名，用到的是正则表达式查询（原来我还不会在MongoDB中使用正则查询。）。 这里find查询方法返回的是一个pymongo对象，使用for循环展开。 在数据库中查询到数据之后拿出来构造url，请求这个url的时候才并没有获得二进制文件 找了半天才发现原来从数据库中提取出来的数据有些特殊符号和我们平时使用的不太一样。这个不仔细看真的很难看出来。我发现从数据库中提取出来的数据中的“-”符号似乎短了一点。原来就是这个符号惹的祸。然后我使用了字符串的replace方法把这个符号替换成了正常的样子。 注意：字符串的replace方法并不是能修改源字符，他其实是一个函数，返回值是替换过后的字符串，所以需要用一个变量来接受它。 如何使用环境：需要安装MongoDB数据库。关于MongoDB怎么使用请看:我的另一篇博客 还需要lxml库和requests库。安装lxml库可以参看：另一篇博客 源代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import requestsimport lxml.htmlfrom pymongo import MongoClientimport osclient = MongoClient()database = client['第三方包']collection = database['包名']def spiderdate(): url = "https://www.lfd.uci.edu/~gohlke/pythonlibs/?tdsourcetag=s_pctim_aiomsg" html = requests.get(url).content#获取网页源代码 selector = lxml.html.fromstring(html) content = selector.xpath('/html/body/ul[@class="pylibs"]/li/ul/li/a/text()')#提取包名 idnum = 1 namelist = [] for name1 in content: """构造字典，插入数据库""" namedict = &#123;'id': idnum, 'name': name1&#125; idnum = idnum+1 namelist.append(namedict) collection.insert_many(namelist)def datefind(): uip = input('请输入包名') content = collection.find(&#123;'name': &#123;'$regex': '&#123;&#125;.*?'.format(uip), '$options': 'i'&#125;&#125;, &#123;'_id': 0&#125;) # collection.fin(&#123;'name': &#123;'$regex': 'xxx', '$options' : 'i'&#125;&#125;) for i in content: print(i['name']) print('请根据自己的python版本和windows版本选择合适的安装包')def download(packname): headers = &#123;'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'zh-CN,zh;q=0.9', 'Connection': 'keep-alive', 'Host': 'download.lfd.uci.edu', 'Referer': 'https://www.lfd.uci.edu/~gohlke/pythonlibs/', 'Upgrade-Insecure-Requests': '1', 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36'&#125; url = "https://download.lfd.uci.edu/pythonlibs/h2ufg7oq/"+packname print(packname) print(url) html = requests.get(url, headers=headers).content#获得文件二进制信息 with open(packname, "wb") as f:#打开文件 f.write(html)#写入二进制 os.system("pip install &#123;&#125;".format(packname))#使用os.system向发送安装指令。num = 1while num &gt; 0: if num == 1: spiderdate() datefind() packname = input('输入包名：') packname = packname.replace("‑", "-") download(packname) else: datefind() packname = input('输入包名：') packname = packname.replace("‑", "-") download(packname) num = num + 1 源代码直接复制到pycharm中，如果我没猜错的话，应该就能正常运行啦！ 总结虽然只有短短的五六十行代码，但这也能算得上是我写的第一个小程序了。收获还是有的呀！ 这五六十行代码就已经让我头皮发麻了，不敢想象以后写那么多代码的样子。]]></content>
      <categories>
        <category>小程序</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XPath学习]]></title>
    <url>%2F2018%2F10%2F24%2FXPath%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[XPath介绍​ XPath（XML Path）是一种查询语言，它在XML( Extensible Markup Language,可标记扩展语言)和HTML的树状结构中寻找节点。形象一点来说，XPath就是一种根据“地址”来”找人“的语言。 ​ 用正则表达式来提取信息，经常会出现不明原因的无法提取想要内容的情况。解决起来很麻烦，需要寻找的内容越复杂，构造正则表达式所需要花费的时间也就越多。而XPath却不一样，熟练使用XPath以后，构造不同的XPath，所需要的时间几乎一样，所以使用XPath从HTML源代码中提取信息可以大大提高效率。 ​ 在Python中，为了使用XPath，需要安装一个第三方库。 安装windows下的安装比较复杂，直接使用pip install lxml会很容易出问题，这是因为lxml的底层时使用C语言实现的，所以计算机上面需要安装Virtual C++运行库。但是即便安装好了运行库，还是有可能出问题。所以我们换一种方法。 打开链接 根据自己计算机的pyhton版本下载对应的whl包。下载完成后吧这个包放到python安装文件夹下的Lib\site-packages文件夹中然后在这个文件夹中打开命令行窗口，运行 pip install 文件名 文件名就是刚才下载的whl文件名，注意，要输入完整文件名包含后缀。 如果使用这种方法还不行的话，那就把whl文件后缀名改成zip然后使用压缩工具进行解压到当前文件夹就可以了。 XPath语法 XPath语句格式 核心思想就是写XPath就是写地址 获取文本： 1//[标签][@属性1=“属性值1”]/标签2[@属性2=“属性值2”]/.../text() 获取属性： 1//[标签][@属性1=“属性值1”]/标签2[@属性2=“属性值2”]/.../@属性n 其中[@属性=“属性值”]不是必须的。它的作用是帮助过滤相同的标签。在不需要过滤相同标签的时候可以省略 标签的选取 1234567891011121314151617181920212223242526import lxml.htmlsource = '''&lt;html&gt; &lt;head&gt; &lt;title&gt;测试&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div class="useful"&gt; &lt;ul&gt; &lt;li class="info"&gt;我需要的信息1&lt;/li&gt; &lt;li class="info"&gt;我需要的信息2&lt;/li&gt; &lt;li class="info"&gt;我需要的信息3&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="useless"&gt; &lt;ul&gt; &lt;li class="info"&gt;垃圾1&lt;/li&gt; &lt;li class="info"&gt;垃圾2&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt;'''selector = lxml.html.fromstring(source)useful = selector.xpath('//div[@class="useful"]/ul/li/text()')#这个时候获取到的是useful标签里的内容 既然是写地址，很多人肯定好奇不是应该写成 /html/body/[@class=”usefli”]/ul/li/text() 实际上这么写也没有错，但是因为我们需要找的是class=”useful”标签里的内容，这个标签在整个HTML代码中已经足够特别了，没有必要再加上html，body标签了，就像我们写快递收货地址一样，没有人会写亚洲，中国，北京。因为大家都知道全世界只有一个北京，在中国。 写XPath最重要的就是找这个标志性的属性值。 那些属性可以省略呢 在上面的代码中因为ul标签本身就没有属性，所以可以省略，那li标签命名有属性为什么也省略了呢？ 这是因为li标签中的属性都一样，全都是li class=”info”所以可以省略 XPath的特殊情况 以相同字符串开头 123456789101112131415161718import lxml.htmlhtml1 = '''&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id="test-1-k"&gt;需要的内容1&lt;/div&gt; &lt;div id="test-2-k"&gt;需要的内容2&lt;/div&gt; &lt;div id="testfault-k"&gt;需要的内容3&lt;/div&gt; &lt;div id="useless"&gt;这是我不需要的内容&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;''' 在上面的HTML代码中，我们需要抓取需要的内容1,2,3,如果不指定div标签的属性，那么就会把不需要的内容也提取出来。但是如果指定了div标签的属性就只能抓取一条信息了。这个时候，就需要用XPath提取所有id以test开头的div标签。 在XPath中，属性以某些字符串开头，可以写为： //标签[starts-with(@属性名,”相同的开头部分”)] 在上面的代码中就可以写成: //div[starts-with(@class,”test”)]/text() 属性值包含相同字符串 寻找属性值包含某些相同字符串的元素时，XPath的写法格式和上面的写法格式是相同的，只不过把关键字starts-with换成了contains。 对XPath返回的对象执行XPath XPath也支持先抓大在抓小，就是先把包含想要的信息的那一大部分内容先使用XPath提取出来，再使用XPath提取一次。 如上面的代码我们下吧div这一部分的标签全部都提取出来 1234selector = lxml.formstring(html1)useful = seletot.xpath('//body')#返回一个列表info_list = useful[0].xpath('div[class="useless"]/text()')print(info_list) 第一个XPath返回的是一个XPath对象在第二次对这个返回对象使用XPath的时候，开头不需要添加斜线，直接以标签名字开始即可。 不同标签下的文字 1234567891011121314151617181920212223242526272829303132html3 = '''&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id="test3"&gt; 我左青龙， &lt;span id="tiger"&gt; 右白虎， &lt;ul&gt;上朱雀， &lt;li&gt;下玄武。&lt;/li&gt; &lt;/ul&gt; 老牛在当中， &lt;/span&gt; 龙头在胸口。 &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;'''#如果使用一般的办法，就会出现获取到的数据不完整的情况selector = lxml.html.fromstring(html3)# content_1 = selector.xpath('//div[@id="test3"]/text()')# for each in content_1:# print(each)# 使用string(.)就可以把数据获取完整data = selector.xpath('//div[@id="test3"]')[0]info = data.xpath('string(.)')print(info) 这段代码其实也是用了先抓大后抓小的方法，先把div标签取出来，但不提取信息，然后在对返回的XPath对象在使用一次XPath，提取这个XPath对象里面的所有字符串。这里用到了一个新的关键字string(.) 使用浏览器开发者工具辅助构造XPath在构造XPath的时候，需要寻找“标志性”的标签。但是网页的源代码往往是混乱的，这个时候依靠肉眼来找就很麻烦了。这时借助开发者工具来构造XPath就能大大提高效率啦！ 打开浏览器，找到我们想要获取的信息 比如我们要获取下图中图片的位置： 鼠标放在图片的位置，然后右键选择检查，就定位到了源代码中图片的位置。 然后右键点击源代码中的位置选择Copy→Copy XPath，然后粘贴下如下： //*[@id=”posts”]/article/div/div[1]/p[1]/a/img 这个XPath写法可以直接被lxml解析。方框中的数字代表这是第几个该标签。如div[1]/p[1]代表这是第一个div标签第一个p标签。这里的数字是从1开始的，可不是像编程语言中从0开始。 这个从浏览器中复制下来的XPath只能获取这一个标签的信息。如果我们想获得这一类标签的信息，例如得到所有图片，就需要将复制下来的XPath作为参考结合网页源代码的结构，手动构造范围更大的更容易读的XPath。]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>XPath</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fiddler抓包工具总结]]></title>
    <url>%2F2018%2F10%2F24%2FFiddler%E6%8A%93%E5%8C%85%E5%B7%A5%E5%85%B7%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[前言我们使用计算机上的浏览器或者客户端软件要与外界进行通信，就必然会有数据的发送或接收，有的时候，我们需要对这些传递的数据进行分析，就需要截获这些传递的数据，其中对这些数据进行截获、重发、编辑、转存的过程叫做抓包。在写爬虫的时候，抓包分析用得相对来说也是较多的，要进行抓包，可以通过一些常见的抓包软件来实现，Fiddler就是一种常见的比较好用的抓包软件。 在写爬虫的时候借助Fiddler能够帮你你模拟出最真实的浏览器请求。 什么是FiddlerFiddler是一种常见的抓包分析软件，同时，我们可以利用Fiddler详细的对HTTP请求进行分析，并模拟对应的HTTP请求。目前抓包软件有很多，除了Fiddler之外，常见的还有： 浏览器自带的调试工具，按f12可以调出。缺点：比较轻量，不能支持一些复杂的抓包。 Wireshark，这是一款通用的抓包工具，功能比较齐全，正因为功能比较齐全，所以较为庞大，而我们写爬虫的时候主要是分析HTTP请求，所以这款软件的很多功能用不到。 爬虫和Fiddler的关系​ 网络爬虫是自动爬取网页的程序，在爬取的过程中必然涉及客户端与服务端之间的通信，自然也需要发送一些HTTP请求，并接受服务器返回的结果。在一些复杂的网络请求中，我们很难看到网址的变化规律，这就很难手动构造来请求来自动爬取网页了。 ​ 比如在浏览一些网页是，浏览到最下面的时候会出现一个‘’加载更多“的字样，此时点击就会加载出更多内容，然而我们观察浏览器中的网站并没有变化，便也无法分析出浏览器向服务器发送了什么数据。 ​ 此时可以使用Fiddler进行抓包，并对这些数据进行分析，这样就可以分析出实现”加载更多“的请求了。 安装和使用Fiddler下载地址从官网下载完成后安装，安装完成后打开 字段说明Fiddler想要抓到数据包，要确保Capture Traffic是开启，在File –&gt; Capture Traffic。开启后再左下角会有显示，当然也可以直接点击左下角的图标来关闭/开启抓包功能。Fiddler开始工作了，抓到的数据包就会显示在列表里面，下面总结了这些都是什么意思： Statistics 请求的性能数据分析随意点击一个请求，就可以看到Statistics关于HTTP请求的性能以及数据分析了 ## Inspectors 查看数据内容Inspectors是用于查看会话的内容，上半部分是请求的内容，下半部分是响应的内容： ## AutoResponder 允许拦截指定规则的请求AutoResponder允许你拦截指定规则的求情，并返回本地资源或Fiddler资源，从而代替服务器响应。 看下图5步，我将“baidu”这个关键字与我电脑“f:\Users\YukiO\Pictures\boy.jpeg”这张图片绑定了，点击Save保存后勾选Enable rules，再访问baidu，就会被劫持。AutoResponder有很多匹配规则： 字符串匹配（默认）：只要包含指定字符串（不区分大小写），全部认为是匹配 正则表达式匹配：以“regex:”开头，使用正则表达式来匹配，这个是区分大小写的Composer 自定义请求发送服务器Composer允许自定义请求发送到服务器，可以手动创建一个新的请求，也可以在会话表中，拖拽一个现有的请求Parsed模式下你只需要提供简单的URLS地址即可（如下图，也可以在RequestBody定制一些属性，如模拟浏览器User-Agent）Filters 请求过滤规则Fiters 是过滤请求用的，左边的窗口不断的更新，当你想看你系统的请求的时候，你刷新一下浏览器，一大片不知道哪来请求，看着碍眼，它还一直刷新你的屏幕。这个时候通过过滤规则来过滤掉那些不想看到的请求。勾选左上角的Use Filters开启过滤器，这里有两个最常用的过滤条件：Zone和Host Zone 指定只显示内网（Intranet）或互联网（Internet）的内容： Host 指定显示某个域名下的会话：如果框框为黄色（如图），表示修改未生效，点击红圈里的文字即可Timeline 请求响应时间在左侧会话窗口点击一个或多个（同时按下 Ctrl 键），Timeline 便会显示指定内容从服务端传输到客户端的时间：Fiddler 设置解密HTTPS的网络数据Fiddler可以通过伪造CA证书来欺骗浏览器和服务器。Fiddler是个很会装逼的好东西，大概原理就是在浏览器面前Fiddler伪装成一个HTTPS服务器，而在真正的HTTPS服务器面前Fiddler又装成浏览器，从而实现解密HTTPS数据包的目的。 解密HTTPS需要手动开启，依次点击： Tools –&gt; Fiddler Options –&gt; HTTPS 勾选Decrypt HTTPS Traffic 点击OKFiddler 抓取Iphone / Android数据包请参考我的另一篇博客 Fiddler 内置命令与断点Fiddler还有一个藏的很深的命令框，平时用的时候很容易忽略FIddler断点功能就是将请求截获下来，但是不发送，这个时候你可以干很多事情，比如说，把包改了，再发送给服务器君。还有balabala一大堆的事情可以做，就不举例子了。示例：?&lt;=@selectclsdump 断点命令断点可以直接点击Fiddler下图的图标位置，就可以设置全部请求的断点，断点的命令可以精确设置需要截获那些请求。如下示例：命令：bpafterbpsbpvg / go 本篇博客借鉴了：链接图片也来自：链接]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Fiddler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫入门]]></title>
    <url>%2F2018%2F10%2F24%2Fpython%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[爬虫基础爬虫是什么？爬虫是什么？一个自动化的数据收集程序爬虫分类？四类1.通用爬虫–什么内容都爬，比如搜索引擎，百度谷歌2.聚焦爬虫–爬取特定内容3.增量式爬虫-爬取更新的内容4.深层网络爬虫-爬取提交表单后的数据通用爬虫弊端： 通用搜索引擎返回太多没有用的数据 服务器资源有限，而网络数据资源无限 网络资源多样化，通用爬虫模型无法很好获取。 爬虫不是独立的技术–需要：前端开发基础知识 数据库基础 网络基础 抓包分析能力 爬虫框架 聚焦爬虫介绍：聚焦爬虫是有目的的爬取，可以节省大量的服务器资源和带宽资源，具有很强的实用性。流程：首先聚焦爬虫有一个控制中心，该控制中心负责对整个爬虫系统进行管理和监控，主要包括控制用户交互、初始化爬行器、确定主题、协调各模块之间的工作、控制爬行过程等方面。然后，将初始的URL集合传递给URL队列，页面爬行模块会从URL队列中读取第一批URL列表，然后根据这些URL地址从互联网中进行相应的页面爬取。爬取后，将爬取到的内容船到页面数据库中存储，同时，在爬行过程中，会爬取到一些新的URL，此时，需要根据我们所定的主题使用连接过滤模块过滤掉无关连接，再将剩下来的URL连接根据主题使用链接评价模块或内容评价模块进行优先级的排序。完成后，将新的URL地址传递到URL队列中，供页面爬行模块使用。另一方面，将页面爬取并存放到页面数据库后，需要根据主题使用页面分析模块对爬取到的页面进行页面分析处理，并根据处理结果机那里索引数据库，用户检索对应信息是，可以从索引数据库中进行相应的检索，并得到对应的结果。 网络爬虫可以干什么？ 爬取多站新闻，集中阅读 爬取金融信息，进行投资分析 制作搜索引擎 爬取图片 爬取网站用户公开的信息进行分析 自动去网页广告 爬取用户公开的联系方式 聚焦网络爬虫聚焦网络爬虫，由于其需要有目的的进行爬取，所以对于通用网络爬虫来说，必须要增加目标的定义和过滤机制，具体来说，此时其执行原理和过程需要比通用网络爬虫多出三步，即目标的定义、无关链接的过滤、下一步要爬取的URL地址的选取。常见的网络更新策略有三种：用户体验策略、历史数据策略、聚类分析策略聚类分析可以分解商品之间的共性进行相应的处理，将共性较多的商品聚为一类。在爬虫对网页爬取的过程中，爬虫需要必须需要访问对应的网页，此时，正规的爬虫会告诉网站站长其爬虫身份。网站的管理员则可以通过爬虫告知的身份信息对爬虫的身份进行识别，开发网络爬虫的语言有很多，常见的有python、java、PHP、Node.js、C++、Go语言等。 urllib库Urllib库是Python中的一个功能强大、用于操作URL，并在做爬虫的时候经常要用到的库。URL读取内容有三种方式：read；读取全部并且把内容赋给一个字符串变量、readline读取一行、readlines读取全部并且把内容赋给一个列表变量1234567imoprt urllib.requestfile=urllib.rqquest.urlopen("网址")data=file.read()print(data)#读取网站信息并打印file1 = open('保存路径及地址’,'wb')#将网页信息以网页的形式保存到本地。file1.write(data)#将变量data写入文件中file1.close#关闭文件 除了以上方法之外还可以使用urllib.request里面的urlretrieve()函数直接将对应信息写入本地文件。格式：urllib.request.urlretrieve(url,filename=本地文件地址)。例子:1234567urllib.request.urlretrieve("http://www.baidu.com",filename="D:/a/index.html)# 注：urlretrieve执行的过程中会产生一些缓存，我们要清除这些缓存信息，可以使用urlcleanup()进行清除eg：urllib.request.urlcleanup()# 除此之外urllib中还有一些常见的用法：file.info()#返回与当前环境相关的信息。file表示当前爬取的网页，file=urllib.request.urlopen(网址)flie.getcode()#返回状态码，若返回200为正确。file.geturl()#返回当前爬取的URL地址 url编解码:url标准中只会允许一部分ASCII字符，而其他的一些字符，比如汉字等，是不符合URL标准的。所以如果我们在URL中使用一些其它不符合标准的字符就会出现问题，此时西药进行URL编码方可解决。1234编码:urllib.request.quote()eg:a = urllib.request.quote("http://www.baidu.com")解码：urllib.request.unquote()eg:urllib.request.unquote(a) 浏览器的模拟——Headres属性当遇到无法爬取的网页时会用到headres属性，因为有些网站为了防止别人恶意采集信息进行了一些反爬虫的设置。这时候就可以设置一些Headres信息，模拟成浏览器去访问这些网站，模拟成浏览器可以设置User-Agent信息。获取User-Agent信息：打开任意一个网站F12调出开发者工具network，任意点击一个链接，使网页发生一个动作。观察下方的窗口会出现一些数据，随意点击一个数据在后侧Headres中可以看到对应的头信息，往下拖动，可以找到User-Agent字样的一串信息。例如：User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36爬虫模拟成浏览器访问页面的设置方法： 方法1,：使用build_opener()修改报头]由于urlopen()不支持一些HTTP的高级功能，所以，我们如果要修改报头，可以使用urllib.request.build_opener()进行eg: 123456import urllib.requesturl="要爬取的网址"headers=("User-Agent","Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36")opener = urllib.request.build_opener()opener.addheaders = [headers]data = opener.open(url).read() 上述代码中，首先，我们定义了一个变量url存储要爬取的网址，然后在定义一个变量headers存储对应的User-Agent信息，定义的格式为(“User-Agent”,具体信息)，具体信息我们已经从浏览器中获取了，该信息获取一次即可，以后再爬取其他网站的时候可以直接用。然后我们需要使用urllib.request.build_opener()创建自定义的opener对象并赋给变量opener，接下来，设置opener对象的addheaders，即设置对应的头信息，设置格式为：”opener对象名.addheaders=[头信息]”，设置好头信息之后，我们就可以使用opener对象的open()方法打开对应的网址了。此时，打开操作已经是具有头信息的打开操作行为，即会模仿为浏览器去打开，使用格式是：opener对象名.open(url地址)打开后再使用read()方法读取对应数据，并赋给data变量. 方法2：使用add_header()添加报头]import urllib.requesturl=网址req=url.request.Request(url)#创建Request对象并赋给变量reqreq.add.header(‘User-Agent’,’Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36’)#添加报头信息data=urllib.request.urlopen(req).read()#打开对应网址并读取网页内容赋给data 超时设置有的时候，我们访问一个网页，如果网页长时间未响应，那么系统就会怕短该网页超时了，即无法打开该网页。有的时候，我们需要根据自己的需要来设置超时的时间值。网站反应快就时间值就设置短一点，网站反应慢就设置长一点。eg：将网站超时时间设置为1 12345678import urllib.requestfor i in range(1,100): try: file=urllib.request.urlopen(&quot;http://www.baidu.com&quot;,timeout=1) data=file.read() print(len(data)) except Exception as e: print(&quot;出现异常--&quot;+str(e)) 上述代码循环了99次每次都输出获得数据的长度，执行之后可以发现有几次出现了异常，这是因为我们设置了超时，网站在1秒钟之内没有做出回应的话代码自动判定为超时异常，并输出异常信息。 HTTP协议请求HTTP协议请求主要分为6种类型： GET请求，GET请求会通过URL网址传递信息，可以直接在URL中写上要传递的信息，也可以由表单进行传递。如果使用表单进行传递，表单中的信息会自动转为URL地址中的信息，通过URL地址传递。 POST请求，可以向服务器提交数据，是一种比较主流也比较安全的数据传递方式，比如在登录时，经常使用POST请求发送数据。 PUL请求，请求服务器存储一个资源，通常要制定存储的位置。 DELETE请求，请求服务器删除一个资源 HEAD请求，请求获取对应的HTTP报头信息。 OPTIONS请求，可以获得当前URL所之气的请求类型。 get请求：我们可以构造一个url让代码去获取我们想要的信息。 123456789import urllib.requestkey="aa"url="http://www.baidu.com/s?wd="+key#构造urlreq=urllib.request.Request(url)#创建一个Request对象并赋给reqdata=urllib.request.urlopen(req).read()#获取网页信息并读取出来赋给data变量。print(len(data))file1=open("D:/a1/xx1.html","wb")file1.write(data)#将信息写入xx1.html文件。file1.close() 上述代码有不完善的地方，如果我们要检索的关键词是中文，就会报错。这时就可以用到urllib.request.quote()函数来编码解决。123key="中文"new_key=urllib.request.quote(key)url="http://www.baidu.com/s?wd="+new_key 总结：使用URL请求： 构造URL地址 构造Request对象 打开Request对象。 读取网页内容、将内容写入文件。如果参数中含有中文要使用quote函数对参数来编码。 post请求post请求常用于登录、注册页面 12345678910import urllib.requestimport urllib.parseurl = "http://www.iqianyue.com/mypost/"postdata = urllib.parse.urlencode(&#123;'name':'yudeqiang','pass':'123456'&#125;).encode('utf-8')#将数据使用urlencode编码处理后，使用encode()设置为utf-8编码req = urllib.request.Request(url,postdata)#构建Request对象参数包括url和要传递的数据#添加头信息模拟浏览器进行爬取req.add_header("User_Agent","Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36")data=urllib.request.urlopen(req).read()print(len(data)) 代理服务器的设置。有时使用同一个ip去爬取同一个网站上的网页，久了之后会被该网站服务器屏蔽，使用代理服务器就可以解决这个问题。eg： 12345678910def user_proxy(proxy_addr,url): import urllib.request proxy = urllib.request.ProxyHandler(&#123;'http':proxy_addr&#125;) opener = urllib.request.build_opener(proxy,urllib.request.HTTPHandler) urllib.request.install_opener(opener) data = urllib.request.urlopen(url).read().decode('utf-8') return dataproxy_addr = "14.215.194.75:34397"data = user_proxy(proxy_addr,"http://www.baidu.com")print(len(data)) 我们首先建立了一个名为use_proxy的自定义函数，该函数主要实现使用代理服务器来爬取某个URL网页的功能。在函数中，我们设置两个形参，第一个形参为代理服务的地址，第二个形参代表要爬取的网页的地址。然后，使用urllib.request.ProxyHandler()来设置对应的代理服务器信息，设置格式为：urllib.request.ProxyHandler({‘http’:代理服务器地址})，接下来，使用urllib.request.build_opener()创建了一个自定义的opener对象，第一个参数为代理信息，第二个参数为urllib.request.HTTPHandler类为了方便，可以使用urllib.request.install_opener()创建全局默认的opener对象，所以下面才可以直接使用urllib.request.urlopen()打开对应网址爬取网页并读取，编码后赋给变量data，最后返回data的值给函数。随后，在函数外设置好对应的代理IP地址，然后嗲用地应以函数use_proxy，并传递两个实参，跟别为使用的代理地址及要爬取的网址。将函数的调用结果直接赋值给变量data，并输出data内容的长度。或者也可以将data的值写进某个文件中存储起来。 DebugLog实战123456import urllib.requesthttphd = urllib.request.HTTPHandler(debuglevel=1)httpshd = urllib.request.HTTPHandler(debuglevel=1)opener = urllib.request.build_opener(httphd,httpshd)urllib.request.install_opener(opener)data=urllib.request.urlopen("http://edu.51cto.com") 如何开启DebugLog？思路如下： 分别使用urllib.request.HTTPHandler()和urllib.request.HTTPSHandler()将debuglevel设置为1 使用urllib.request.build_opener()创建自定义的opener对象，并使用1中设置的值作为默认参数。 用urllib.request.install_opener()创建全局默认的opener对象，这样在使用urlopen()时也会使用我们安装的opener对象 进行后续相应的操作。 异常处理神器——URLError实战程序再执行的过程中，难免会发生异常，常见的异常处理方法是try except1234567import urllib.requestimport urllib.errortry: urllib.request.urlopen("http://ww1w.baidu.com")except urllib.error.URLError as e: print(e.code) print(e.reson)]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
        <tag>urllib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP理论基础]]></title>
    <url>%2F2018%2F10%2F24%2FHTTP%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[HTTP理论基础在学习HTTP协议之前，我们首先需要了解计算机网络体系结构：OSI模型和TCP/IP参考模型，现在主流的计算机网络体系是TCP/IP，而HTTP协议则是TCP/IP应用层的协议，用于获取万维网上的网页信息。也是我们写爬虫必须要了解的协议~ 计算机网络体系结构OSI模型：osi参考模型讲计算机网络分为七层：（从低到高）物理层、数据链路层、网络层、传输层、会话层、表示层、应用层。TCP/IP参考模型：TCP IP 被分为4层模型，分别是：网络接口层、网际层、传输层、应用层。 它把OSI模型的应用层表示层会话层合成应用层、数据链路层和物理层合为网络接口层。 网络接口层：由物理层和数据链路层合并而成。这层定义了与不同的网络进行连接的接口，网络接口层负责吧IP数据包发送到网络传输介质(双绞线、光纤等) 上传输，以及从网络传输介质上接收数据并解封，取出数据包并交给上一层网际层。 网际层：主要功能是负责将数据封装成包，并从源主机发送到目的主机，解决如何进行数据包的路由选择、阻塞控制、网络互连等问题。IP协议：网间互联协议 网际层的核心协议，另外还有一些辅助协议，包括ARP(地址解析协议)、RARP(逆向地址解析协议)、ICMP(网间控制报文协议)以及IGMP(互联网组管理协议)协议等。负责ip数据报在网络间寻址。IP协议可以进行IP数据包的分割与封装，封装前在数据包前加上源主机的ip地址和目的主机的ip地址及其他信息。特点：只提供传输，不负责纠错。ARP:负责将IP地址解析为物理地址，以便按该地址发送和接收数据。RARP:负责将物理地址解析为IP地址。ICMP:用于在主机和路由器之间传递控制消息，指出网络不通、为用户群进行软件升级、共享白板式多媒体应用等，这些情况就是多播。IGMP:负责对IP多播组进行管理，包括多播组成员的加入和删除等。 传输层：负责在源主机和目的主机的应用进程之间提供端到端的数据传输服务。负责数据分段、数据确认、丢失和重传等。 TCP:传输控制协议 是一个可靠的、面向连接的端对端的传输层协议，由TCP提供的连接叫虚连接。在发送方，TCP将用户提交的字节流分割成 若干个数据段并传递给网际层进行打包发送；在接收方，TCP将所接收的数据包重新装配并交付给用户，它通过序列确认及包重发机制解决IP协议传输时的错误 UDP:用户数据报协议 是一个不可靠的、面向无连接的出传输层协议。使用UDP协议发送报文之后无法得知其是否安全到达。UDP协议将可靠性问题交给应用程序来 解决。UDP协议应用于对那些可靠性要求不高，但要求网络延迟较小的场合，如语音和视频数据的传送。 -端口：为了识别传输层之上的各个不同的网络应用进程，传输层引入了端口的概念。要进行网络通信的进程向系统提出申请，系统返回一个唯一的端口号，将 进程和端口号联系在一起，成为绑定。传输层使用其报文头中的端口号，吧收到的数据送到不同的应用程序。端口是一种软件结构，包括一些 数据结构和I/O缓冲区。一些端口经常会被黑客、木马病毒所利用。 应用层：TCPIP的应用层综合了OSI应用层、表示层、以及会话层的功能。应用层为用户的应用程序提供了访问网络服务的能力并定义了不同主机上的应用程序之间交换用户 数据的一系列协议。由于不同的网络对网络服务的需求各不相同，因此应用层协议非常丰富，并且不断有新的协议加入， 应用层的常用协议： 超文本传输协议：HTTP 用于获取万维网上的网页信息 文件传输协议：FTP 用于点对点的文件传输 简单邮件传输协议：SMTP 用于发送邮件以及在邮件服务器之间转发邮件 邮局协议：POP用于重邮件服务器上获取邮件 仿真终端协议：TELNET 用于远程登录到网络主机 域名系统：DNS 域名解析 用于将主机域名解析为IP地址 简单网络管理协议：SNMP 用于从网络设备（路由器、网桥、集线器等）中收集网络管理信息。 TCPIP总结网络体系结构：计算机网络的层次及各层协议和层间接口的集合被称为网络结构IP提供的主要功能：1、寻址与路由2、数据包分割和重组，在数据包前加上源主机和目的主机的IP地址。网络接口层负责吧IP数据包发送到网络传输介质上传输，以及从网络传输介质上接收数据并解析，取出数据包交给网际层，网际层负责将数据封装成包，并从源主机发送到目的主机，解决的是数据包的路由选择、阻塞控制和网络互连等问题传输层负责在源主机和目的主机的应用程序之间提端到端的数据传输服务，负责数据分段、数据确认、丢失和重传等。应用层为用户的应用程序提供了访问网络服务的能力并定义了不同主机的应用程序之间交换用户数据的一系列协议。简单来说就是网络接口层吧数据送到网络传输介质上（网线）再把数据取出交给网际层，网际层封装数据并发送，传输层确保传输的数据是正确的应用层使用传输的各种数据在用户的应用程序上进行数据的交换等。 书上的总结：TCP协议先把数据分成若干数据报，并给每个数据加上一个TCP信封，上面写上数据报的编号，以便在接收端吧数据还原成原来的格式。IP协议把每个TCP信封再套上一个信封，在上面写上接收主机的地址。有了IP信封就可以在物理网络上传送数据。IP协议还具有利用路由算法进行路由选择的功能。这些信封可以通过不同的传输途径（路由）进行传输，由于路径不同以及其他原因，可能出现顺序颠倒、数据丢失、数据重复等问题。这些问题由TCP协议来处理，其具有检查和处理错误的功能，必要时还可以请求发送端重发。因此可以说IP协议负责数据的传输，TCP协议负责数据的可靠传输。 HTTP是应用层的协议，用于获取万维网上的网页信息。当我们浏览器输入百度网址后： 客户端请求：浏览器访问网址 www.baidu.com DNS：找到网址对应的IP地址 HTTP：请求的资源+请求的内容+请求IP地址等。 TCP:HTTP报文拆装成多个TCP报文，TCP报文按照三次握手可靠的传输。 IP：网络传输过程中 选择路径、进行中转。 服务端接收到了多个TCP报文。 服务端把接收到的TCP报文合并了http报文。读取到了信息。 服务端接受信息并返回资源给客户端，过程跟上面步骤一致。 http超文本传输协议）是一个基于请求与响应模式的、无状态的、应用层的协议，常基于TCP的连接方式，HTTP1.1版本中给出一种持续连接的机制，绝大多数的Web开发，都是构建在HTTP协议之上的Web应用。 HTTP URL (URL是一种特殊类型的URI，包含了用于查找某个资源的足够的信息)的格式如下：http://host[&quot;:&quot;port][abs_path]http表示要通过HTTP协议来定位网络资源；host表示合法的Internet主机域名或者IP地址；port指定一个端口号，为空则使用缺省端口80；abs_path指定请求资源的URI；如果URL中没有给出abs_path，那么当它作为请求URI时，必须以“/”的形式给出，通常这个工作浏览器自动帮我们完成。eg:​ 输入：www.baidu.edu.cn​ 浏览器自动转换成：http://www.baidu.edu.cn/ HTTP协议–请求 http请求由三部分组成，分别是：请求行、消息报头、请求正文 请求方法（所有方法全为大写）有多种，各个方法的解释如下： GET 请求获取Request-URI所标识的资源 POST 在Request-URI所标识的资源后附加新的数据 HEAD 请求获取由Request-URI所标识的资源的响应消息 报头 PUT 请求服务器存储一个资源，并用Request-URI作为其标识 DELETE 请求服务器删除Request-URI所标识的资源 TRACE 请求服务器回送收到的请求信息，主要用于测试或诊断 CONNECT 保留将来使用 OPTIONS 请求查询服务器的性能，或者查询与资源相关的选项和需求​ HTTP协议–响应 在接收和解释请求消息后，服务器返回一个HTTP响应消息。 HTTP响应也是由三个部分组成，分别是：状态行、消息报头、响应正文 状态代码有三位数字组成，第一个数字定义了响应的类别，且有五种可能取值： 1xx：指示信息–表示请求已接收，继续处理 2xx：成功–表示请求已被成功接收、理解、接受 3xx：重定向–要完成请求必须进行更进一步的操作 4xx：客户端错误–请求有语法错误或请求无法实现 5xx：服务器端错误–服务器未能实现合法的请求常见状态代码、状态描述、说明： 200 OK //客户端请求成功 400 Bad Request //客户端请求有语法错误，不能被服务器所理解 401 Unauthorized //请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用 403 Forbidden //服务器收到请求，但是拒绝提供服务 404 Not Found //请求资源不存在，eg：输入了错误的URL 500 Internal Server Error //服务器发生不可预期的错误 503 Server Unavailable //服务器当前不能处理客户端的请求，一段时间后可能恢复正常 ​ IP地址Internet网络中所有计算机均称为主机，ip地址由网络号和主机表示组成，目前使用的ipv4协议规定ip地址的长度为32位。一般以4个字节表示，每个字节用十进制表示，所以每个字节的取值是0-255，并且每个字节数之间用.割开，这种记录方法称为“点-分”十进制记号法。网络地址可分为五类 A类：分配给主要的服务提供商。IP地址的前八位二进制数代表网络类型 取值范围是0-127 B类：分配给拥有大型网络的机构。。。。。16。。。。。。。。。。。。。。。。128-191 C类：。。。。。。。。小型网络。。。。。24。。。。。。。。。。。。。。。。192-223 D类：为多路广播保留。取值224-239 E类：实验性地址，保留未用240-247 IP地址结构：网络类型+网络号+主机ID特殊IP：127.0.0.1 localhost本地主机 自己的电脑0.0.0.0 不是ip 请求这个地址代表所有请求被丢弃由于计算机的发展ipv4能表示的ip地址越来越紧张 且网络号即将用尽。所以产生了ipv6协议ipv6使用128位地址。支持的地址数是足够用的。 子网掩码子网掩码(subnet mask)：又叫网络掩码、地址掩码、子网络遮罩，它是一种用来指明一个IP地址的哪些位标识的是主机所在的子网，以及哪些位标识的是主机的位掩码。子网掩码不能单独存在，它必须结合IP地址一起使用。子网掩码只有一个作用，就是将某个IP地址划分成网络地址和主机地址两部分。子网掩码是一个32位地址，用于屏蔽IP地址的一部分以区别网络标识和主机标识，并说明该IP地址是在局域网上，还是在远程网上。子网掩码的设定必须遵循一定的规则。与二进制IP地址相同，子网掩码由1和0组成，且1和0分别连续。子网掩码的长度也是32位，左边是网络位，用二进制数字“1”表示，1的数目等于网络位的长度；右边是主机位，用二进制数字“0”表示，0的数目等于主机位的长度。这样做的目的是为了让掩码与ip地址做按位与运算时用0遮住原主机数，而不改变原网络段数字，而且很容易通过0的位数确定子网的主机数（2的主机位数次方-2，因为主机号全为1时表示该网络广播地址，全为0时表示该网络的网络号，这是两个特殊地址）。只有通过子网掩码，才能表明一台主机所在的子网与其他子网的关系，使网络正常工作。 网关网关(Gateway)：又称网间连接器、协议转换器。网关在网络层以上实现网络互连，是最复杂的网络互连设备，仅用于两个高层协议不同的网络互连。网关既可以用于广域网互连，也可以用于局域网互连。 网关是一种充当转换重任的计算机系统或设备。使用在不同的通信协议、数据格式或语言，甚至体系结构完全不同的两种系统之间，网关是一个翻译器。与网桥只是简单地传达信息不同，网关对收到的信息要重新打包，以适应目的系统的需求。同层–应用层。 DHCPDHCP协议：路由器自动为局域网下的客户端分配局域网ip。好处是比较方便不用手动设置ip。 DNS域名解析协议 将域名转化为ip地址 原创文章，转载请注明]]></content>
      <categories>
        <category>HTTP</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
        <tag>TCP/IP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日志2018.10.23]]></title>
    <url>%2F2018%2F10%2F23%2F%E6%97%A5%E5%BF%972018-10-23%2F</url>
    <content type="text"><![CDATA[问题： 上午在使用requests.Session()获取登录后的知乎首页时遇到了问题，知乎首页不知道用了什么编码，获取到数据后使用UTF-8/GBK都无法正常解码，都是一堆乱码。现在这个问题还没有解决，手足无措了。 下午学习scrapy框架时也遇到了很多问题，安装和运行都很麻烦，我打算在学习完这个框架之后再来总结。 收获： 在爬取知乎首页时，学习到了decode()的第二个参数，ignore 在获取到网页的数据后，有一些特殊字符导致不能使用decode解码 在这里我直接加上了ignore参数忽略了不能解码的特殊字符 scrapy: scrapy的安装是真的麻烦： 首先要安装Visual C++ Build Tools，因为接下来要安装的两个文件的底层是基于C语言开发的，所以需要C语言的编译环境 下载地址有可能会提示下载.NET Framework下载地址 安装pywin32,下载地址下载.whl文件后使用pip install 文件名(包含后缀) 安装Twisted,下载地址安装方法同上 然后就是pip install scrapy了 使用： 创建工程命令scrapy startproject 名称 cd 名称 scrapy genspider 爬虫名 要爬的网址 eg:scrapy genspider explam baidu.com 修改settings文件，修改ROBOTSTXT_OBEY=False python执行SCRAPY SHELL 提示DEF WRITE(SELF, DATA, ASYNC=FALSE)出错的问题解决： 只需要把python里面manhole.py文本的所有的async替换成其他名字就行，修改python\Lib\site-packages\twisted\conch\manhole.py文件 把这里的所有async改个名字就行了]]></content>
      <categories>
        <category>日志</category>
      </categories>
      <tags>
        <tag>学习</tag>
        <tag>日志</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Fiddler对手机APP抓包]]></title>
    <url>%2F2018%2F10%2F23%2F%E4%BD%BF%E7%94%A8Fiddler%E5%AF%B9%E6%89%8B%E6%9C%BAAPP%E6%8A%93%E5%8C%85%2F</url>
    <content type="text"><![CDATA[介绍Fiddler是一款抓包工具，具有很强大的功能，使用Fiddler不仅可以轻松抓取电脑端的数据包，还可以抓取手机、ipad的数据包。 使用在Fiddler官网下载并安装Fiddler，安装后打开界面如下： 因为fiddler抓包的原理就是通过代理，所以被测终端需要和安装fiddler的电脑在同一个局域网中。 开启Fiddler的远程连接，Fiddler 主菜单 Tools -&gt; Fiddler Options…-&gt; Connections页签，选中Allowremote computers to connect，并记住端口号为8888，等会设置手机代理时需要。设置好后重启fiddler保证设置生效。设置如下： 接下来要做的就是手机端的设置啦： 手机和电脑必须在同一个局域网内，然后打开wifi，进行以下设置： 代理服务器主机名就是电脑ip，端口就是Fiddler监听端口8888 然后在手机上点击任意app就可以看到有请求在Fiddler上面流动了 Fiddler的强大不止于此，Fiddler还能做很多事情，目前还在学习中。 原创文章，转载请注明出处！]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Fiddler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习记录2018.10.22]]></title>
    <url>%2F2018%2F10%2F22%2F%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%952018-10-22%2F</url>
    <content type="text"><![CDATA[随便写写现在已经晚上了，目前为止今天还没有学习新知识。不知道为什么，每次放完假回来就得一天时间来调整学习状态，说到底还是懒。。。 上个周末，英雄联盟s8全球总决赛已经来到了1/4决赛。对于我这个loler来说自然很关注，甚至周六上自习的时候都在偷偷看比赛直播。我玩英雄联盟差不多四年了，s4开始，现在已经s8了。我对这个游戏真是又爱又恨，恨的是自己在这个游戏上面投入了太多时间，说网瘾少年真的一点都不为过，但这个游戏也带给了我很多快乐，对于我们这样一群人来说，娱乐就是几朋友一块开黑玩几把，互相嘴臭。这届世界赛频频爆冷，世界第一赛区，从s3开始统治英雄联盟的最强赛区LCK竟然全部倒在了8强，而我们LPL也仅剩1支开始不被多数人看好的IG挺进了4强。分析这届总决赛，不难发现，传统的打法已经不适应版本了，而LCK，LPL的大多数队伍都还在固步自封，不肯接受新的东西，这才导致了这么多冷门的爆出。最令我失望的就是RNG了，这个从春天开始包揽所有冠军的队伍，竟然倒在了8强。当然我也不是喷子，我也不想喷RNG的表现有多糟糕，也没有心思去贴吧微博骂人。虽然这只是个在外人看来玩物丧志的游戏比赛，但是我从中也明白了很多。骄兵必败，这句话用来形容RNG在合适不过了，这支RNG今年拿了太多冠军，广告代言接到手软。就在S赛期间，UZI又拿到了NIKE的代言 这个英雄联盟的ADC选手越来越商业化了。RNG在今年也拿到了很多大牌赞助，奔驰，惠普，谷粒多，这或许也为今年的总决赛惨败埋下了伏笔。全员膨胀，自认为对手G2与自己实力天差地别，人家实力确实跟你有差距，但没有这么离谱吧，把把把对面不当人，从教练到选手。结果就是送给观众一场屎一样的BO5，也已RNG的惨败结束。RNG不是没有实力赢下G2，赛前大部分人（包括解说）都预测3:0带走对面的，结果队员无限膨胀，不认真对待比赛，以惨败收场。总结下来就是RNG输给了骄傲，输给了自己的态度。现在想想8强抽签时候的那副嘴脸真是充满了讽刺。 我突然想到了易大师说的话： 不要被骄傲遮蔽了双眼 真正的大师永远都怀着一颗学徒的心 其实关于这个比赛还有很多事情能给我们带来很多启示。 等等，我这个题目不是学习记录吗，怎么扯到游戏上了，我佛了。关于游戏，要是让我放开了讲，我能给你说一天一夜，我就不多BB了，毕竟学习现在才是我该干的事情。 记录下学习中遇到的问题吧今天我好像真的没有看一点新知识。那就记录下上周六写爬虫吧，上周六闲来无事，写了个小爬虫。在写这个爬虫的时候也是遇到了很多问题值得我记录下。首先，关于一个请求返回的数据，有些异步请求返回的是json数据，就像腾讯视频的视频评论就是json数据，这里就需要用到json解析数据才能作为Python的字典使用了，开始我一直想用正则表达式把这个jaon提取出来，因为在json数据之前还包含了一串没用的字符串导致这个数据不能直接使用json.loads()解析，正则表达式试了半天，就是无法剔除那个无关信息，还包括括号，正则表达式对于现在的我来说确实有点困难。不得已向前辈请教，他一语道破天机“为什么不试试字符串切片呢？”对啊，这个问题用个简单的切片就完事了，我还在苦苦寻求正则表达式，这也启示了我解决问题的方式。明明有简单的方法，怎么没想到呢；下次再遇到类似的问题先想想除了第一个想到的解决方法之外还有没有其他的方法。 还有就是我在把数据写进数据库的时候，提示错误，key不能包含 . 我又无奈了，还是去请教，大佬说replace可以吧 . 替换成其他字符，我一想replace不是字符串的方法吗，于是马上提出疑问，结果大佬一句话让我哑口无言，key不就是字符串吗？ 是啊，我咋又没想到呢？= =真的是醉了。希望自己吃一堑，长一智，能够记住这些问题吧。]]></content>
      <categories>
        <category>日志</category>
      </categories>
      <tags>
        <tag>学习</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Seleniun获取A_JAX数据]]></title>
    <url>%2F2018%2F10%2F19%2Fa-jax%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[前言有一些网站在发起AJAX请求的时候，会带上特殊的字符串用于身份验证。这种字符串称为Token。比如说会带上时间戳，时间戳精确到毫秒。还会跟另一个属性存在某种一一对应的关系，虽然这种对应关系的算法会写在网站的某一个JavaScript文件中，但是要想读懂这种关系需要深厚的JavaScript功底。如果一个网站只需要爬一次，或者对爬取速度没有什么要求，那么可以通过另一种方式来解决这种问题。本文就来介绍这种方法是如何实现的! SeleniumSelenium介绍虽然网页的源代码中无法看到被异步加载的内容，但是Chrome浏览器开发者工具的’Elements’选项卡下却可以看到网页上的内容。这就说明Chrom开发者工具Elements选项卡里的HTML代码和网页源代码中的HTML代码是不一样的。在开发者工具中，此时显示的内容是已经加载完成的内容。如果能够获得这个被加载的内容，那么就能绕过手动构造Token的过程，可以直接使用XPath或者正则来获得想要的内容。这种情况下，就需要使用Selenium操作浏览器来解析JavaScript，在爬取被解析以后的代码。Selenium是一个网页自动化测试工具，可以通过代码来操作网页上的各个元素。Selenium是Python中的第三方库，可以实现用Python来操作网页。 安装pip install selenium下载ChromDriver，根据自己的系统选择合适的版本：下载下来的是一个zip文件，解压完成后得到一个exe文件接下来就是使用了 使用将上面解压得到的chromedriver.exe与代码放在同一个文件夹中以方便代码直接调用。初始化Selenium，导入selenium库，在指定WebDriver如果chromedriver与代码不在同一个文件夹中可以通过绝对路径来指定，需要注意的是在Windows中路径的分隔符’\’和Python中的转义字符’\’冲突，所以在指明绝对路径的时候可以在路径字符串左引号的左边加一个‘r’符号：1driver = webdriver.Chrome(r&apos;D:\user\asd\chromedriver&apos;) 这样Python就能正确处理反斜杠的问题。初始化完成以后下面第7行的代码就是使用selenium打开网页啦。代码运行以后会自动打开一个chrome浏览器窗口，并自动打开这个网址对应的页面。一旦被异步加载的内容出现在了这个窗口中，那么这是使用：1html = driver.page_source 就能得到在开发者工具中出现的HTML代码。如下图所示：上图中标明的第6行代码设置了一个5s的延迟，这是由于selenium并不会等待网页加载完成在执行后面的代码。它是向ChromeDriver发送了一个命令，让ChromeDriver打开某个网页。至于网页需要多久打开，多久才能完成异步加载，这些selenium并不管，所以才需要设置一个延迟等待异步加载完成之后再抓取网页信息。这样手动设置延迟的方式很浪费时间资源，并且如果在指定的延迟时间内网页还没有加载出来，那么就获取不到网页信息了。怎么让selenium智能化呢？请看以下代码1234567891011121314from selenium import webdriverfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support import expected_conditions as ECdriver = webdriver.Chrome(&apos;./chromedriver&apos;)driver.get(&apos;http://exercise.kingname.info/exercise_advanced_ajax.html&apos;)try: WebDriverWait(driver, 30).until(EC.text_to_be_present_in_element((By.CLASS_NAME, &quot;content&quot;), &apos;通关&apos;))except: print(&apos;网页加载太慢&apos;)element = driver.find_element_by_xpath(&apos;//div[@class=&quot;content&quot;]&apos;)print(element.text) WebDriverWait(driver, 30).until(EC.text_to_be_present_in_element((By.CLASS_NAME, “content”), ‘通关’))WebDriverWait会阻塞程序的运行，30表示最多等待30s。在这30秒内，每0.5秒检查一次网页，直到expected_conditions（EC）期待条件，这个条件就是text_to_be_present_in_element((By.CLASS_NAME, “content”)，（注释：class为content的元素里面的文本中包含了通关两个子）出现。所以这行代码的完整意思就是等待网页加载，直到class为countent的元素里面包含了”通关”两个汉字。By除了指定class之外，还可以指定很多其他的属性，例如： By.ID By.NAME 当然，也可以使用XPth:1EC.present_of_element_located((By.XPATH,&apos;//div[@class=&quot;content&quot;])) 需要注意的是：”present_of_element_located”的参数是一个元组，元组第0项为By.XX，第1项为具体内容，”text_to_be_present_in_element”的参数有两个第一个参数为一个元组，元组第0项为By.XX，第1项为具体内容；第二个参数为部分或全部文本，又或者是一段正则表达式。 获取元素在网页中寻找需要的内容，可以使用XPath12element = driver.find_element_by_xpath(&apos;//div[@class=&quot;content&quot;]&apos;)print(element.text) driver.find_element_by_xpath如果有多个符合条件的返回第一个driver.find_elements_by_xpath以列表形式返回所有的符合条件的element推荐使用driver.find_elements_by_xpath因为driver.find_element_by_xpath返回的是一个Element对象，如果没有符合条件的元素，就会报错，而driver.find_elements_by_xpath返回的是一个Element对象列表，就算没有符合条件的元素也会返回一个空列表，不会报错。因为driver.find_element_by_xpath返回的是一个Element对象,通过它的.text属性才能获取到文本信息，当使用driver.find_elements_by_xpath得到了一个Element对象列表时，可以通过for循环展开这个列表然后在通过.text属性来获取到文本信息。一定要注意，不能直接在XPath的末尾加上text()，会报错。 原创文章，转载注明出处]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>学习</tag>
        <tag>a_jax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2018%2F10%2F19%2Ftest%2F</url>
    <content type="text"><![CDATA[test]]></content>
      <categories>
        <category>私密博客</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hexo文章加密]]></title>
    <url>%2F2018%2F10%2F19%2Fhexo%E6%96%87%E7%AB%A0%E5%8A%A0%E5%AF%86%2F</url>
    <content type="text"><![CDATA[hexo文章简单加密访问方法来源于简书这里只介绍怎么实现，具体原理访问上面链接 实现找到themes-&gt;next-&gt;layout-&gt;_partials-&gt;head.swig文件。按道理是添加在任何地方都行，但是推荐加在所有的标签之后，个人建议，仅做参考。以下是原作者加的代码：12345678910&lt;script&gt; (function()&#123; if(&apos;&#123;&#123; page.password &#125;&#125;&apos;)&#123; if (prompt(&apos;请输入文章密码&apos;) !== &apos;&#123;&#123; page.password &#125;&#125;&apos;)&#123; alert(&apos;密码错误！&apos;); history.back(); &#125; &#125; &#125;)();&lt;/script&gt; 再写新博客的时候在顶部加上新属性如下图所示：description是描述password后就是自定义的密码了具体效果如下图所示： 原创文章，转载请注明出处！]]></content>
      <categories>
        <category>HEXO</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python基础复习]]></title>
    <url>%2F2018%2F10%2F19%2Fpython%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[基础复习基础类型控制语句基础类型： 整数型 int 浮点数 float 字符串 str 布尔值 True、Flase 空值 None 变量 变量在计算机中不仅可以是数字，还可以是任意数据类型，变量在称剧中就是用一个变量名表示 字符串详解===转义字符:因为一些特殊字符是python中的关键字或一些特殊的概念如换行。所以以特殊字符 \ 开头，构造转义字符。常见的转义字符：\n 换行 \t 制表符\’ 单引号 \” 双引号\ 反斜杠 遍历：for i in ‘abc’:​ print(i)‘a’,’b’,’c’ 下标访问：‘hello’[4]→o 搜索：(了解)’字符串’.count(子字符串) 搜索子串出现次数‘xyaxyaXY’.count(‘xy’)→ 2‘xyaxyaXY’.count(‘xy’, 2)(了解)判断字符串是否以某个字母开头→ 1‘abcd’.startswith(‘a’)→ True‘abcd’.endswith(‘d’)→ True 字符串.find(子串) 找到返回下标，未找到返回-1‘axyaXY’.find(‘xy’)→ 1‘aaXY’.find(‘xy’)→ -1index()方法与find()类似，区别是未找到的时候报错。 替换 ：字符串.replace(老子串，新字符串)‘aaXY’.replace(‘aa’, ‘bb’)‘bbXY’分隔：(了解)partition把一个字符串切成几块并返回，包含子串。‘xyaxyaXY’.partition(‘xy’)(‘’, ‘xy’, ‘axyaXY’) 字符串.split(子串)，根据子串分成几部分并返回列表，不包含子串。‘xyaxyaXY’.split(‘x’)[‘’, ‘ya’, ‘yaXY’] 连接：join()用一个字符串连接可迭代对象的各个项。‘-‘.join([‘小明’, ‘hong’, ‘li’])→ ‘小明-hong-li’ 删除：字符串.strip(要删除的子串)‘今天天气真好\n’.strip(‘\n’)→ ‘今天天气真好’ 大小写转换：‘aa AA’.lower()→ ‘aa aa’‘aa AA’.upper()→ ‘AA AA’‘hello world’.capitalize()→ ‘Hello world‘aa AA’.swapcase()→ ‘AA aa’ isxxx判断:判断是否字母‘a’.isalpha()→ True判断是否空格‘ ‘.isspace()→ True判断是否数字‘1’.isdigit()→ True判断是否合法的变量名‘a4’.isidentifier()→ True 填充：对齐的时候会用到‘’.center(填充后的字符串总长度，要填充的字符串)‘abc’.center(5, ‘_’)→ ‘abc‘右侧填充‘abc’.ljust(10, ‘_’)→ ‘abc_______’左侧填充‘abc’.rjust(10, ‘_’)→ ‘___abc’ （以上了解end） 判断变量类型 type()函数 判断变量类型 isinstance(值， 类型) 如果值属于类型的话返回True 数据类型转换 int(x) 将x转换为一个整数 long(x) 将x转换为一个长整数 float(x ) 将x转换到一个浮点数 complex(real) 创建一个复数 str(x ) 将对象 x 转换为字符串 repr(x ) 将对象 x 转换为表达式字符串 eval(str ) 用来计算在字符串中的有效Python表达式,并返回一个对象 tuple(s ) 将序列 s 转换为一个元组 list(s ) 将序列 s 转换为一个列表 chr(x ) 将一个整数转换为一个字符 unichr(x ) 将一个整数转换为Unicode字符 ord(x ) 将一个字符转换为它的整数值 hex(x ) 将一个整数转换为一个十六进制字符串 oct(x ) 将一个整数转换为一个八进制字符串 控制语句：if else语句12345678if 条件a: 执行1elif 条件b: 执行2elif 条件c: 执行3else： 执行4 循环for循环：123打印三次字符串for i in range(1,4): print(&apos;你好 我是你爸&apos;) while 循环1234567891011121314151617181920i = 1while i&lt;4: print(&apos;你好 我是你爸&apos;) i+=1#循环中也可以加入条件控制语句i = 0while i &lt;10: i = i+1 if i %2 == 0: continue#当i是偶数时跳过此次循环 print(i)#break语句：用来结束循环i = 0while i &lt;10: i = i+1 print(i) if i == 6: break 字典列表元组列表list：定义：原来的单值变量无法满足业务需求，需求一个”容器“来装内容列表存储一些列有序（有下标）数据。容器内可以保存整数、布尔、字符串、或其他容器语法：创建一个列表：list1 = [1,’a’]还可以通过类 内置关键字创建 list1 = list() 添加项： append()方法，添加新项到列表的末尾list.append(要添加的数据) insert()方法，可以根据索引值插入制定数据。list.insert(索引，要添加的数据) for 循环 +append,批量添加项。list1 = [1,2,3,4]for i in range(5, 8): list1.append(i)print(list1) 两个列表拼接，用加号list1 = [1,2,3]list2 = [4,5,6]list3 = list1 + list2 删除项： pop()list.pop(索引)，根据索引删除列表中的某一个元素，返回删除成功的元素。pop()不传索引参数的时候，默认删除列表最后一项。pop()函数相当于append和insert函数的逆运算。 remove()list.remove(想要删除的项目值) 根据项item的值value来删除 clear()清空列表lisr.clear() 清空所有项目，返回空列表。 del删除列表对象，根据索引删除元素list = [1,2,3,4]del list[0]表示删除的是1 访问查询：通过索引（下标）list[0] 修改：修改基于索引的。访问列表下标，然后等号赋值。list [索引] = 新值 遍历：列表是一个可迭代对象，通过for循环可以吧一个列表中的元素依次取出如果判断一个对象是可迭代对象：通过collections模块的Itearable类型来判断。from collections import Iterableisinstance(‘abc’,Iterable)#判断字符串abc是否可以迭代True#返回true代表可以迭代list = [1,2,3,4]for i in list:​ print(i)#循环遍历出列表所有元素。 切片：列表[索引开始：索引结束：步进]切割截取出来区间。切片参数为负数是，截取方向向右为正，步进为正式才能截取。步进与截取方向相反时，结果为空索引0可以省略list[0:2] 等同于 list[:2]反向reverse输出列表list[::-1] &gt;&gt;&gt; [5, 4, 3, 2, 1] 列表生成式（不常用）12345678list1 = []for i in range(1,5) list1.append(i)#同样是在列表中加入1,2,3,4也可以用列表生成式写：[i for i in range(1,5)]#列表生成式也可以加入条件判断[i for i in range(1,10) if i%2==0]输出偶数 字典字典也是一种容器，特点：键值对存储没有索引，是无序的。靠键来增删改查 创建： 内置类实例化创建dict1 = dict() 大括号，内容空dict1 = {} 创建时附初始值dict1 = {‘name’: ‘小明’, ‘age’: 13, ‘sex’: ‘male’} 查询： （常用）dict1[键key]dict1 = {‘name’: ‘小明’, ‘age’: 13, ‘sex’: ‘male’}dict1[‘name’] &gt;&gt; ‘小明’如果键不存在的话，报 KeyError错误。dict1[‘aaa’] &gt;&gt;&gt; KeyError:aaa get(键, 默认值)跟dict[键]非常相似，只不过多了个默认值。当键不存在的时候会报Nonedict1.get(‘name’) &gt;&gt;&gt; ‘小明’dict1.get(‘aaa’, None) &gt;&gt;&gt; None 添加：dict[新建] = 新值 修改：dict[键] = 新值 遍历：先dict.items() 转换为 大列表套小列表项的形式。再用 for 循环遍历出来。for k,v in 字典.items():​ print(k, v)dict.keys()吧字典中的key取出来dict.values()把字典中的value取出来 删除： 键访问，值设置为None, 键还在字典[键] = Nonedict1 = {‘name’: ‘小明’, ‘age’: 13, ‘sex’: ‘male’}dict1[‘name’] = Nonedict1[‘name’] &gt;&gt;&gt; None （常用）字典.pop(键) 根据键删除，返回删除成功的值，会把键和值都删除dict1 = {‘name’: ‘小明’, ‘age’: 13, ‘sex’: ‘male’}dict1.pop(‘name’) &gt;&gt;&gt; ‘小明’dict1 &gt;&gt;&gt; {‘age’: 13, ‘sex’: ‘male’} del 字典[键]， del关键字删除，没有返回值，会把键和值都删除 字典.clear() 删除所有内容dict1.clear()dict1 &lt;&lt;&lt; {} 其他的常用方法：dict.items() 返回一个列表，每一项是 键值对dict.keys() 返回一个列表，每一项是字典里的 键dict.values() 返回一个列表，每一项是字典里的 值dict.contains(key) 键存在的话返回True，不存在返回False 元组元组跟list非常相似，特点和区别是 “不可修改”。所以元组需要在创建的时候就指定数据。语法是小括号括起来，逗号分隔每一项 创建:tuple2 = tuple((10, 20, ‘张三’))（常用）tuple1 = (10, 20, ‘张三’)场景:元组由于不可变，适合定义 常量、配置、不需要改变的值。这样在复杂代码中就不用害怕因为bug误修改值。例如定义 中国的所有省份，一个注册登录表单中的下拉框选项 查询:有索引，通过下标访问tuple[index]tuple2 = (‘河南’, ‘云南’)tuple2[0] &gt;&gt;&gt; ‘河南’ 也支持切片:tuple2[0:1] &gt;&gt;&gt; (‘河南’,) 注意的一个地方元组有时只有一项的时候，后面仍有一个逗号。原创文章，转载请注明出处！]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分享一种在markdown中插入图片的新方法]]></title>
    <url>%2F2018%2F10%2F18%2F%E5%88%86%E4%BA%AB%E4%B8%80%E7%A7%8D%E5%9C%A8markdown%E4%B8%AD%E6%8F%92%E5%85%A5%E5%9B%BE%E7%89%87%E7%9A%84%E6%96%B0%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言我在前面的hexo使用笔记中介绍过怎么插入图片，前面那种方法虽然能用，但还是有点麻烦，每次插入图片都要写一段markdown代码。每次创建新博客的时候都会新建一个md文件和一个文件夹，不仅麻烦而且还让博客文件夹变得不简洁。下面我介绍一种新的方法，既不用创建新的存放图片的文件夹，而且还自动生成markdown语法代码，再插入图片的时候直接ctrl+v就完事了，看起来是不是很方便呢？ 介绍下面进入正文，这种酷炫的新方法来自于大佬的git链接MarkdownPicPicker 是一个Markdown写作辅助工具。它能将剪贴板中的图片上传到网络图床中，并将markdown格式的图片链接(![]（&lt;图片地址&gt;))复制到剪贴板中。项目的readme中指明了安装使用方法下载链接下载完成后并解压，解压完成后得到一个MarkdownPicPicker.exe可执行文件和一个pic文件夹。 使用截图之后，比如我使用qq的快捷键截图（截图后点完成，不用保存），然后运行一下MarkdownPicPicker.exe，在你的编辑器中按下ctrl+v,神奇的事情就发生了，你会直接得到一段markdown插入图片的代码。是不是比上次的方法方便多了。如果你只想复制链接，不想让他变成这种形式，那么，你可以在命令行中输入markdownpicpicker.exe -linkonly （当前目录下按住shift点击右键选择在此处打开命令行窗口） 优化如果你觉得上面的方法还不够方便？是不是每次还要用鼠标来运行exe文件觉得很麻烦？没关系，还有一种方法可以优化这些步骤。官方文档说使用AutoHotKey来启动程序可以吧整个流程缩短到两秒钟 AutoHotkeyAutoHotKey是什么东西？我其实也是第一次听说这个什么软件，完全一脸懵比，上网上查了下才知道是干什么用的，在官网上下载之后然后运行出来发现是一份英文帮助文档，我也看不懂说的是什么。后面才知道这是个脚本程序，可以自定义快捷键来执行某个程序或某种命令。 ## 怎么使用呢？新建一个文本本件，名字随便起，然后把后缀名改为.ahk然后编辑这个文件输入以下代码： 1234#b::Run, D:\git第三方包和软件\MarkdownPicPicker_v1.0.0\markdownpicpicker.exeReturn!b::Run, D:\git第三方包和软件\MarkdownPicPicker_v1.0.0\markdownpicpicker.exe -linkonlyReturn 在鼠标右键点击run script即可运行。这里定义了两个快捷键，第一行代码表示按下键盘win+b键执行markdownpicpicker.exe第三行表示按下alt+b执行markdownpicpicker.exe -linkonly这和在命令行中的执行文件是一样的。关于autohotkey还有很多作用，请自行baidu,google。 原创文章，转载请注明出处！]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>图片</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python与MongoDB]]></title>
    <url>%2F2018%2F10%2F18%2Fpython%E4%B8%8EMongoDB-1%2F</url>
    <content type="text"><![CDATA[MongoDB介绍：MongoDB是一款基于c++开发的开源文档数据库，数据在MongoDB中以Key-Value的形式存储，就像是python的字典一样。使用MongoDB的管理软件Robo3T可以实现数据库的可视化。 安装：首先下载并解压MongoDB:访问官网，从官网上下载：官网下载选择适合自己的版本和操作系统然后Download就可以了解压后把Bin下的文件辅助到新的文件夹（在c盘或者d盘创建MongoDB文件夹）并创建存储数据库的文件夹Data和日志文件夹Log并创建mongod.conf，操作完成后文件目录如下图所示：接下来就是编辑mongod.conf了使用notepad++或其他除了编辑器(除了记事本，因为YAML对格式要求很严格，使用记事本会出现错乱)打开mongod.conf然后将以下代码复制进去： 12345678910systemLog: destination: file path: D:\MongoDB\Log\mongo.log logAppend: truestorage: dbPath: D:\MongoDB\Datanet: bindIp: 127.0.0.1security: authorization: disabled 这里的path和dbpath根据设置成自己的路径，比如我的Log和Data是放在D盘MongoDB文件夹下的。然后在安装目录下启动命令行，输入： 1mongod.exe --config mongod.conf 即可启动Mongodb 将MongoDB安装为windows服务在安装目录下打开cmd，输入以下命令 mongod.exe --config &quot;D:\MongoDB\mongod.conf&quot; --install 这里的D:\MongoDB\mongod.conf换成自己的mongod.conf路径 这个界面就代表安装服务成功了，然后在计算机管理→服务里面找到MongoDB开启服务，mongodb会作为计算机服务一直运行，不用再输入命令手动启动服务啦！！ Robo 3T介绍Robo 3T是一个快平台的MongoDB管理工具，可以在图形界面中查询或者修改MongoDB 下载和安装访问官网下载robo3t选择Download robo3t等待安装完成后打开 使用单机create链接(如下图),如果monggodb在本地计算机上面运行，只需要在Name这一栏填一个名字就可(也可以不填使用默认名字)，单击save即可然后点击connect就可以连接MongoDB了可以看到，数据在MongoDB中是按照库(Database)——集合——文档的层级关系来存储的。文档就像是python中的一个字典，集合相当于一个包含了很多字典的列表；库相当于一个大字典，大字典里面的每一个键值对都对应了一个集合，key为集合的名字，Value就是一个集合。 PyMongo安装1pip install pymongo pymongo的使用使用pymongo初始化数据库连接12345from pymongo import MongoClientclient = MongoClient()database = client[&apos;库名&apos;]#创建一个库collection = database[&apos;集合名&apos;]#创建一个集合#这里的库名和集合名除了可以是字符串还可以是一个变量，当库名或者集合名是一个变量的时候，可以通过循环来批量操作数据库，比如要创建多个集合的时候，可以吧集合名先保存到一个列表中，然后通过循环穿件多个集合。 插入数据插入操作用到的方法为insert(参数)参数就是python的字典。 123456789101112131415from pymongo import MongoClientclient = MongoClient()database = client[&apos;Chapter6&apos;]collection = database[&apos;spider&apos;]data = &#123;&apos;id&apos;: 2, &apos;name&apos;: &apos;张三&apos;, &apos;age&apos;: 20, &apos;salary&apos;: 9999&#125;collection.insert(data)#插入一条数据#当然也可以一次插入多条数据，先把多条数据保存到一个列表中，然后直接使用insert()即可more_data = [ &#123;&apos;id&apos;: 3, &apos;name&apos;: &apos;张四&apos;, &apos;age&apos;: 20, &apos;salary&apos;: 999&#125;, &#123;&apos;id&apos;: 4, &apos;name&apos;: &apos;张五&apos;, &apos;age&apos;: 20, &apos;salary&apos;: 99&#125;, &#123;&apos;id&apos;: 5, &apos;name&apos;: &apos;张六&apos;, &apos;age&apos;: 20, &apos;salary&apos;: 9&#125;, &#123;&apos;id&apos;: 6, &apos;name&apos;: &apos;张七&apos;, &apos;age&apos;: 20, &apos;salary&apos;: 19999&#125;]collection.insert(more_data) 查找数据查找功能对应的方法是 12find(查询条件，返回字段)#返回所有符合的信息find_one(查询条件，返回字段)#返回一条符合的信息 在不写find方法参数的时候，表示获取指定集合中所有字段。返回字段的参数指定返回内容。这个参数也是一个字典，key就是字段的名称，value是0或1,0表示不返回这个字段，1表示返回这个字段。其中_id这个字段比较特殊，必须人工指定它的值为0，这样才不会返回。而对于其他数据，应该统一使用返回，或者不返回。eg: 1collection.find(&#123;&apos;age&apos;:20&#125;,&#123;&apos;_id&apos;:0&#125;)#查询所有age为20的记录，返回除了_id的所有字段 这里需要知道：find()方法返回的是一个pymongo对象，这个对象可以被for循环展开，展开之后可以得到很多个字典。pymongo也支持逻辑查询：它们对应的关键词如下所示： $gt great than 大于 $lt less than 小于 $gte Greater than equal to 大于等于 $lte less than equal to 小于等于 $eq equal to 等于 $ne not equal to 不等于eg: 12collection.find(&#123;&apos;age&apos;:&#123;&apos;$gt&apos;:19&#125;&#125;)#查询age&gt;19的记录collection.find(&#123;&apos;age&apos;:&#123;&apos;$gte&apos;:19,&apos;$lt&apos;:30&#125;&#125;)#查询19≤age&lt;30的记录 对查询结果进行排序排序的方法为sort(),这个方法一般和find()配合使用他有两个参数，第一个参数指明以那一项进行排序，第二个参数为1或者-1，1表示升序，-1表示降序。eg: 1collection.find(&#123;&apos;age&apos;:&#123;&apos;$gt&apos;:19&#125;&#125;).sort(&apos;age&apos;,-1)#查询所有age大于19并以age按照降序进行排序。 修改修改也有两个方法： 12collection.updata_one(参数1，参数2)#修改一条collection.updata_many(参数1，参数2)#修改多条 参数1和2都是字典形式具体使用如下： 1collection.upadta_many(&#123;&apos;name&apos;:&apos;张三&apos;&#125;,&#123;&apos;$set&apos;:&#123;&apos;age&apos;:30&#125;&#125;)#将姓名为张三的人年龄全部改为30 删除删除也有两个方法 12collection.delete_one(&#123;&apos;name&apos;:&apos;张三&apos;&#125;)#把第一个name是张三的记录删除collection.delete_many(&#123;&apos;name&apos;:&apos;张三&apos;&#125;)#把name是张三的记录全部删除 删除方法只有一个参数，是字典形式。至此,mongodb以及pymongo的使用暂时结束了！ 原创文章，转载请注明出处！]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MongoDB</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一键设置爬虫headers]]></title>
    <url>%2F2018%2F10%2F18%2F%E4%B8%80%E9%94%AE%E8%AE%BE%E7%BD%AE%E7%88%AC%E8%99%ABheaders%2F</url>
    <content type="text"><![CDATA[在写爬虫的时候我们经常需要设置headers属性来让爬虫模拟浏览器从而获得数据。在添加headers属性的时候，需要把浏览器所有的headers属性都写上去：这么长的headers如果复制下来然后手动把它设置成字典的形式太麻烦、太费时间。那么有没有办法一下把这些属性转为字典形式呢？带着这个疑问我向大佬请教：大佬不愧是大佬，分分钟解决我的问题好吧。下面介绍下这个方法：安装（输入以下命令）：pip install –upgrade git+https://github.com/kingname/CrawlerUtility.git使用：首先引入这个包然后把从浏览器复制下来的headers保存成长字符串。在使用ChromeHeaders2Dict解析一下就完事了是不是很简单！推荐：大佬的github 后记：我使用这个包的时候，是第一次使用别人手动写的第三方包，中间也遇到了很多问题。比如一开始我不知道怎么安装，然后在网上搜了一下怎么安装的资料。我这才知道安装第三方包原来有两种方式，我以前还以为只能手动下载安装呢。下面记录下两种安装方式：一、手动安装 在github上面下载包 然后解压该文件 在该文件夹按住shift+鼠标右键 在此处打开命令行窗口，然后输入python setup.py install 二、自动安装直接在命令行输入pip install 包eg:pip install pillow其实在每个github项目下都有README文件在这个文件里都会介绍怎么install，比如上面的这个项目：然后：问题又来了，在上面这些东西都弄好了之后我在pycharm上面引入的时候pycharm报错，对于我这样一个萌新来说瞬间又懵了。于是我又向大佬请教…这才知道pycharm原来还有一个加载时间，果然一小会过后，这个包就能正常使用了。不得不说，这个大佬人真的很好，身为这么大的一个大佬，对于我这个萌新的问题都很耐心的解答，再次表示感谢。这个大佬就是我最近看的一本爬虫书的作者技术过硬人又好！！！推荐去看看这本书，写的真不错，我这种死笨死笨的萌新都看的很明白！ 原创文章，转载请注明出处！]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo使用笔记]]></title>
    <url>%2F2018%2F10%2F17%2Fhexo%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[创建新文件在D:/个人博客/BLOG/source/_posts文件夹下执行命令hexo new ‘新文件名字’ 在文本中添加图片 把主页配置文件_config.yml里的post_asset_folder:这个选项设置为true(如过以后不想生成同名的文件夹了改为false即可) 在hexo目录下执行mup install hexo_asset_image –save,这是下载安装上传本地图片的插件。 等待安装完成后，再运行上面创建新文件的命令来生成新md文件时，/source/_posts文件夹中除了xxx.md文件还有一个同名的文件夹。 最后在xxx.md中想引入图片时，先把图片复制到这个文件夹中，然后只需要在xxx.md中按照markdown的格式引入图片：ps:!后面没有空格，hexo使用笔记前可加/也可以不加，图片名字一定不要写错。在页面中添加超链接关于文章推送的问题之前由于不会弄，导致每次推送时都把git上面的CNAME文件弄丢了，每次推送完之后还要重新创建CNAME文件，这样很麻烦。通过查阅资料知道了把CNAME文件放在本地hexo目录下source的_posts文件夹下就可以解决这个问题了。关于云解析首先要有一个域名，我用的是腾讯云域名：www和@主机记录的记录值是自己的githubpage的地址然后本地文件中要有一个CNAME文件，这个文件只有一行：这样就行了 原创文章，转载请注明出处！]]></content>
      <categories>
        <category>HEXO</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HELL HEXO]]></title>
    <url>%2F2018%2F10%2F17%2FHELL-HEXO%2F</url>
    <content type="text"><![CDATA[This is my BLOG]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
        <tag>npm</tag>
      </tags>
  </entry>
</search>
