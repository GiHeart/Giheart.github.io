<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>YuMou&#39;sblog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-11-01T13:10:43.274Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Yu mou</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MySQL入坑</title>
    <link href="http://yoursite.com/2018/11/01/MySQL%E5%85%A5%E5%9D%91/"/>
    <id>http://yoursite.com/2018/11/01/MySQL入坑/</id>
    <published>2018-11-01T12:34:21.000Z</published>
    <updated>2018-11-01T13:10:43.274Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2018/11/01/5bdafb4536175.png" alt=""> </p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>MySQL是当今世界上最流行的开源数据库，我在前面已经学习过非关系型数据库MongoDB。MongoDB的优点是快速、高扩展性，json的存储格式，就像python的字典一样，学习起来比较快；但缺点就是它是非关系型数据库，属于文档型数据库，不支持事物。</p><p>所以今天闲了没事就来学学MySQL。</p><h1 id="安装与使用"><a href="#安装与使用" class="headerlink" title="安装与使用"></a>安装与使用</h1><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>我就知道数据库这玩意安装没那么简单，上次安装MongoDB安装了半天才弄好，这次安装MySQL也是，我不知道在心里骂了多少次(mysql NMSL)。</p><p>首先在<a href="https://dev.mysql.com/downloads/mysql/" target="_blank" rel="noopener">官网上</a>下载mysql</p><p><img src="https://i.loli.net/2018/11/01/5bdaf58abbe10.png" alt=""> </p><p>下载好了之后是个zip文件，找个地方解压就好了。</p><p>然后在解压后的文件夹里创建my.ini使用notepad++或其它编译器打开并输入以下代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[mysql]</span><br><span class="line"># 设置mysql客户端默认字符集</span><br><span class="line">default-character-set=utf8</span><br><span class="line"> </span><br><span class="line">[mysqld]</span><br><span class="line"># 设置3306端口</span><br><span class="line">port = 3306</span><br><span class="line"># 设置mysql的安装目录</span><br><span class="line">basedir=C:/web/mysql-8.0.13-winx64</span><br><span class="line">datadir=C:/web/mysql-8.0.13-winx64/data</span><br><span class="line"># 设置 mysql数据库的数据的存放目录，MySQL 8+ 不需要以下配置，系统自己生成即可，否则有可能报错</span><br><span class="line"># datadir=C:\\web\\sqldata</span><br><span class="line"># 允许最大连接数</span><br><span class="line"># max_connections=20</span><br><span class="line"># 服务端使用的字符集默认为8比特编码的latin1字符集</span><br><span class="line">character-set-server=utf8</span><br><span class="line"># 创建新表时将使用的默认存储引擎</span><br><span class="line">default-storage-engine=INNODB</span><br></pre></td></tr></table></figure><p>需要注意的是basedir和datadir要换成自己的安装目录和data目录。</p><p>弄好了之后这个文件夹差不长这样。</p><p><img src="https://i.loli.net/2018/11/01/5bdaf6a4c61c0.png" alt=""> </p><p>接下来就是启动数据库啦。</p><p>cmd下切换到到数据库安装目录\bin下：</p><p>然后输入如下命令初始化数据库。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqld --initialize --console</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2018/11/01/5bdaf75477c37.png" alt=""> </p><p>注意啦，初始化完成之后这里会生成一个用户名和密码，这个很重要，用户名是root密码就是上面用红框标注的地方，这个一定要记下来，下面会用到。这个xx密码我也是佛了，就是这个东西坑了我半天。</p><p>然后输入安装命令，将mysql安装为Windows server</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql install</span><br></pre></td></tr></table></figure><p>在输入以下命令启动服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net start mysql</span><br></pre></td></tr></table></figure><h2 id="登录MySQL"><a href="#登录MySQL" class="headerlink" title="登录MySQL"></a>登录MySQL</h2><p>当MySQL服务已经运行时，输入以下命令登录MySQL</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2018/11/01/5bdaf93773a96.png" alt=""> </p><ul><li><p>-u 就是用户的意思</p></li><li><p>-p 表示用密码登录</p></li><li>还有一个-h属性，由于在这里mysql服务运行在本地，所以可以忽略</li></ul><p>我在这个登录的过程中失败了N次，百度了一大堆没用的东西，当出现错误的时候首先检查mysql服务开启了没有，可以在计算机关机→服务 中查看，确定开启了之后，就别管其他的，一次两次没登进去就多试几次。就是这个D密码，太麻烦了，一般都是密码没输对。</p><p>在第一次登录到mysql之后，需要重新设置下密码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter user &apos;root&apos;@&apos;localhost&apos; identified by &apos;xxx&apos; PASSWORD EXPIRE NEVER account unlock;</span><br></pre></td></tr></table></figure><p>xxx就是新密码。一定要用这个命令！！！</p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>MySQL用的就是传统的SQL语句，但是我还没学会。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2018/11/01/5bdafb4536175.png&quot; alt=&quot;&quot;&gt; &lt;/p&gt;
&lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h
      
    
    </summary>
    
      <category term="MySQL" scheme="http://yoursite.com/categories/MySQL/"/>
    
    
      <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="MySQL" scheme="http://yoursite.com/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>【持续更新】常用知识点</title>
    <link href="http://yoursite.com/2018/10/31/%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%E7%AC%94%E8%AE%B0-1/"/>
    <id>http://yoursite.com/2018/10/31/持续更新笔记-1/</id>
    <published>2018-10-31T02:55:39.000Z</published>
    <updated>2018-11-01T12:33:05.129Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.pixabay.com/photo/2018/10/21/21/28/autumn-3763897__340.jpg" alt=""></p><p>在很多时候，往往最基本最简单的知识能帮上大忙，在这儿就记录下经常用到的小知识点。希望以后再碰到问题的时候能够首先想到使用这些简单的方法来解决！</p><h1 id="字符串相关："><a href="#字符串相关：" class="headerlink" title="字符串相关："></a>字符串相关：</h1><p>我们几乎天天都在跟字符串打交道，字符串应该是python中最常见的数据类型了；但是对于字符串的基本知识掌握的还是不太牢固。</p><h2 id="字符串切片"><a href="#字符串切片" class="headerlink" title="字符串切片"></a>字符串切片</h2><p>字符串切片：切片操作（slice）可以从一个字符串中获取子字符串（字符串的一部分）。我们使用一对方括号、起始偏移量start、终止偏移量end 以及可选的步长step 来定义一个分片。</p><p>格式： [start：end:step]</p><p>• [:] 提取从开头（默认位置0）到结尾（默认位置-1）的整个字符串<br>• [start:] 从start 提取到结尾<br>• [:end] 从开头提取到end - 1<br>• [start:end] 从start 提取到end - 1<br>• [start：end:step] 从start 提取到end - 1，每step 个字符提取一个<br>• 左侧第一个字符的位置/偏移量为0，右侧最后一个字符的位置/偏移量为-1</p><p>几个特别的examples 如下：</p><p>提取最后N个字符：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>letter = <span class="string">'abcdefghijklmnopqrstuvwxyz'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>letter[<span class="number">-3</span>:]</span><br><span class="line"><span class="string">'xyz'</span></span><br></pre></td></tr></table></figure><p>从开头到结尾，step为N：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>letter[::<span class="number">5</span>]</span><br><span class="line"><span class="string">'afkpuz'</span></span><br></pre></td></tr></table></figure><p>将字符串倒转(reverse)， 通过设置步长为负数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>letter[::<span class="number">-1</span>]</span><br><span class="line"><span class="string">'zyxwvutsrqponmlkjihgfedcba'</span></span><br></pre></td></tr></table></figure><h2 id="字符串常用方法"><a href="#字符串常用方法" class="headerlink" title="字符串常用方法"></a>字符串常用方法</h2><p>分割：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">str1 = <span class="string">'safasf.jpg'</span></span><br><span class="line">alist = str1.split(<span class="string">'.'</span>)</span><br><span class="line">print(alist)</span><br><span class="line">&gt;&gt;&gt;[<span class="string">'safasf'</span>, <span class="string">'jpg'</span>]</span><br><span class="line"><span class="comment">#split方法的参数是一个字符，它以这个字符为分割点，把字符串分割为两部分，返回一个列表</span></span><br></pre></td></tr></table></figure><p>替换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">str1 = <span class="string">'safasf.jpg'</span></span><br><span class="line">str2 = str1.replace(<span class="string">'.'</span>, <span class="string">'='</span>)</span><br><span class="line">print(str2)</span><br><span class="line">&gt;&gt;&gt;safasf=jpg</span><br><span class="line"><span class="comment">#replace方法接收两个参数，第一个想要替换的字符，第二个是新字符。返回的是字符串。</span></span><br></pre></td></tr></table></figure><h2 id="爬虫相关"><a href="#爬虫相关" class="headerlink" title="爬虫相关"></a>爬虫相关</h2><p>我记得我在写知乎爬虫的时候碰到了问题，就是无论用什么解码得到的都是乱码。后来才知道是Accept-Encoding惹的祸。</p><p>Accept-Encoding主要表示浏览器支持的压缩编码有哪些。gzip是压缩编码的一种，deflate是一种无损数据压缩算法。</p><p>那浏览器压缩编码跟我们需要的HTML代码有什么关系呢？因为如果这个地段设置成gzip、deflate，那么从服务器返回来的是对应的gzip，deflate压缩的代码，此时没有进行解码，所以会出现乱码的情况，而一些常规浏览器中，从服务器返回对应的gzip、deflate压缩的代码后，浏览器可以自动进行解压缩，忽而不会出现乱码。解决的办法是直接忽略不写此字段或者把此字段值改为”utf-8”，”gbk”</p><p>还有一个问题，就是在开启了Fiddler后，所爬取的网址要以具体文件或者“/”结尾，如果没有具体文件，直接写该具体文件的网址即可，比如将要爬取的网址写为“<a href="http://news.163.com/16/0825/09/BVA89U500014ESH.html”这种写法是可以的，如果被爬网址是一个文件夹，比如要爬取“http://www.baidu.com”，此时爬取的是一个目录(文件夹)，所以需要以“/”结尾。" target="_blank" rel="noopener">http://news.163.com/16/0825/09/BVA89U500014ESH.html”这种写法是可以的，如果被爬网址是一个文件夹，比如要爬取“http://www.baidu.com”，此时爬取的是一个目录(文件夹)，所以需要以“/”结尾。</a></p><p>在使用requests爬取https网址时，需要添加一个参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">html = requests.get(<span class="string">"https://www.baidu.com"</span> verify=<span class="keyword">False</span>).content.decode()</span><br></pre></td></tr></table></figure><p>就是这个verify，不然会报错。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://cdn.pixabay.com/photo/2018/10/21/21/28/autumn-3763897__340.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;在很多时候，往往最基本最简单的知识能帮上大忙，在这儿就记录下经常用到的小知识点。
      
    
    </summary>
    
      <category term="日志" scheme="http://yoursite.com/categories/%E6%97%A5%E5%BF%97/"/>
    
    
      <category term="笔记" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>爬虫开发之requests库</title>
    <link href="http://yoursite.com/2018/10/27/%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91%E4%B9%8Brequests%E5%BA%93/"/>
    <id>http://yoursite.com/2018/10/27/爬虫开发之requests库/</id>
    <published>2018-10-27T07:41:59.000Z</published>
    <updated>2018-10-29T08:32:14.291Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2018/10/29/5bd6b00ba609d.png" alt=""> </p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    我在前面的博客  <a href="https://forali.club/2018/10/24/python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/" target="_blank" rel="noopener">爬虫入门链接</a>  中已经介绍了python自带的网络库urllib库的基本用法和简单爬虫的基本流程。但是不知道你有没有觉得urllib库有点麻烦，语法臃肿。我今天就来介绍一个更间接地第三方库–requests库。我在第一次遇到requests库之后就果断弃用了urllib，这俩库一比起来高下立判。我曾经问过一个大佬为什么在写爬虫的时候只用requests库？他回答说“人生苦短，为什么不用高效的工具。”</p><p>​    requests是python的一个第三方HTTP库，它比python自带的urllib库更加简单、方便和人性化。requests库的作者这样介绍requests：HTTP for humans，这才是给人用的HTTP库。</p><h1 id="安装与使用"><a href="#安装与使用" class="headerlink" title="安装与使用"></a>安装与使用</h1><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>可以直接使用pip安装requests:</p><p>pip install requests</p><p>或者手动安装打开：<a href="http://github.com/kennethreitz/requests" target="_blank" rel="noopener">链接</a></p><p><img src="https://i.loli.net/2018/10/29/5bd6b805b8a19.png" alt=""> </p><p>下载到本地后解压，然后找到包含setup.py的文件夹，在此处打开命令行窗口后输入以下命令：</p><p>python setup.py install</p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>​    使用浏览器来访问网页，看起来只需要输入网址就可以。经过前面的学习，我们知道网页有多种打开方式，最常见的是GET和POST方式。在浏览器里面可以直接通过输入网址访问的页面，就是使用了GET方式。还有一些页面，只能通过从另一个页面单击某个链接或者某个按钮以后跳过来，不能直接通过浏览器输入网址访问，这种网页就使用了POST方式。</p><h2 id="GET-方式"><a href="#GET-方式" class="headerlink" title="GET 方式"></a>GET 方式</h2><p>​    对于使用GET方式访问的网页，在Python里面可以使用requests的get()方法获取网页的源代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">html = requests.get(<span class="string">"网址"</span>)</span><br><span class="line">html_bytes = html.content</span><br><span class="line">html_str = html_bytes.decode()</span><br></pre></td></tr></table></figure><p>​    在这四行代码中，第一行导入了requests库，这样代码才能使用。</p><p>第二行使用GET方法获取了网页，得到一个Response对象</p><p>第三行使用.content属性来显示bytes(字节)类型的网页源代码</p><p>第四行使用decode来解码字节。</p><p>大部分情况下，这四行代码我一般都合成两行来写：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> request</span><br><span class="line">html_str = request.get(<span class="string">"网址"</span>).content.decode()</span><br></pre></td></tr></table></figure><p>其实在解码的时候，我们会经常遇到很多错误。这个时候首先需要查看源网页的编码方式，一般在网页的头部会表明编码方式。</p><p><img src="https://i.loli.net/2018/10/29/5bd6bc455f5e7.png" alt=""> </p><p>有时，即使解码方式和源网页保持了一致，还是会出错，比如网页源代码中包含了无法是别的字符。这时可以在decode()时加上第二个参数“ignore”，强制忽略无法识别的字符。</p><p>但是，问题往往没有这么简单，有的时候，你解码方式也对了，ignore参数也加了，但解码出来的网页还是一堆乱码。我就遇到过，郁闷了半天，然后一个大佬给我解决了。是因为在设置请求头的时候，请求头里面有一个Accept Encoding属性，他定义了网站接受什么解码方式，把这个属性给删除就行了。注意，这个Accept Encoding属性并不是每个网站都遵守的，所以在出现错误的时候在吧这个属性给删除了。</p><p><img src="https://i.loli.net/2018/10/29/5bd6c585f415d.png" alt=""> </p><h2 id="POST方式"><a href="#POST方式" class="headerlink" title="POST方式"></a>POST方式</h2><p>网页的访问方式除了GET方式以外，还有POST方式。有一些网页，使用GET和POST方式访问同样的网址，得到的结果是不一样的。还有一些网页，只能使用POST方式访问，如果使用GET方式访问，网站会直接返回错误信息。</p><p>POST方法的格式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">data = &#123;<span class="string">'key'</span>:<span class="string">'value1'</span>,</span><br><span class="line"><span class="string">'key2'</span>:<span class="string">'value2'</span>&#125;</span><br><span class="line">html_formdata = request.post(<span class="string">'网址'</span>.data=data).content.decode()</span><br></pre></td></tr></table></figure><p>data这个字典的内容需要根据实际情况修改，key和value在不同的网站是不一样的。而做爬虫，构造这个字典是任务之一。</p><p>还有一些网址，提交的内容是JSON格式的，因此post()方法的参数需要进行一些修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">html_json = requests.post(<span class="string">'网址'</span>,json=data).content.decode()</span><br></pre></td></tr></table></figure><p>这样写代码，request可以自动将字典转换为JSON格式。</p><p>关于网页是什么访问方式，可以使用浏览器开发者工具，或者使用Fiddler分析出来。具体方法暂时不讲。</p><h1 id="关于请求头"><a href="#关于请求头" class="headerlink" title="关于请求头"></a>关于请求头</h1><p>​    我们知道有些网站添加了反爬虫机制，请求头过滤就是最常见的一种发爬虫机制。我在前面的文章中已经介绍过urllib库添加请求头的方法，那么requests库怎么添加请求头呢？很简单，只需要加一行代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">headers = &#123;<span class="string">'user-agent'</span>:<span class="string">' Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36'</span>&#125;</span><br><span class="line">html = requerst.get(<span class="string">'网址'</span>, headers=headers).content.decode()</span><br><span class="line"><span class="comment">#可以看到headers是python的字典形式</span></span><br></pre></td></tr></table></figure><p>我在前面的博客中忘了说，我们在写爬虫的时候最好吧浏览器里面的headers全都添加进去，因为很多网站不是设置一个user-agent就能蒙混过关的。</p><p><img src="https://i.loli.net/2018/10/29/5bd6c31e4de1d.png" alt=""> </p><p>看到这么多条属性要一个一个手动转换成python的字典形式是不是感到头皮发麻，还好我有神器！</p><p>具体方法在我的:<a href="https://forali.club/2018/10/18/%E4%B8%80%E9%94%AE%E8%AE%BE%E7%BD%AE%E7%88%AC%E8%99%ABheaders/" target="_blank" rel="noopener">另一篇博客</a></p><p>requests库很强大，我先简单写这么多，等以后用到了再更新！</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2018/10/29/5bd6b00ba609d.png&quot; alt=&quot;&quot;&gt; &lt;/p&gt;
&lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h
      
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="requests" scheme="http://yoursite.com/tags/requests/"/>
    
  </entry>
  
  <entry>
    <title>HTML总结</title>
    <link href="http://yoursite.com/2018/10/26/HTML%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2018/10/26/HTML总结/</id>
    <published>2018-10-26T11:41:02.000Z</published>
    <updated>2018-10-26T12:40:58.216Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2018/10/26/5bd2fec7531af.png" alt=""> </p><p>本篇博客介绍HTML的语法，还包括CSS等。</p><h1 id="HTML介绍"><a href="#HTML介绍" class="headerlink" title="HTML介绍"></a>HTML介绍</h1><p>HTML：超文本标记语言，除了文本，还包括图片、视频、下载文件等内容。</p><p>标记语言： 变成有逻辑、变量、ifalse。标记语言相比比较简单，由信息和结构语法组成。</p><h1 id="历史和版本"><a href="#历史和版本" class="headerlink" title="历史和版本"></a>历史和版本</h1><p>xhtml：类似html但语法要求非常严格。</p><p>html4：就是我们要学习的内容。</p><p>html5：在html4的基础上添加了新功能和新语法。html5兼容html4目前最流行的就是html5</p><h1 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h1><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>helloworld</span><br><span class="line">    <span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span>&gt;</span>hello<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span>&gt;</span>world<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p>##　常用标签：</p><table><thead><tr><th>标签名</th><th>用法</th></tr></thead><tbody><tr><td>html</td><td>根标签</td></tr><tr><td>head</td><td>头部，元信息，引用css、jss和title</td></tr><tr><td>title</td><td>标题</td></tr><tr><td>h1-h6</td><td>网页内容里的标题。六个级别。数字越大，标题越小</td></tr><tr><td>p</td><td>段落，输出完会换行。</td></tr><tr><td>br /br br/</td><td>都表示换行</td></tr><tr><td>b</td><td>加粗</td></tr><tr><td>i</td><td>斜体</td></tr><tr><td>strong</td><td>加重语气</td></tr><tr><td>small</td><td>字体变小变细</td></tr><tr><td>hr</td><td>水平分割线</td></tr><tr><td>a</td><td>超链接标签</td></tr><tr><td>table</td><td>表格</td></tr><tr><td>img</td><td>图片</td></tr><tr><td>ul</td><td>无序列表</td></tr><tr><td>li</td><td>列表中的每一项</td></tr><tr><td>ol</td><td>有序列表</td></tr><tr><td>form</td><td>表单</td></tr><tr><td>select</td><td>下拉菜单</td></tr><tr><td>option</td><td>下拉菜单的每一项</td></tr><tr><td>textarea</td><td>文本域</td></tr><tr><td>submit</td><td>提交。会将数据传递到后台</td></tr></tbody></table><p>标签嵌套：html标签是可以嵌套的</p><p>空元素：标签的内容可以为空。</p><p>！DOCTYPE html：声明html类型，不容易出错，可以不写</p><p>meta charset=”UTF-8”元信息，作者，编码提、网页描述</p><p>属性和值为了定义标签的功能。</p><p>属性attribate:标签里的键值对里的键。</p><p>值 value: 标签里的键值对的值。只需要用双引号括起来。</p><h2 id="常用属性"><a href="#常用属性" class="headerlink" title="常用属性"></a>常用属性</h2><table><thead><tr><th>属性名</th><th>用法</th></tr></thead><tbody><tr><td>algin</td><td>对齐。center,left,right</td></tr><tr><td>style</td><td>样式，值是css语句</td></tr><tr><td>title</td><td>鼠标在热点区域停留时出现的提示信息。</td></tr><tr><td>name</td><td>一个标签的别名，可以用来描述标签，name可以重复</td></tr><tr><td>id</td><td>一个标签的识别符或别名，跟name的区别是不可重复</td></tr><tr><td>href</td><td>链接地址</td></tr><tr><td>target</td><td>链接打开方式，默认是self当前标签页打开 blank新标签页打开</td></tr><tr><td>src</td><td>图片资源地址</td></tr><tr><td>width</td><td>宽</td></tr><tr><td>height</td><td>高</td></tr><tr><td>action</td><td>定义请求地址</td></tr><tr><td>input下的</td><td>属性type：</td></tr><tr><td>text</td><td>普通的文本输入框</td></tr><tr><td>password</td><td>密码输入框</td></tr><tr><td>radio</td><td>单选</td></tr><tr><td>checkbox</td><td>多选框</td></tr><tr><td>placeholder</td><td>占位符输出一些提示性信息</td></tr><tr><td>label</td><td>标签，写在input前面作为信息提示。</td></tr><tr><td></td></tr></tbody></table><h1 id="CSS"><a href="#CSS" class="headerlink" title="CSS"></a>CSS</h1><p>css:层叠样式表。对html布局、字体、颜色进行精确的外观控制</p><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><p>css表达式实例:</p><p>header{background-color:”black”}</p><p>上面的header是选择器，大括号包住语句块。每一句语句由声明和值构成，值用括号括住，冒号分割，分号结尾。</p><p>选择器：选择这句css声明具体作用在html文件的哪个部分。</p><p>引用样式的方式：</p><ol><li>行内样式：直接写在html标签内的style属性上。</li><li>内部像是：css语句写在head标签下的style标签中。</li><li>外部样式：css语句写在xxx.css文件当中。link标签，rel目标文件夹种类，href目标文件地址，type文件类型。</li></ol><p>优先级：不同引用样式方式作用于同一个标签。</p><p>行内样式优先级最高，    其次是内部样式，最后是外部样式</p><p>可读性易维护性：行内样式多之后html文件显得混乱</p><p>外部样式最易于维护，其次是内部样式，最后是行内样式。</p><p>css选择器：</p><ol><li><p>派生选择器。外层的标签包含里层的标签，通过空格表示层级关系。</p><p>eg:  li strong{color:red;}</p></li><li><p>ID选择器。标签里实现定义好ID值，语法#开头跟ID名字。</p><p>eg:  secondp{color:red;}</p></li><li><p>类class选择器。标签里定义好class值。css中语法，开头跟class值。</p></li></ol><p>css常用标签：</p><p>background 背景相关<br>background-color 定义背景颜色的<br>background-img:url(“”) 背景图片<br>background-repeat:no-repeat; 背景图片是否可重复<br>background-position:top;<br>background-attachment:fixed;<br>background-size:200px 300px;背景图宽高，长度可以是像素px 也可以用百分比</p><p>继承：</p><p>外层标签如果定义了一个样式，外层标签所包含的标签都会继承这个样式。</p><p>子标签重写一个相同的样式，子标签里的定义优先级更高。</p><p>盒子模型：</p><p><img src="https://i.loli.net/2018/10/26/5bd30aa9db369.png" alt=""> </p><p>padding:内容和边缘的距离</p><p>border:边缘，可以定义style、width、color</p><p>margin:div块距离其他div块边缘的距离。两个div的外边距重合时，以大的为准。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2018/10/26/5bd2fec7531af.png&quot; alt=&quot;&quot;&gt; &lt;/p&gt;
&lt;p&gt;本篇博客介绍HTML的语法，还包括CSS等。&lt;/p&gt;
&lt;h1 id=&quot;HTML介绍&quot;&gt;&lt;a href=&quot;#HTML介绍&quot; c
      
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="HTML" scheme="http://yoursite.com/tags/HTML/"/>
    
      <category term="前端" scheme="http://yoursite.com/tags/%E5%89%8D%E7%AB%AF/"/>
    
  </entry>
  
  <entry>
    <title>Python自动安装第三方包</title>
    <link href="http://yoursite.com/2018/10/26/Python%E8%87%AA%E5%8A%A8%E5%AE%89%E8%A3%85%E7%AC%AC%E4%B8%89%E6%96%B9%E5%8C%85/"/>
    <id>http://yoursite.com/2018/10/26/Python自动安装第三方包/</id>
    <published>2018-10-26T03:08:52.000Z</published>
    <updated>2018-10-30T02:29:15.929Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2018/10/26/5bd285d4ebe61.png" alt=""> </p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>昨天突然心血来潮想利用我目前学过的知识一个能自动安装python第三方包的小程序。你肯定想问了，我明明有pip可以实现自动安装第三方包了，为什么还要写这么个看似毫无卵用的东西呢？</p><p>我也不知道，就是心血来潮吧。</p><p>但是这个小程序还是有好处的:</p><p>他首先是从第三方网站下载了第三方包到本地，然后在使用pip install xxxxx.whl在本地安装第三方包，所以更快，成功率更高。</p><h1 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h1><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><ol><li>使用爬虫爬取python<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/?tdsourcetag=s_pctim_aiomsg" target="_blank" rel="noopener">第三方包网站</a>，</li><li>从网站源代码中提取出所有包名字并存入数据库（使用MongoDB）</li><li>根据需求从数据库中查询关键字（也就是需要的包名）</li><li>根据包名构建get请求</li><li>发送get请求，得到文件</li><li>引入os包，执行命令os.system(“pip install 包名”)</li></ol><h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><ol><li><p>在写爬虫的时候，对XPath的使用还不是很清楚，导致我卡在爬虫这一环节很久。</p><p><img src="https://i.loli.net/2018/10/26/5bd2887079305.png" alt=""> </p><p>就是提取包名这一行，想了很久。开始我以为像这样写XPath只能获取到网页源代码中第一个包名。原来，在没有指定特殊属性的时候，XPath会返回所有符合条件的信息，就像上面这个XPath，它返回的就是div class=”pylibs”标签下的li标签下的ul标签下的li标签下的a标签的所有文本信息。这句话虽然有点绕，但是没毛病哦铁汁。</p></li><li><p>在插入数据库的时候，首先要构造一个列表，然后使用for循环把上面提取到的信息（也就是提取到的包名）展开，然后构造字典，再然后将这些字典添加到列表中。最后使用collection.insert_many()插入到数据库中。</p><p><img src="https://i.loli.net/2018/10/26/5bd28ab4854ec.png" alt=""> </p></li><li><p>从数据库中查询包名，用到的是正则表达式查询（原来我还不会在MongoDB中使用正则查询。）。</p><p><img src="https://i.loli.net/2018/10/26/5bd28bbe8d8a6.png" alt=""> </p><p>这里find查询方法返回的是一个pymongo对象，使用for循环展开。</p></li><li><p>在数据库中查询到数据之后拿出来构造url，请求这个url的时候才并没有获得二进制文件</p><p><img src="https://i.loli.net/2018/10/26/5bd28c279174d.png" alt=""> </p><p>找了半天才发现原来从数据库中提取出来的数据有些特殊符号和我们平时使用的不太一样。这个不仔细看真的很难看出来。我发现从数据库中提取出来的数据中的“-”符号似乎短了一点。原来就是这个符号惹的祸。然后我使用了字符串的replace方法把这个符号替换成了正常的样子。</p><p>注意：字符串的replace方法并不是能修改源字符，他其实是一个函数，返回值是替换过后的字符串，所以需要用一个变量来接受它。</p></li></ol><h1 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h1><p>环境：需要安装MongoDB数据库。关于MongoDB怎么使用请看:<a href="https://forali.club/2018/10/18/python%E4%B8%8EMongoDB-1/" target="_blank" rel="noopener">我的另一篇博客</a></p><p>还需要lxml库和requests库。安装lxml库可以参看：<a href="https://forali.club/2018/10/24/XPath%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">另一篇博客</a></p><p>源代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> lxml.html</span><br><span class="line"><span class="keyword">from</span> pymongo <span class="keyword">import</span> MongoClient</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">client = MongoClient()</span><br><span class="line">database = client[<span class="string">'第三方包'</span>]</span><br><span class="line">collection = database[<span class="string">'包名'</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spiderdate</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">"https://www.lfd.uci.edu/~gohlke/pythonlibs/?tdsourcetag=s_pctim_aiomsg"</span></span><br><span class="line">    html = requests.get(url).content<span class="comment">#获取网页源代码</span></span><br><span class="line">    selector = lxml.html.fromstring(html)</span><br><span class="line">    content = selector.xpath(<span class="string">'/html/body/ul[@class="pylibs"]/li/ul/li/a/text()'</span>)<span class="comment">#提取包名</span></span><br><span class="line">    idnum = <span class="number">1</span></span><br><span class="line">    namelist = []</span><br><span class="line">    <span class="keyword">for</span> name1 <span class="keyword">in</span> content:</span><br><span class="line">        <span class="string">"""构造字典，插入数据库"""</span></span><br><span class="line">        namedict = &#123;<span class="string">'id'</span>: idnum, <span class="string">'name'</span>: name1&#125;</span><br><span class="line">        idnum = idnum+<span class="number">1</span></span><br><span class="line">        namelist.append(namedict)</span><br><span class="line">    collection.insert_many(namelist)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">datefind</span><span class="params">()</span>:</span></span><br><span class="line">    uip = input(<span class="string">'请输入包名'</span>)</span><br><span class="line">    content = collection.find(&#123;<span class="string">'name'</span>: &#123;<span class="string">'$regex'</span>: <span class="string">'&#123;&#125;.*?'</span>.format(uip), <span class="string">'$options'</span>: <span class="string">'i'</span>&#125;&#125;, &#123;<span class="string">'_id'</span>: <span class="number">0</span>&#125;)</span><br><span class="line">    <span class="comment"># collection.fin(&#123;'name': &#123;'$regex': 'xxx', '$options' : 'i'&#125;&#125;)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> content:</span><br><span class="line">        print(i[<span class="string">'name'</span>])</span><br><span class="line">    print(<span class="string">'请根据自己的python版本和windows版本选择合适的安装包'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span><span class="params">(packname)</span>:</span></span><br><span class="line">    headers = &#123;<span class="string">'Accept'</span>: <span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8'</span>, <span class="string">'Accept-Encoding'</span>: <span class="string">'gzip, deflate, br'</span>, <span class="string">'Accept-Language'</span>: <span class="string">'zh-CN,zh;q=0.9'</span>, <span class="string">'Connection'</span>: <span class="string">'keep-alive'</span>, <span class="string">'Host'</span>: <span class="string">'download.lfd.uci.edu'</span>, <span class="string">'Referer'</span>: <span class="string">'https://www.lfd.uci.edu/~gohlke/pythonlibs/'</span>, <span class="string">'Upgrade-Insecure-Requests'</span>: <span class="string">'1'</span>, <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36'</span>&#125;</span><br><span class="line"></span><br><span class="line">    url = <span class="string">"https://download.lfd.uci.edu/pythonlibs/h2ufg7oq/"</span>+packname</span><br><span class="line">    print(packname)</span><br><span class="line">    print(url)</span><br><span class="line">    html = requests.get(url, headers=headers).content<span class="comment">#获得文件二进制信息</span></span><br><span class="line">    <span class="keyword">with</span> open(packname, <span class="string">"wb"</span>) <span class="keyword">as</span> f:<span class="comment">#打开文件</span></span><br><span class="line">        f.write(html)<span class="comment">#写入二进制</span></span><br><span class="line">    os.system(<span class="string">"pip install &#123;&#125;"</span>.format(packname))<span class="comment">#使用os.system向发送安装指令。</span></span><br><span class="line"></span><br><span class="line">num = <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> num &gt; <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">if</span> num == <span class="number">1</span>:</span><br><span class="line">        spiderdate()</span><br><span class="line">        datefind()</span><br><span class="line">        packname = input(<span class="string">'输入包名：'</span>)</span><br><span class="line">        packname = packname.replace(<span class="string">"‑"</span>, <span class="string">"-"</span>)</span><br><span class="line">        download(packname)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        datefind()</span><br><span class="line">        packname = input(<span class="string">'输入包名：'</span>)</span><br><span class="line">        packname = packname.replace(<span class="string">"‑"</span>, <span class="string">"-"</span>)</span><br><span class="line">        download(packname)</span><br><span class="line">    num = num + <span class="number">1</span></span><br></pre></td></tr></table></figure><p>源代码直接复制到pycharm中，如果我没猜错的话，应该就能正常运行啦！</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>虽然只有短短的五六十行代码，但这也能算得上是我写的第一个小程序了。收获还是有的呀！</p><p>这五六十行代码就已经让我头皮发麻了，不敢想象以后写那么多代码的样子。</p><p><img src="https://i.loli.net/2018/10/26/5bd28fafc9f87.png" alt=""> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2018/10/26/5bd285d4ebe61.png&quot; alt=&quot;&quot;&gt; &lt;/p&gt;
&lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h
      
    
    </summary>
    
      <category term="小程序" scheme="http://yoursite.com/categories/%E5%B0%8F%E7%A8%8B%E5%BA%8F/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="MongoDB" scheme="http://yoursite.com/tags/MongoDB/"/>
    
  </entry>
  
  <entry>
    <title>XPath学习</title>
    <link href="http://yoursite.com/2018/10/24/XPath%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2018/10/24/XPath学习/</id>
    <published>2018-10-24T11:12:20.000Z</published>
    <updated>2018-10-24T13:57:17.569Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2018/10/24/5bd0536f610da.png" alt=""> </p><h1 id="XPath介绍"><a href="#XPath介绍" class="headerlink" title="XPath介绍"></a>XPath介绍</h1><p>​    XPath（XML Path）是一种查询语言，它在XML( Extensible Markup Language,可标记扩展语言)和HTML的树状结构中寻找节点。形象一点来说，XPath就是一种根据“地址”来”找人“的语言。</p><p>​    用正则表达式来提取信息，经常会出现不明原因的无法提取想要内容的情况。解决起来很麻烦，需要寻找的内容越复杂，构造正则表达式所需要花费的时间也就越多。而XPath却不一样，熟练使用XPath以后，构造不同的XPath，所需要的时间几乎一样，所以使用XPath从HTML源代码中提取信息可以大大提高效率。</p><p>​    在Python中，为了使用XPath，需要安装一个第三方库。</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>windows下的安装比较复杂，直接使用pip install lxml会很容易出问题，这是因为lxml的底层时使用C语言实现的，所以计算机上面需要安装Virtual C++运行库。但是即便安装好了运行库，还是有可能出问题。所以我们换一种方法。</p><p>打开<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml" target="_blank" rel="noopener">链接</a></p><p>根据自己计算机的pyhton版本下载对应的whl包。下载完成后吧这个包放到python安装文件夹下的Lib\site-packages文件夹中然后在这个文件夹中打开命令行窗口，运行</p><p>pip install 文件名</p><p>文件名就是刚才下载的whl文件名，注意，要输入完整文件名包含后缀。</p><p>如果使用这种方法还不行的话，那就把whl文件后缀名改成zip然后使用压缩工具进行解压到当前文件夹就可以了。</p><h1 id="XPath语法"><a href="#XPath语法" class="headerlink" title="XPath语法"></a>XPath语法</h1><ol><li><p>XPath语句格式</p><p>核心思想就是写XPath就是写地址</p><p>获取文本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">//[标签][@属性1=“属性值1”]/标签2[@属性2=“属性值2”]/.../text()</span><br></pre></td></tr></table></figure><p>获取属性：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">//[标签][@属性1=“属性值1”]/标签2[@属性2=“属性值2”]/.../@属性n</span><br></pre></td></tr></table></figure><p>其中[@属性=“属性值”]不是必须的。它的作用是帮助过滤相同的标签。在不需要过滤相同标签的时候可以省略</p></li><li><p>标签的选取</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> lxml.html</span><br><span class="line"></span><br><span class="line">source = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;html&gt;</span></span><br><span class="line"><span class="string">  &lt;head&gt;</span></span><br><span class="line"><span class="string">    &lt;title&gt;测试&lt;/title&gt;</span></span><br><span class="line"><span class="string">  &lt;/head&gt;</span></span><br><span class="line"><span class="string">  &lt;body&gt;</span></span><br><span class="line"><span class="string">    &lt;div class="useful"&gt;</span></span><br><span class="line"><span class="string">      &lt;ul&gt;</span></span><br><span class="line"><span class="string">        &lt;li class="info"&gt;我需要的信息1&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;li class="info"&gt;我需要的信息2&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;li class="info"&gt;我需要的信息3&lt;/li&gt;</span></span><br><span class="line"><span class="string">      &lt;/ul&gt;</span></span><br><span class="line"><span class="string">     &lt;/div&gt;</span></span><br><span class="line"><span class="string">     &lt;div class="useless"&gt;</span></span><br><span class="line"><span class="string">       &lt;ul&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="info"&gt;垃圾1&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="info"&gt;垃圾2&lt;/li&gt;</span></span><br><span class="line"><span class="string">       &lt;/ul&gt;</span></span><br><span class="line"><span class="string">     &lt;/div&gt;</span></span><br><span class="line"><span class="string">  &lt;/body&gt;</span></span><br><span class="line"><span class="string">&lt;/html&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">selector = lxml.html.fromstring(source)</span><br><span class="line">useful = selector.xpath(<span class="string">'//div[@class="useful"]/ul/li/text()'</span>)<span class="comment">#这个时候获取到的是useful标签里的内容</span></span><br></pre></td></tr></table></figure><p>既然是写地址，很多人肯定好奇不是应该写成</p><p>/html/body/[@class=”usefli”]/ul/li/text()</p><p>实际上这么写也没有错，但是因为我们需要找的是class=”useful”标签里的内容，这个标签在整个HTML代码中已经足够特别了，没有必要再加上html，body标签了，就像我们写快递收货地址一样，没有人会写亚洲，中国，北京。因为大家都知道全世界只有一个北京，在中国。</p><p>写XPath最重要的就是找这个标志性的属性值。</p></li><li><p>那些属性可以省略呢</p><p>在上面的代码中因为ul标签本身就没有属性，所以可以省略，那li标签命名有属性为什么也省略了呢？</p><p>这是因为li标签中的属性都一样，全都是li class=”info”所以可以省略</p></li><li><p>XPath的特殊情况</p><ol><li>以相同字符串开头</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> lxml.html</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">html1 = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="string">&lt;html&gt;</span></span><br><span class="line"><span class="string">&lt;head lang="en"&gt;</span></span><br><span class="line"><span class="string">    &lt;meta charset="UTF-8"&gt;</span></span><br><span class="line"><span class="string">    &lt;title&gt;&lt;/title&gt;</span></span><br><span class="line"><span class="string">&lt;/head&gt;</span></span><br><span class="line"><span class="string">&lt;body&gt;</span></span><br><span class="line"><span class="string">    &lt;div id="test-1-k"&gt;需要的内容1&lt;/div&gt;</span></span><br><span class="line"><span class="string">    &lt;div id="test-2-k"&gt;需要的内容2&lt;/div&gt;</span></span><br><span class="line"><span class="string">    &lt;div id="testfault-k"&gt;需要的内容3&lt;/div&gt;</span></span><br><span class="line"><span class="string">    &lt;div id="useless"&gt;这是我不需要的内容&lt;/div&gt;</span></span><br><span class="line"><span class="string">&lt;/body&gt;</span></span><br><span class="line"><span class="string">&lt;/html&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><p>在上面的HTML代码中，我们需要抓取需要的内容1,2,3,如果不指定div标签的属性，那么就会把不需要的内容也提取出来。但是如果指定了div标签的属性就只能抓取一条信息了。这个时候，就需要用XPath提取所有id以test开头的div标签。</p><p>在XPath中，属性以某些字符串开头，可以写为：</p><p>//标签[starts-with(@属性名,”相同的开头部分”)]</p><p>在上面的代码中就可以写成:</p><p>//div[starts-with(@class,”test”)]/text()</p><ol start="2"><li>属性值包含相同字符串</li></ol><p>寻找属性值包含某些相同字符串的元素时，XPath的写法格式和上面的写法格式是相同的，只不过把关键字starts-with换成了contains。</p><ol start="3"><li>对XPath返回的对象执行XPath</li></ol><p>XPath也支持先抓大在抓小，就是先把包含想要的信息的那一大部分内容先使用XPath提取出来，再使用XPath提取一次。</p><p>如上面的代码我们下吧div这一部分的标签全部都提取出来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">selector = lxml.formstring(html1)</span><br><span class="line">useful = seletot.xpath(<span class="string">'//body'</span>)<span class="comment">#返回一个列表</span></span><br><span class="line">info_list = useful[<span class="number">0</span>].xpath(<span class="string">'div[class="useless"]/text()'</span>)</span><br><span class="line">print(info_list)</span><br></pre></td></tr></table></figure><p>第一个XPath返回的是一个XPath对象在第二次对这个返回对象使用XPath的时候，开头不需要添加斜线，直接以标签名字开始即可。</p><ol start="4"><li>不同标签下的文字</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">html3 = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="string">&lt;html&gt;</span></span><br><span class="line"><span class="string">&lt;head lang="en"&gt;</span></span><br><span class="line"><span class="string">    &lt;meta charset="UTF-8"&gt;</span></span><br><span class="line"><span class="string">    &lt;title&gt;&lt;/title&gt;</span></span><br><span class="line"><span class="string">&lt;/head&gt;</span></span><br><span class="line"><span class="string">&lt;body&gt;</span></span><br><span class="line"><span class="string">    &lt;div id="test3"&gt;</span></span><br><span class="line"><span class="string">        我左青龙，</span></span><br><span class="line"><span class="string">        &lt;span id="tiger"&gt;</span></span><br><span class="line"><span class="string">            右白虎，</span></span><br><span class="line"><span class="string">            &lt;ul&gt;上朱雀，</span></span><br><span class="line"><span class="string">                &lt;li&gt;下玄武。&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;/ul&gt;</span></span><br><span class="line"><span class="string">            老牛在当中，</span></span><br><span class="line"><span class="string">        &lt;/span&gt;</span></span><br><span class="line"><span class="string">        龙头在胸口。</span></span><br><span class="line"><span class="string">    &lt;/div&gt;</span></span><br><span class="line"><span class="string">&lt;/body&gt;</span></span><br><span class="line"><span class="string">&lt;/html&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment">#如果使用一般的办法，就会出现获取到的数据不完整的情况</span></span><br><span class="line">selector = lxml.html.fromstring(html3)</span><br><span class="line"><span class="comment"># content_1 = selector.xpath('//div[@id="test3"]/text()')</span></span><br><span class="line"><span class="comment"># for each in content_1:</span></span><br><span class="line"><span class="comment">#     print(each)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用string(.)就可以把数据获取完整</span></span><br><span class="line">data = selector.xpath(<span class="string">'//div[@id="test3"]'</span>)[<span class="number">0</span>]</span><br><span class="line">info = data.xpath(<span class="string">'string(.)'</span>)</span><br><span class="line">print(info)</span><br></pre></td></tr></table></figure><p>这段代码其实也是用了先抓大后抓小的方法，先把div标签取出来，但不提取信息，然后在对返回的XPath对象在使用一次XPath，提取这个XPath对象里面的所有字符串。这里用到了一个新的关键字string(.)</p></li></ol><h1 id="使用浏览器开发者工具辅助构造XPath"><a href="#使用浏览器开发者工具辅助构造XPath" class="headerlink" title="使用浏览器开发者工具辅助构造XPath"></a>使用浏览器开发者工具辅助构造XPath</h1><p>在构造XPath的时候，需要寻找“标志性”的标签。但是网页的源代码往往是混乱的，这个时候依靠肉眼来找就很麻烦了。这时借助开发者工具来构造XPath就能大大提高效率啦！</p><p>打开浏览器，找到我们想要获取的信息</p><p>比如我们要获取下图中图片的位置：</p><p><img src="https://i.loli.net/2018/10/24/5bd078036c763.png" alt=""> </p><p>鼠标放在图片的位置，然后右键选择检查，就定位到了源代码中图片的位置。</p><p>然后右键点击源代码中的位置选择Copy→Copy XPath，然后粘贴下如下：</p><p>//*[@id=”posts”]/article/div/div[1]/p[1]/a/img</p><p>这个XPath写法可以直接被lxml解析。方框中的数字代表这是第几个该标签。如div[1]/p[1]代表这是第一个div标签第一个p标签。这里的数字是从1开始的，可不是像编程语言中从0开始。</p><p>这个从浏览器中复制下来的XPath只能获取这一个标签的信息。如果我们想获得这一类标签的信息，例如得到所有图片，就需要将复制下来的XPath作为参考结合网页源代码的结构，手动构造范围更大的更容易读的XPath。</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2018/10/24/5bd0536f610da.png&quot; alt=&quot;&quot;&gt; &lt;/p&gt;
&lt;h1 id=&quot;XPath介绍&quot;&gt;&lt;a href=&quot;#XPath介绍&quot; class=&quot;headerlink&quot; title=&quot;XPa
      
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="XPath" scheme="http://yoursite.com/tags/XPath/"/>
    
  </entry>
  
  <entry>
    <title>Fiddler抓包工具总结</title>
    <link href="http://yoursite.com/2018/10/24/Fiddler%E6%8A%93%E5%8C%85%E5%B7%A5%E5%85%B7%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2018/10/24/Fiddler抓包工具总结/</id>
    <published>2018-10-24T09:08:22.000Z</published>
    <updated>2018-10-24T10:35:34.615Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>我们使用计算机上的浏览器或者客户端软件要与外界进行通信，就必然会有数据的发送或接收，有的时候，我们需要对这些传递的数据进行分析，就需要截获这些传递的数据，其中对这些数据进行截获、重发、编辑、转存的过程叫做抓包。在写爬虫的时候，抓包分析用得相对来说也是较多的，要进行抓包，可以通过一些常见的抓包软件来实现，Fiddler就是一种常见的比较好用的抓包软件。</p><p>在写爬虫的时候借助Fiddler能够帮你你模拟出最真实的浏览器请求。</p><h1 id="什么是Fiddler"><a href="#什么是Fiddler" class="headerlink" title="什么是Fiddler"></a>什么是Fiddler</h1><p>Fiddler是一种常见的抓包分析软件，同时，我们可以利用Fiddler详细的对HTTP请求进行分析，并模拟对应的HTTP请求。<br>目前抓包软件有很多，除了Fiddler之外，常见的还有：  </p><ol><li>浏览器自带的调试工具，按f12可以调出。缺点：比较轻量，不能支持一些复杂的抓包。</li><li>Wireshark，这是一款通用的抓包工具，功能比较齐全，正因为功能比较齐全，所以较为庞大，而我们写爬虫的时候主要是分析HTTP请求，所以这款软件的很多功能用不到。</li></ol><h1 id="爬虫和Fiddler的关系"><a href="#爬虫和Fiddler的关系" class="headerlink" title="爬虫和Fiddler的关系"></a>爬虫和Fiddler的关系</h1><p>​    网络爬虫是自动爬取网页的程序，在爬取的过程中必然涉及客户端与服务端之间的通信，自然也需要发送一些HTTP请求，并接受服务器返回的结果。在一些复杂的网络请求中，我们很难看到网址的变化规律，这就很难手动构造来请求来自动爬取网页了。</p><p>​    比如在浏览一些网页是，浏览到最下面的时候会出现一个‘’加载更多“的字样，此时点击就会加载出更多内容，然而我们观察浏览器中的网站并没有变化，便也无法分析出浏览器向服务器发送了什么数据。</p><p>​    此时可以使用Fiddler进行抓包，并对这些数据进行分析，这样就可以分析出实现”加载更多“的请求了。</p><h1 id="安装和使用"><a href="#安装和使用" class="headerlink" title="安装和使用"></a>安装和使用</h1><p><a href="https://www.telerik.com/download/fiddler" target="_blank" rel="noopener">Fiddler下载地址</a>从官网下载完成后安装，安装完成后打开</p><h2 id="字段说明"><a href="#字段说明" class="headerlink" title="字段说明"></a>字段说明</h2><p>Fiddler想要抓到数据包，要确保Capture Traffic是开启，在File –&gt; Capture Traffic。开启后再左下角会有显示，当然也可以直接点击左下角的图标来关闭/开启抓包功能。<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160118234157312-154346340.png" alt=""><br>Fiddler开始工作了，抓到的数据包就会显示在列表里面，下面总结了这些都是什么意思：<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160118234158609-143657944.png" alt=""><br><img src="/2018/10/24/Fiddler抓包工具总结/1.png" alt=""><br><img src="/2018/10/24/Fiddler抓包工具总结/2.png" alt=""><br><img src="/2018/10/24/Fiddler抓包工具总结/3.png" alt=""></p><h2 id="Statistics-请求的性能数据分析"><a href="#Statistics-请求的性能数据分析" class="headerlink" title="Statistics 请求的性能数据分析"></a>Statistics 请求的性能数据分析</h2><p>随意点击一个请求，就可以看到Statistics关于HTTP请求的性能以及数据分析了<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160118234218890-2133347180.png" alt=""></p><p>##　Inspectors 查看数据内容<br>Inspectors是用于查看会话的内容，上半部分是请求的内容，下半部分是响应的内容：<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160120130545953-2034481316.png" alt=""></p><p>##　AutoResponder 允许拦截指定规则的请求<br>AutoResponder允许你拦截指定规则的求情，并返回本地资源或Fiddler资源，从而代替服务器响应。</p><p>看下图5步，我将“baidu”这个关键字与我电脑“f:\Users\YukiO\Pictures\boy.jpeg”这张图片绑定了，点击Save保存后勾选Enable rules，再访问baidu，就会被劫持。<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160118234219765-703426619.png" alt=""><br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160118234224843-907043204.png" alt=""><br>AutoResponder有很多匹配规则：　</p><ol><li>字符串匹配（默认）：只要包含指定字符串（不区分大小写），全部认为是匹配<br><img src="/2018/10/24/Fiddler抓包工具总结/4.png" alt=""></li><li>正则表达式匹配：以“regex:”开头，使用正则表达式来匹配，这个是区分大小写的<br><img src="/2018/10/24/Fiddler抓包工具总结/5.png" alt=""><h2 id="Composer-自定义请求发送服务器"><a href="#Composer-自定义请求发送服务器" class="headerlink" title="Composer 自定义请求发送服务器"></a>Composer 自定义请求发送服务器</h2>Composer允许自定义请求发送到服务器，可以手动创建一个新的请求，也可以在会话表中，拖拽一个现有的请求<br>Parsed模式下你只需要提供简单的URLS地址即可（如下图，也可以在RequestBody定制一些属性，如模拟浏览器User-Agent）<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160118234227062-846408602.png" alt=""><h2 id="Filters-请求过滤规则"><a href="#Filters-请求过滤规则" class="headerlink" title="Filters 请求过滤规则"></a>Filters 请求过滤规则</h2>Fiters 是过滤请求用的，左边的窗口不断的更新，当你想看你系统的请求的时候，你刷新一下浏览器，一大片不知道哪来请求，看着碍眼，它还一直刷新你的屏幕。这个时候通过过滤规则来过滤掉那些不想看到的请求。<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160120125401968-22426265.png" alt=""><br>勾选左上角的Use Filters开启过滤器，这里有两个最常用的过滤条件：Zone和Host</li><li>Zone 指定只显示内网（Intranet）或互联网（Internet）的内容：<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160118235316718-1553324600.png" alt=""></li><li>Host 指定显示某个域名下的会话：<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160118235317328-711964625.png" alt=""><br>如果框框为黄色（如图），表示修改未生效，点击红圈里的文字即可<h2 id="Timeline-请求响应时间"><a href="#Timeline-请求响应时间" class="headerlink" title="Timeline 请求响应时间"></a>Timeline 请求响应时间</h2>在左侧会话窗口点击一个或多个（同时按下 Ctrl 键），Timeline 便会显示指定内容从服务端传输到客户端的时间：<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160118235318172-1052872585.png" alt=""><h1 id="Fiddler-设置解密HTTPS的网络数据"><a href="#Fiddler-设置解密HTTPS的网络数据" class="headerlink" title="Fiddler 设置解密HTTPS的网络数据"></a>Fiddler 设置解密HTTPS的网络数据</h1>Fiddler可以通过伪造CA证书来欺骗浏览器和服务器。Fiddler是个很会装逼的好东西，大概原理就是在浏览器面前Fiddler伪装成一个HTTPS服务器，而在真正的HTTPS服务器面前Fiddler又装成浏览器，从而实现解密HTTPS数据包的目的。</li></ol><p>解密HTTPS需要手动开启，依次点击：</p><ol><li>Tools –&gt; Fiddler Options –&gt;  HTTPS<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160118234228140-2037050814.png" alt=""></li><li>勾选Decrypt HTTPS Traffic<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160118234229250-1993071078.png" alt=""></li><li>点击OK<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160118234230343-471116797.png" alt=""><h1 id="Fiddler-抓取Iphone-Android数据包"><a href="#Fiddler-抓取Iphone-Android数据包" class="headerlink" title="Fiddler 抓取Iphone / Android数据包"></a>Fiddler 抓取Iphone / Android数据包</h1><a href="https://forali.club/2018/10/23/%E4%BD%BF%E7%94%A8Fiddler%E5%AF%B9%E6%89%8B%E6%9C%BAAPP%E6%8A%93%E5%8C%85/" target="_blank" rel="noopener">请参考我的另一篇博客</a></li></ol><h1 id="Fiddler-内置命令与断点"><a href="#Fiddler-内置命令与断点" class="headerlink" title="Fiddler 内置命令与断点"></a>Fiddler 内置命令与断点</h1><p>Fiddler还有一个藏的很深的命令框，平时用的时候很容易忽略<br>FIddler断点功能就是将请求截获下来，但是不发送，这个时候你可以干很多事情，比如说，把包改了，再发送给服务器君。还有balabala一大堆的事情可以做，就不举例子了。<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160120125408672-279510487.png" alt=""><br><img src="/2018/10/24/Fiddler抓包工具总结/(Fiddler抓包工具总结/6.png" alt=""><br><img src="/2018/10/24/Fiddler抓包工具总结/(Fiddler抓包工具总结/7.png" alt=""><br>示例：<br>?<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160120125409703-1678932502.png" alt=""><br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160120125414093-255856593.png" alt=""><br>&lt;<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160120125414797-2077867027.png" alt=""><br>=<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160120125415547-1825599853.png" alt=""><br>@<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160120125416093-439467539.png" alt=""><br>select<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160120125417734-817450905.png" alt=""><br>cls<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160120125418343-1404670131.png" alt=""><br>dump<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160120125418906-1712835498.png" alt=""></p><h2 id="断点命令"><a href="#断点命令" class="headerlink" title="断点命令"></a>断点命令</h2><p>断点可以直接点击Fiddler下图的图标位置，就可以设置全部请求的断点，断点的命令可以精确设置需要截获那些请求。如下示例：<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160120125424047-1175695668.png" alt=""><br>命令：<br>bpafter<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160120125424797-1724110564.png" alt=""><br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160120125425984-995334279.png" alt=""><br>bps<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160120125426687-1642870815.png" alt=""><br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160120125430750-1560884194.png" alt=""><br>bpv<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160120125431687-881357645.png" alt=""><br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160120125433422-1492238172.png" alt=""><br>g / go<br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160120125437359-2010010315.png" alt=""><br><img src="https://images2015.cnblogs.com/blog/626593/201601/626593-20160120125438093-1008328282.png" alt=""></p><hr><p>本篇博客借鉴了：<a href="https://www.cnblogs.com/yyhh/p/5140852.html" target="_blank" rel="noopener">链接</a>图片也来自：<a href="https://www.cnblogs.com/yyhh/p/5140852.html" target="_blank" rel="noopener">链接</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;我们使用计算机上的浏览器或者客户端软件要与外界进行通信，就必然会有数据的发送或接收，有的时候，我们需要对这些传递的数据进行分析，就需要截获这
      
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Fiddler" scheme="http://yoursite.com/tags/Fiddler/"/>
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>python爬虫入门</title>
    <link href="http://yoursite.com/2018/10/24/python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/"/>
    <id>http://yoursite.com/2018/10/24/python爬虫入门/</id>
    <published>2018-10-24T02:46:01.000Z</published>
    <updated>2018-10-24T06:50:14.053Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2018/10/24/5bcfe6b3159c8.png" alt=""> </p><h1 id="爬虫基础"><a href="#爬虫基础" class="headerlink" title="爬虫基础"></a>爬虫基础</h1><h2 id="爬虫是什么？"><a href="#爬虫是什么？" class="headerlink" title="爬虫是什么？"></a>爬虫是什么？</h2><p>爬虫是什么？一个自动化的数据收集程序<br>爬虫分类？四类<br>1.通用爬虫–什么内容都爬，比如搜索引擎，百度谷歌<br>2.聚焦爬虫–爬取特定内容<br>3.增量式爬虫-爬取更新的内容<br>4.深层网络爬虫-爬取提交表单后的数据<br>通用爬虫弊端：</p><ul><li>通用搜索引擎返回太多没有用的数据  </li><li>服务器资源有限，而网络数据资源无限 </li><li>网络资源多样化，通用爬虫模型无法很好获取。</li></ul><p>爬虫不是独立的技术–需要：前端开发基础知识 数据库基础 网络基础 抓包分析能力 爬虫框架 </p><h2 id="聚焦爬虫介绍："><a href="#聚焦爬虫介绍：" class="headerlink" title="聚焦爬虫介绍："></a>聚焦爬虫介绍：</h2><p>聚焦爬虫是有目的的爬取，可以节省大量的服务器资源和带宽资源，具有很强的实用性。<br>流程：<br>首先聚焦爬虫有一个控制中心，该控制中心负责对整个爬虫系统进行管理和监控，主要包括控制用户交互、初始化爬行器、确定主题<br>、协调各模块之间的工作、控制爬行过程等方面。<br>然后，将初始的URL集合传递给URL队列，页面爬行模块会从URL队列中读取第一批URL列表，然后根据这些URL地址从互联网中进行相应的页面爬取。爬取后，将爬取到的内容船到页面数据库中存储，同时，在爬行过程中，会爬取到一些新的URL，此时，需要根据我们所定的主题使用连接过滤模块过滤掉无关连接，再将剩下来的URL连接根据主题使用链接评价模块或内容评价模块进行优先级的排序。完成后，将新的URL地址传递到URL队列中，供页面爬行模块使用。另一方面，将页面爬取并存放到页面数据库后，需要根据主题使用页面分析模块对爬取到的页面进行页面分析处理，并根据处理结果机那里索引数据库，用户检索对应信息是，可以从索引数据库中进行相应的检索，并得到对应的结果。</p><h2 id="网络爬虫可以干什么？"><a href="#网络爬虫可以干什么？" class="headerlink" title="网络爬虫可以干什么？"></a>网络爬虫可以干什么？</h2><ol><li>爬取多站新闻，集中阅读</li><li>爬取金融信息，进行投资分析</li><li>制作搜索引擎</li><li>爬取图片</li><li>爬取网站用户公开的信息进行分析</li><li>自动去网页广告</li><li>爬取用户公开的联系方式</li></ol><h2 id="聚焦网络爬虫"><a href="#聚焦网络爬虫" class="headerlink" title="聚焦网络爬虫"></a>聚焦网络爬虫</h2><p>聚焦网络爬虫，由于其需要有目的的进行爬取，所以对于通用网络爬虫来说，必须要增加目标的定义和过滤机制，具体来说，此时其执行原理和过程需要比通用网络爬虫多出三步，即目标的定义、无关链接的过滤、下一步要爬取的URL地址的选取。<br>常见的网络更新策略有三种：用户体验策略、历史数据策略、聚类分析策略<br>聚类分析可以分解商品之间的共性进行相应的处理，将共性较多的商品聚为一类。<br>在爬虫对网页爬取的过程中，爬虫需要必须需要访问对应的网页，此时，正规的爬虫会告诉网站站长其爬虫身份。网站的管理员则可<br>以通过爬虫告知的身份信息对爬虫的身份进行识别，开发网络爬虫的语言有很多，常见的有python、java、PHP、Node.js、C++、Go语言等。</p><h1 id="urllib库"><a href="#urllib库" class="headerlink" title="urllib库"></a>urllib库</h1><p>Urllib库是Python中的一个功能强大、用于操作URL，并在做爬虫的时候经常要用到的库。<br>URL读取内容有三种方式：read；读取全部并且把内容赋给一个字符串变量、readline读取一行、readlines读取全部并且把内容赋给一个列表变量<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">imoprt urllib.request</span><br><span class="line">file=urllib.rqquest.urlopen(<span class="string">"网址"</span>)</span><br><span class="line">data=file.read()</span><br><span class="line">print(data)<span class="comment">#读取网站信息并打印</span></span><br><span class="line">file1 = open(<span class="string">'保存路径及地址’,'</span>w<span class="string">b')#将网页信息以网页的形式保存到本地。</span></span><br><span class="line"><span class="string">file1.write(data)#将变量data写入文件中</span></span><br><span class="line"><span class="string">file1.close#关闭文件</span></span><br></pre></td></tr></table></figure></p><p>除了以上方法之外还可以使用urllib.request里面的urlretrieve()函数直接将对应信息写入本地文件。<br>格式：urllib.request.urlretrieve(url,filename=本地文件地址)。<br>例子:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">urllib.request.urlretrieve(<span class="string">"http://www.baidu.com"</span>,filename=<span class="string">"D:/a/index.html)</span></span><br><span class="line"><span class="string"># 注：urlretrieve执行的过程中会产生一些缓存，我们要清除这些缓存信息，可以使用urlcleanup()进行清除</span></span><br><span class="line"><span class="string">eg：urllib.request.urlcleanup()</span></span><br><span class="line"><span class="string"># 除此之外urllib中还有一些常见的用法：</span></span><br><span class="line"><span class="string">file.info()#返回与当前环境相关的信息。file表示当前爬取的网页，file=urllib.request.urlopen(网址)</span></span><br><span class="line"><span class="string">flie.getcode()#返回状态码，若返回200为正确。</span></span><br><span class="line"><span class="string">file.geturl()#返回当前爬取的URL地址</span></span><br></pre></td></tr></table></figure></p><h2 id="url编解码"><a href="#url编解码" class="headerlink" title="url编解码:"></a>url编解码:</h2><p>url标准中只会允许一部分ASCII字符，而其他的一些字符，比如汉字等，是不符合URL标准的。所以如果我们在URL中使用一些其它不符合<br>标准的字符就会出现问题，此时西药进行URL编码方可解决。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">编码:urllib.request.quote()</span><br><span class="line">eg:a = urllib.request.quote(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line">解码：urllib.request.unquote()</span><br><span class="line">eg:urllib.request.unquote(a)</span><br></pre></td></tr></table></figure></p><h2 id="浏览器的模拟——Headres属性"><a href="#浏览器的模拟——Headres属性" class="headerlink" title="浏览器的模拟——Headres属性"></a>浏览器的模拟——Headres属性</h2><p>当遇到无法爬取的网页时会用到headres属性，因为有些网站为了防止别人恶意采集信息进行了一些反爬虫的设置。<br>这时候就可以设置一些Headres信息，模拟成浏览器去访问这些网站，<br>模拟成浏览器可以设置User-Agent信息。<br>获取User-Agent信息：打开任意一个网站F12调出开发者工具network，任意点击一个链接，使网页发生一个动作。观察下方的窗口会出现<br>一些数据，随意点击一个数据在后侧Headres中可以看到对应的头信息，往下拖动，可以找到User-Agent字样的一串信息。<br>例如：User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36<br>爬虫模拟成浏览器访问页面的设置方法：</p><h3 id="方法1-：使用build-opener-修改报头"><a href="#方法1-：使用build-opener-修改报头" class="headerlink" title="方法1,：使用build_opener()修改报头]"></a>方法1,：使用build_opener()修改报头]</h3><p>由于urlopen()不支持一些HTTP的高级功能，所以，我们如果要修改报头，可以使用urllib.request.build_opener()进行<br>eg:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">url=<span class="string">"要爬取的网址"</span></span><br><span class="line">headers=(<span class="string">"User-Agent"</span>,<span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36"</span>)</span><br><span class="line">opener = urllib.request.build_opener()</span><br><span class="line">opener.addheaders = [headers]</span><br><span class="line">data = opener.open(url).read()</span><br></pre></td></tr></table></figure><p>上述代码中，首先，我们定义了一个变量url存储要爬取的网址，然后在定义一个变量headers存储对应的User-Agent信息，定义的格式为<br>(“User-Agent”,具体信息)，具体信息我们已经从浏览器中获取了，该信息获取一次即可，以后再爬取其他网站的时候可以直接用。<br>然后我们需要使用urllib.request.build_opener()创建自定义的opener对象并赋给变量opener，接下来，设置opener对象的addheaders，<br>即设置对应的头信息，设置格式为：”opener对象名.addheaders=[头信息]”，设置好头信息之后，我们就可以使用opener对象的open()<br>方法打开对应的网址了。此时，打开操作已经是具有头信息的打开操作行为，即会模仿为浏览器去打开，使用格式是：<br>opener对象名.open(url地址)打开后再使用read()方法读取对应数据，并赋给data变量.  </p><h3 id="方法2：使用add-header-添加报头"><a href="#方法2：使用add-header-添加报头" class="headerlink" title="方法2：使用add_header()添加报头]"></a>方法2：使用add_header()添加报头]</h3><p>import urllib.request<br>url=网址<br>req=url.request.Request(url)#创建Request对象并赋给变量req<br>req.add.header(‘User-Agent’,’Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36’)#添加报头信息<br>data=urllib.request.urlopen(req).read()#打开对应网址并读取网页内容赋给data  </p><h2 id="超时设置"><a href="#超时设置" class="headerlink" title="超时设置"></a>超时设置</h2><p>有的时候，我们访问一个网页，如果网页长时间未响应，那么系统就会怕短该网页超时了，即无法打开该网页。<br>有的时候，我们需要根据自己的需要来设置超时的时间值。网站反应快就时间值就设置短一点，网站反应慢就设置长一点。<br>eg：将网站超时时间设置为1</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">for i in range(1,100):</span><br><span class="line">    try:</span><br><span class="line">    file=urllib.request.urlopen(&quot;http://www.baidu.com&quot;,timeout=1)</span><br><span class="line">        data=file.read()</span><br><span class="line">        print(len(data))</span><br><span class="line">    except Exception as e:</span><br><span class="line">        print(&quot;出现异常--&quot;+str(e))</span><br></pre></td></tr></table></figure><p>上述代码循环了99次每次都输出获得数据的长度，执行之后可以发现有几次出现了异常，这是因为我们设置了超时，网站在1秒钟之内没<br>有做出回应的话代码自动判定为超时异常，并输出异常信息。</p><h2 id="HTTP协议请求"><a href="#HTTP协议请求" class="headerlink" title="HTTP协议请求"></a>HTTP协议请求</h2><p>HTTP协议请求主要分为6种类型：</p><ol><li>GET请求，GET请求会通过URL网址传递信息，可以直接在URL中写上要传递的信息，也可以由表单进行传递。如果使用表单进行传递，表单中的信息会自动转为URL地址中的信息，通过URL地址传递。</li><li>POST请求，可以向服务器提交数据，是一种比较主流也比较安全的数据传递方式，比如在登录时，经常使用POST请求发送数据。</li><li>PUL请求，请求服务器存储一个资源，通常要制定存储的位置。</li><li>DELETE请求，请求服务器删除一个资源</li><li>HEAD请求，请求获取对应的HTTP报头信息。</li><li>OPTIONS请求，可以获得当前URL所之气的请求类型。</li></ol><h3 id="get请求："><a href="#get请求：" class="headerlink" title="get请求："></a>get请求：</h3><p>我们可以构造一个url让代码去获取我们想要的信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">key=<span class="string">"aa"</span></span><br><span class="line">url=<span class="string">"http://www.baidu.com/s?wd="</span>+key<span class="comment">#构造url</span></span><br><span class="line">req=urllib.request.Request(url)<span class="comment">#创建一个Request对象并赋给req</span></span><br><span class="line">data=urllib.request.urlopen(req).read()<span class="comment">#获取网页信息并读取出来赋给data变量。</span></span><br><span class="line">print(len(data))</span><br><span class="line">file1=open(<span class="string">"D:/a1/xx1.html"</span>,<span class="string">"wb"</span>)</span><br><span class="line">file1.write(data)<span class="comment">#将信息写入xx1.html文件。</span></span><br><span class="line">file1.close()</span><br></pre></td></tr></table></figure><p>上述代码有不完善的地方，如果我们要检索的关键词是中文，就会报错。<br>这时就可以用到urllib.request.quote()函数来编码解决。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">key=<span class="string">"中文"</span></span><br><span class="line">new_key=urllib.request.quote(key)</span><br><span class="line">url=<span class="string">"http://www.baidu.com/s?wd="</span>+new_key</span><br></pre></td></tr></table></figure></p><p>总结：<br>使用URL请求：</p><ol><li>构造URL地址</li><li>构造Request对象</li><li>打开Request对象。</li><li>读取网页内容、将内容写入文件。如果参数中含有中文要使用quote函数对参数来编码。</li></ol><h3 id="post请求"><a href="#post请求" class="headerlink" title="post请求"></a>post请求</h3><p>post请求常用于登录、注册页面</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line">url = <span class="string">"http://www.iqianyue.com/mypost/"</span></span><br><span class="line">postdata = urllib.parse.urlencode(&#123;<span class="string">'name'</span>:<span class="string">'yudeqiang'</span>,<span class="string">'pass'</span>:<span class="string">'123456'</span>&#125;).encode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="comment">#将数据使用urlencode编码处理后，使用encode()设置为utf-8编码</span></span><br><span class="line">req = urllib.request.Request(url,postdata)<span class="comment">#构建Request对象参数包括url和要传递的数据</span></span><br><span class="line"><span class="comment">#添加头信息模拟浏览器进行爬取</span></span><br><span class="line">req.add_header(<span class="string">"User_Agent"</span>,<span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36"</span>)</span><br><span class="line">data=urllib.request.urlopen(req).read()</span><br><span class="line">print(len(data))</span><br></pre></td></tr></table></figure><h2 id="代理服务器的设置。"><a href="#代理服务器的设置。" class="headerlink" title="代理服务器的设置。"></a>代理服务器的设置。</h2><p>有时使用同一个ip去爬取同一个网站上的网页，久了之后会被该网站服务器屏蔽，使用代理服务器就可以解决这个问题。<br>eg：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">user_proxy</span><span class="params">(proxy_addr,url)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> urllib.request</span><br><span class="line">    proxy = urllib.request.ProxyHandler(&#123;<span class="string">'http'</span>:proxy_addr&#125;)</span><br><span class="line">    opener = urllib.request.build_opener(proxy,urllib.request.HTTPHandler)</span><br><span class="line">    urllib.request.install_opener(opener)</span><br><span class="line">    data = urllib.request.urlopen(url).read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line">proxy_addr = <span class="string">"14.215.194.75:34397"</span></span><br><span class="line">data = user_proxy(proxy_addr,<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line">print(len(data))</span><br></pre></td></tr></table></figure><p>我们首先建立了一个名为use_proxy的自定义函数，该函数主要实现使用代理服务器来爬取某个URL网页的功能。在函数中，我们设置两个形参，第一个形参为代理服务的地址，第二个形参代表要爬取的网页的地址。<br>然后，使用urllib.request.ProxyHandler()来设置对应的代理服务器信息，设置格式为：urllib.request.ProxyHandler({‘http’:<br>代理服务器地址})，接下来，使用urllib.request.build_opener()创建了一个自定义的opener对象，第一个参数为代理信息，第二个参<br>数为urllib.request.HTTPHandler类<br>为了方便，可以使用urllib.request.install_opener()创建全局默认的opener对象，所以下面才可以直接使用urllib.request.urlopen(<br>)打开对应网址爬取网页并读取，编码后赋给变量data，最后返回data的值给函数。<br>随后，在函数外设置好对应的代理IP地址，然后嗲用地应以函数use_proxy，并传递两个实参，跟别为使用的代理地址及要爬取的网址。<br>将函数的调用结果直接赋值给变量data，并输出data内容的长度。或者也可以将data的值写进某个文件中存储起来。</p><h2 id="DebugLog实战"><a href="#DebugLog实战" class="headerlink" title="DebugLog实战"></a>DebugLog实战</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">httphd = urllib.request.HTTPHandler(debuglevel=<span class="number">1</span>)</span><br><span class="line">httpshd = urllib.request.HTTPHandler(debuglevel=<span class="number">1</span>)</span><br><span class="line">opener = urllib.request.build_opener(httphd,httpshd)</span><br><span class="line">urllib.request.install_opener(opener)</span><br><span class="line">data=urllib.request.urlopen(<span class="string">"http://edu.51cto.com"</span>)</span><br></pre></td></tr></table></figure><p>如何开启DebugLog？思路如下：  </p><ol><li>分别使用urllib.request.HTTPHandler()和urllib.request.HTTPSHandler()将debuglevel设置为1  </li><li>使用urllib.request.build_opener()创建自定义的opener对象，并使用1中设置的值作为默认参数。  </li><li>用urllib.request.install_opener()创建全局默认的opener对象，这样在使用urlopen()时也会使用我们安装的opener对象 </li><li>进行后续相应的操作。</li></ol><h2 id="异常处理神器——URLError实战"><a href="#异常处理神器——URLError实战" class="headerlink" title="异常处理神器——URLError实战"></a>异常处理神器——URLError实战</h2><p>程序再执行的过程中，难免会发生异常，常见的异常处理方法是try except<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    urllib.request.urlopen(<span class="string">"http://ww1w.baidu.com"</span>)</span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.code)</span><br><span class="line">    print(e.reson)</span><br></pre></td></tr></table></figure></p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2018/10/24/5bcfe6b3159c8.png&quot; alt=&quot;&quot;&gt; &lt;/p&gt;
&lt;h1 id=&quot;爬虫基础&quot;&gt;&lt;a href=&quot;#爬虫基础&quot; class=&quot;headerlink&quot; title=&quot;爬虫基础&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="urllib" scheme="http://yoursite.com/tags/urllib/"/>
    
  </entry>
  
  <entry>
    <title>HTTP理论基础</title>
    <link href="http://yoursite.com/2018/10/24/HTTP%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2018/10/24/HTTP理论基础/</id>
    <published>2018-10-24T01:37:49.000Z</published>
    <updated>2018-10-24T03:35:31.426Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.pixabay.com/photo/2015/11/10/21/45/frogs-1037868__340.jpg" alt=""></p><h1 id="HTTP理论基础"><a href="#HTTP理论基础" class="headerlink" title="HTTP理论基础"></a>HTTP理论基础</h1><h2 id="计算机网络体系结构"><a href="#计算机网络体系结构" class="headerlink" title="计算机网络体系结构"></a>计算机网络体系结构</h2><p>OSI模型：osi参考模型讲计算机网络分为七层：（从低到高）物理层、数据链路层、网络层、传输层、会话层、表示层、应用层。<br>TCP/IP参考模型：TCP IP 被分为4层模型，分别是：网络接口层、网际层、传输层、应用层。<br> 它把OSI模型的应用层表示层会话层合成应用层、数据链路层和物理层合为网络接口层。</p><ol><li>网络接口层：由物理层和数据链路层合并而成。这层定义了与不同的网络进行连接的接口，网络接口层负责吧IP数据包发送到网络传输介质(双绞线、光纤等)<br> 上传输，以及从网络传输介质上接收数据并解封，取出数据包并交给上一层网际层。</li><li>网际层：主要功能是负责将数据封装成包，并从源主机发送到目的主机，解决如何进行数据包的路由选择、阻塞控制、网络互连等问题。<br>IP协议：网间互联协议 网际层的核心协议，另外还有一些辅助协议，包括ARP(地址解析协议)、RARP(逆向地址解析协议)、ICMP(网间控制报文协议)以及IGMP(互联网组管理协议)协议等。<br>负责ip数据报在网络间寻址。IP协议可以进行IP数据包的分割与封装，封装前在数据包前加上源主机的ip地址和目的主机的ip地址及其他信息。<br>特点：只提供传输，不负责纠错。<br>ARP:负责将IP地址解析为物理地址，以便按该地址发送和接收数据。<br>RARP:负责将物理地址解析为IP地址。<br>ICMP:用于在主机和路由器之间传递控制消息，指出网络不通、为用户群进行软件升级、共享白板式多媒体应用等，这些情况就是多播。<br>IGMP:负责对IP多播组进行管理，包括多播组成员的加入和删除等。</li><li>传输层：负责在源主机和目的主机的应用进程之间提供端到端的数据传输服务。负责数据分段、数据确认、丢失和重传等。<br> TCP:传输控制协议 是一个可靠的、面向连接的端对端的传输层协议，由TCP提供的连接叫虚连接。在发送方，TCP将用户提交的字节流分割成<br> 若干个数据段并传递给网际层进行打包发送；在接收方，TCP将所接收的数据包重新装配并交付给用户，它通过序列确认及包重发机制解决IP协议传输时的错误<br> UDP:用户数据报协议 是一个不可靠的、面向无连接的出传输层协议。使用UDP协议发送报文之后无法得知其是否安全到达。UDP协议将可靠性问题交给应用程序来<br> 解决。UDP协议应用于对那些可靠性要求不高，但要求网络延迟较小的场合，如语音和视频数据的传送。<br> -端口：为了识别传输层之上的各个不同的网络应用进程，传输层引入了端口的概念。要进行网络通信的进程向系统提出申请，系统返回一个唯一的端口号，将<br> 进程和端口号联系在一起，成为绑定。传输层使用其报文头中的端口号，吧收到的数据送到不同的应用程序。端口是一种软件结构，包括一些<br> 数据结构和I/O缓冲区。一些端口经常会被黑客、木马病毒所利用。</li><li>应用层：TCPIP的应用层综合了OSI应用层、表示层、以及会话层的功能。应用层为用户的应用程序提供了访问网络服务的能力并定义了不同主机上的应用程序之间交换用户<br> 数据的一系列协议。由于不同的网络对网络服务的需求各不相同，因此应用层协议非常丰富，并且不断有新的协议加入，<br> 应用层的常用协议：<ul><li>超文本传输协议：HTTP 用于获取万维网上的网页信息</li><li>文件传输协议：FTP 用于点对点的文件传输</li><li>简单邮件传输协议：SMTP 用于发送邮件以及在邮件服务器之间转发邮件</li><li>邮局协议：POP用于重邮件服务器上获取邮件</li><li>仿真终端协议：TELNET 用于远程登录到网络主机</li><li>域名系统：DNS 域名解析 用于将主机域名解析为IP地址</li><li>简单网络管理协议：SNMP 用于从网络设备（路由器、网桥、集线器等）中收集网络管理信息。  </li></ul></li></ol><h2 id="TCPIP总结"><a href="#TCPIP总结" class="headerlink" title="TCPIP总结"></a>TCPIP总结</h2><p>网络体系结构：计算机网络的层次及各层协议和层间接口的集合被称为网络结构<br>IP提供的主要功能：1、寻址与路由2、数据包分割和重组，在数据包前加上源主机和目的主机的IP地址。<br>网络接口层负责吧IP数据包发送到网络传输介质上传输，以及从网络传输介质上接收数据并解析，取出数据包交给网际层，<br>网际层负责将数据封装成包，并从源主机发送到目的主机，解决的是数据包的路由选择、阻塞控制和网络互连等问题<br>传输层负责在源主机和目的主机的应用程序之间提端到端的数据传输服务，负责数据分段、数据确认、丢失和重传等。<br>应用层为用户的应用程序提供了访问网络服务的能力并定义了不同主机的应用程序之间交换用户数据的一系列协议。<br>简单来说就是网络接口层吧数据送到网络传输介质上（网线）再把数据取出交给网际层，网际层封装数据并发送，传输层确保传输的数据是正确的应用层使用传输的各种数据在用户的应用程序上进行数据的交换等。  </p><p>书上的总结：TCP协议先把数据分成若干数据报，并给每个数据加上一个TCP信封，上面写上数据报的编号，以便在接收端吧数据还原成原来的格式。IP协议把每个TCP信封再套上一个信封，在上面写上接收主机的地址。有了IP信封就可以在物理网络上传送数据。IP协议还具有利用路由算法进行路由选择的功能。这些信封可以通过不同的传输途径（路由）进行传输，由于路径不同以及其他原因，可能出现顺序颠倒、数据丢失、数据重复等问题。这些问题由TCP协议来处理，其具有检查和处理错误的功能，必要时还可以请求发送端重发。因此可以说IP协议负责数据的传输，TCP协议负责数据的可靠传输。</p><h2 id="HTTP"><a href="#HTTP" class="headerlink" title="HTTP"></a>HTTP</h2><p>是应用层的协议，用于获取万维网上的网页信息。<br>当我们浏览器输入百度网址后：</p><ol><li>客户端请求：浏览器访问网址 <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a></li><li>DNS：找到网址对应的IP地址</li><li>HTTP：请求的资源+请求的内容+请求IP地址等。</li><li>TCP:HTTP报文拆装成多个TCP报文，TCP报文按照三次握手可靠的传输。</li><li>IP：网络传输过程中 选择路径、进行中转。</li><li>服务端接收到了多个TCP报文。</li><li>服务端把接收到的TCP报文合并了http报文。读取到了信息。</li></ol><p>服务端接受信息并返回资源给客户端，过程跟上面步骤一致。  </p><ol><li><p>http超文本传输协议）是一个基于请求与响应模式的、无状态的、应用层的协议，常基于TCP的连接方式，HTTP1.1版本中给出一种<br>持续连接的机制，绝大多数的Web开发，都是构建在HTTP协议之上的Web应用。</p></li><li><p>HTTP URL (URL是一种特殊类型的URI，包含了用于查找某个资源的足够的信息)的格式如下：<br><a href="http://host[&quot;:&quot;port][abs_path]" target="_blank" rel="noopener">http://host[&quot;:&quot;port][abs_path]</a><br>http表示要通过HTTP协议来定位网络资源；host表示合法的Internet主机域名或者IP地址；port指定一个端口号，为空则使用缺省端口80；abs_path指定请求资源的URI；如果URL中没有给出abs_path，那么当它作为请求URI时，必须以“/”的形式给出，通常这个工作浏览器自动帮我们完成。<br>eg:<br>​    输入：<a href="http://www.baidu.edu.cn" target="_blank" rel="noopener">www.baidu.edu.cn</a><br>​    浏览器自动转换成：<a href="http://www.baidu.edu.cn/" target="_blank" rel="noopener">http://www.baidu.edu.cn/</a></p></li><li><p>HTTP协议–请求</p><p>http请求由三部分组成，分别是：请求行、消息报头、请求正文</p><p>请求方法（所有方法全为大写）有多种，各个方法的解释如下：</p><ul><li>GET     请求获取Request-URI所标识的资源</li><li>POST    在Request-URI所标识的资源后附加新的数据</li><li>HEAD    请求获取由Request-URI所标识的资源的响应消息</li><li>报头</li><li>PUT     请求服务器存储一个资源，并用Request-URI作为其标识</li><li>DELETE  请求服务器删除Request-URI所标识的资源</li><li>TRACE   请求服务器回送收到的请求信息，主要用于测试或诊断</li><li>CONNECT 保留将来使用</li><li>OPTIONS 请求查询服务器的性能，或者查询与资源相关的选项和需求<br>​            </li></ul></li><li><p>HTTP协议–响应</p><p>在接收和解释请求消息后，服务器返回一个HTTP响应消息。</p><p>HTTP响应也是由三个部分组成，分别是：状态行、消息报头、响应正文</p><p>状态代码有三位数字组成，第一个数字定义了响应的类别，且有五种可能取值：</p><ul><li>1xx：指示信息–表示请求已接收，继续处理</li><li>2xx：成功–表示请求已被成功接收、理解、接受</li><li>3xx：重定向–要完成请求必须进行更进一步的操作</li><li>4xx：客户端错误–请求有语法错误或请求无法实现</li><li>5xx：服务器端错误–服务器未能实现合法的请求<br>常见状态代码、状态描述、说明：</li><li>200 OK      //客户端请求成功</li><li>400 Bad Request  //客户端请求有语法错误，不能被服务器所理解</li><li>401 Unauthorized //请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用 </li><li>403 Forbidden  //服务器收到请求，但是拒绝提供服务</li><li>404 Not Found  //请求资源不存在，eg：输入了错误的URL</li><li>500 Internal Server Error //服务器发生不可预期的错误</li><li>503 Server Unavailable  //服务器当前不能处理客户端的请求，一段时间后可能恢复正常</li></ul></li></ol><p>​    </p><h2 id="IP地址"><a href="#IP地址" class="headerlink" title="IP地址"></a>IP地址</h2><p>Internet网络中所有计算机均称为主机，<br>ip地址由网络号和主机表示组成，目前使用的ipv4协议规定ip地址的长度为32位。一般以4个字节表示，每个字节用十进制表示，所以<br>每个字节的取值是0-255，并且每个字节数之间用.割开，这种记录方法称为“点-分”十进制记号法。<br>网络地址可分为五类</p><ul><li>A类：分配给主要的服务提供商。IP地址的前八位二进制数代表网络类型 取值范围是0-127</li><li>B类：分配给拥有大型网络的机构。。。。。16。。。。。。。。。。。。。。。。128-191</li><li>C类：。。。。。。。。小型网络。。。。。24。。。。。。。。。。。。。。。。192-223</li><li>D类：为多路广播保留。取值224-239</li><li>E类：实验性地址，保留未用240-247</li></ul><p>IP地址结构：<br>网络类型+网络号+主机ID<br>特殊IP：127.0.0.1 localhost本地主机 自己的电脑<br>0.0.0.0 不是ip 请求这个地址代表所有请求被丢弃<br>由于计算机的发展ipv4能表示的ip地址越来越紧张 且网络号即将用尽。所以产生了ipv6协议<br>ipv6使用128位地址。支持的地址数是足够用的。</p><h3 id="子网掩码"><a href="#子网掩码" class="headerlink" title="子网掩码"></a>子网掩码</h3><p>子网掩码(subnet mask)：又叫网络掩码、地址掩码、子网络遮罩，它是一种用来指明一个IP地址的哪些位标识的是主机所在的子网，以及哪些位标识的是主机的位掩码。子网掩码不能单独存在，它必须结合IP地址一起使用。子网掩码只有一个作用，就是将某个IP地址划分成网络地址和主机地址两部分。<br>子网掩码是一个32位地址，用于屏蔽IP地址的一部分以区别网络标识和主机标识，并说明该IP地址是在局域网上，还是在远程网上。<br>子网掩码的设定必须遵循一定的规则。与二进制IP地址相同，子网掩码由1和0组成，且1和0分别连续。子网掩码的长度也是32位，左<br>边是网络位，用二进制数字“1”表示，1的数目等于网络位的长度；右边是主机位，用二进制数字“0”表示，0的数目等于主机位的<br>长度。这样做的目的是为了让掩码与ip地址做按位与运算时用0遮住原主机数，而不改变原网络段数字，而且很容易通过0的位数确定<br>子网的主机数（2的主机位数次方-2，因为主机号全为1时表示该网络广播地址，全为0时表示该网络的网络号，这是两个特殊地址）。只有通过子网掩码，才能表明一台主机所在的子网与其他子网的关系，使网络正常工作。</p><h3 id="网关"><a href="#网关" class="headerlink" title="网关"></a>网关</h3><p>网关(Gateway)：又称网间连接器、协议转换器。网关在网络层以上实现网络互连，是最复杂的网络互连设备，仅用于两个高层协议不同的网络互连。网关既可以用于广域网互连，也可以用于局域网互连。 网关是一种充当转换重任的计算机系统或设备。使用在不同的通信协议、数据格式或语言，甚至体系结构完全不同的两种系统之间，网关是一个翻译器。与网桥只是简单地传达信息不同，网关对收到的信息要重新打包，以适应目的系统的需求。同层–应用层。</p><h3 id="DHCP"><a href="#DHCP" class="headerlink" title="DHCP"></a>DHCP</h3><p>DHCP协议：路由器自动为局域网下的客户端分配局域网ip。好处是比较方便不用手动设置ip。</p><h3 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h3><p>域名解析协议 将域名转化为ip地址  </p><hr><p>原创文章，转载请注明</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://cdn.pixabay.com/photo/2015/11/10/21/45/frogs-1037868__340.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;HTTP理论基础&quot;&gt;&lt;a href=&quot;#HTTP理论基础&quot; class=
      
    
    </summary>
    
      <category term="HTTP" scheme="http://yoursite.com/categories/HTTP/"/>
    
    
      <category term="HTTP" scheme="http://yoursite.com/tags/HTTP/"/>
    
      <category term="TCP/IP" scheme="http://yoursite.com/tags/TCP-IP/"/>
    
  </entry>
  
  <entry>
    <title>日志2018.10.23</title>
    <link href="http://yoursite.com/2018/10/23/%E6%97%A5%E5%BF%972018-10-23/"/>
    <id>http://yoursite.com/2018/10/23/日志2018-10-23/</id>
    <published>2018-10-23T11:04:21.000Z</published>
    <updated>2018-10-23T12:14:38.712Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.pixabay.com/photo/2017/01/14/12/59/iceland-1979445__340.jpg" alt=""></p><p>问题：  </p><p>上午在使用requests.Session()获取登录后的知乎首页时遇到了问题，知乎首页不知道用了什么编码，获取到数据后使用UTF-8/GBK都无法正常解码，都是一堆乱码。现在这个问题还没有解决，手足无措了。</p><p>下午学习scrapy框架时也遇到了很多问题，安装和运行都很麻烦，我打算在学习完这个框架之后再来总结。</p><p>收获：</p><p>在爬取知乎首页时，学习到了decode()的第二个参数，ignore</p><p><img src="https://i.loli.net/2018/10/23/5bcf0509c0b2c.png" alt=""> </p><p>在获取到网页的数据后，有一些特殊字符导致不能使用decode解码</p><p>在这里我直接加上了ignore参数忽略了不能解码的特殊字符  </p><p>scrapy:</p><p>scrapy的安装是真的麻烦：</p><ol><li><p>首先要安装Visual C++ Build Tools，因为接下来要安装的两个文件的底层是基于C语言开发的，所以需要C语言的编译环境</p><p><a href="https://support.microsoft.com/zh-cn/help/2977003/the-latest-supported-visual-c-downloads" target="_blank" rel="noopener">下载地址</a>有可能会提示下载.NET Framework<a href="https://www.microsoft.com/net/download/dotnet-framework-runtime/net452" target="_blank" rel="noopener">下载地址</a></p></li><li><p>安装pywin32,<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/?tdsourcetag=s_pctim_aiomsg" target="_blank" rel="noopener">下载地址</a>下载.whl文件后使用pip install 文件名(包含后缀)</p></li><li><p>安装Twisted,<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/?tdsourcetag=s_pctim_aiomsg" target="_blank" rel="noopener">下载地址</a>安装方法同上</p></li><li><p>然后就是pip install scrapy了</p></li></ol><p>使用：</p><ol><li><p>创建工程命令scrapy startproject 名称</p></li><li><p>cd 名称</p></li><li><p>scrapy genspider 爬虫名 要爬的网址</p><p>eg:scrapy genspider explam baidu.com</p></li><li><p>修改settings文件，修改ROBOTSTXT_OBEY=False</p></li></ol><p>python执行SCRAPY SHELL 提示DEF WRITE(SELF, DATA, ASYNC=FALSE)出错的问题解决：  </p><p>只需要把python里面manhole.py文本的所有的async替换成其他名字就行，修改python\Lib\site-packages\twisted\conch\manhole.py文件</p><p><img src="https://i.loli.net/2018/10/23/5bcf105873189.png" alt=""> 把这里的所有async改个名字就行了</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://cdn.pixabay.com/photo/2017/01/14/12/59/iceland-1979445__340.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;问题：  &lt;/p&gt;
&lt;p&gt;上午在使用requests.Session()获取登
      
    
    </summary>
    
      <category term="日志" scheme="http://yoursite.com/categories/%E6%97%A5%E5%BF%97/"/>
    
    
      <category term="学习" scheme="http://yoursite.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="日志" scheme="http://yoursite.com/tags/%E6%97%A5%E5%BF%97/"/>
    
      <category term="Scrapy" scheme="http://yoursite.com/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>使用Fiddler对手机APP抓包</title>
    <link href="http://yoursite.com/2018/10/23/%E4%BD%BF%E7%94%A8Fiddler%E5%AF%B9%E6%89%8B%E6%9C%BAAPP%E6%8A%93%E5%8C%85/"/>
    <id>http://yoursite.com/2018/10/23/使用Fiddler对手机APP抓包/</id>
    <published>2018-10-23T07:00:47.000Z</published>
    <updated>2018-10-23T07:30:32.569Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.pixabay.com/photo/2018/05/18/15/30/webdesign-3411373__340.jpg" alt=""></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>Fiddler是一款抓包工具，具有很强大的功能，使用Fiddler不仅可以轻松抓取电脑端的数据包，还可以抓取手机、ipad的数据包。</p><h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><p>在<a href="https://www.telerik.com/fiddler" target="_blank" rel="noopener">Fiddler官网</a>下载并安装Fiddler，安装后打开界面如下：</p><p><img src="https://i.loli.net/2018/10/23/5bcec950edf17.png" alt=""> </p><p>因为fiddler抓包的原理就是通过代理，所以被测终端需要和安装fiddler的电脑在同一个局域网中。  </p><p>开启Fiddler的远程连接，Fiddler 主菜单 Tools -&gt; Fiddler Options…-&gt; Connections页签，选中Allowremote computers to connect，并记住端口号为8888，等会设置手机代理时需要。设置好后重启fiddler保证设置生效。设置如下：</p><p><img src="https://i.loli.net/2018/10/23/5bcec9fa6e914.png" alt=""> </p><p>接下来要做的就是手机端的设置啦：  </p><p>手机和电脑必须在同一个局域网内，然后打开wifi，进行以下设置： </p><p><img src="https://i.loli.net/2018/10/23/5bceca9f8eb4a.png" alt=""> </p><p>代理服务器主机名就是电脑ip，端口就是Fiddler监听端口8888  </p><p>然后在手机上点击任意app就可以看到有请求在Fiddler上面流动了</p><p><img src="https://i.loli.net/2018/10/23/5bceccb36efef.png" alt=""> </p><p>Fiddler的强大不止于此，Fiddler还能做很多事情，目前还在学习中。</p><hr><p>原创文章，转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://cdn.pixabay.com/photo/2018/05/18/15/30/webdesign-3411373__340.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerl
      
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Fiddler" scheme="http://yoursite.com/tags/Fiddler/"/>
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>学习记录2018.10.22</title>
    <link href="http://yoursite.com/2018/10/22/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%952018-10-22/"/>
    <id>http://yoursite.com/2018/10/22/学习记录2018-10-22/</id>
    <published>2018-10-22T10:39:52.000Z</published>
    <updated>2018-10-22T13:29:21.926Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.pixabay.com/photo/2017/03/12/14/48/terminalia-catappa-2137221__340.jpg" alt=""></p><h1 id="随便写写"><a href="#随便写写" class="headerlink" title="随便写写"></a>随便写写</h1><p>现在已经晚上了，目前为止今天还没有学习新知识。不知道为什么，每次放完假回来就得一天时间来调整学习状态，说到底还是懒。。。 </p><p>上个周末，英雄联盟s8全球总决赛已经来到了1/4决赛。对于我这个loler来说自然很关注，甚至周六上自习的时候都在偷偷看比赛直播。我玩英雄联盟差不多四年了，s4开始，现在已经s8了。我对这个游戏真是又爱又恨，恨的是自己在这个游戏上面投入了太多时间，说网瘾少年真的一点都不为过，但这个游戏也带给了我很多快乐，对于我们这样一群人来说，娱乐就是几朋友一块开黑玩几把，互相嘴臭。这届世界赛频频爆冷，世界第一赛区，从s3开始统治英雄联盟的最强赛区LCK竟然全部倒在了8强，而我们LPL也仅剩1支开始不被多数人看好的IG挺进了4强。分析这届总决赛，不难发现，传统的打法已经不适应版本了，而LCK，LPL的大多数队伍都还在固步自封，不肯接受新的东西，这才导致了这么多冷门的爆出。最令我失望的就是RNG了，这个从春天开始包揽所有冠军的队伍，竟然倒在了8强。当然我也不是喷子，我也不想喷RNG的表现有多糟糕，也没有心思去贴吧微博骂人。虽然这只是个在外人看来玩物丧志的游戏比赛，但是我从中也明白了很多。骄兵必败，这句话用来形容RNG在合适不过了，这支RNG今年拿了太多冠军，广告代言接到手软。就在S赛期间，UZI又拿到了NIKE的代言</p><p><img src="https://i.loli.net/2018/10/22/5bcdb1ee803fa.png" alt=""> </p><p>这个英雄联盟的ADC选手越来越商业化了。RNG在今年也拿到了很多大牌赞助，奔驰，惠普，谷粒多，这或许也为今年的总决赛惨败埋下了伏笔。全员膨胀，自认为对手G2与自己实力天差地别，人家实力确实跟你有差距，但没有这么离谱吧，把把把对面不当人，从教练到选手。结果就是送给观众一场屎一样的BO5，也已RNG的惨败结束。RNG不是没有实力赢下G2，赛前大部分人（包括解说）都预测3:0带走对面的，结果队员无限膨胀，不认真对待比赛，以惨败收场。总结下来就是RNG输给了骄傲，输给了自己的态度。现在想想8强抽签时候的那副嘴脸真是充满了讽刺。  </p><p>我突然想到了易大师说的话：  </p><ul><li>不要被骄傲遮蔽了双眼</li><li>真正的大师永远都怀着一颗学徒的心</li></ul><p><img src="data:image/jpeg;base64,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" alt=""></p><p>其实关于这个比赛还有很多事情能给我们带来很多启示。  </p><p>等等，我这个题目不是学习记录吗，怎么扯到游戏上了，我佛了。关于游戏，要是让我放开了讲，我能给你说一天一夜，我就不多BB了，毕竟学习现在才是我该干的事情。  </p><h1 id="记录下学习中遇到的问题吧"><a href="#记录下学习中遇到的问题吧" class="headerlink" title="记录下学习中遇到的问题吧"></a>记录下学习中遇到的问题吧</h1><p>今天我好像真的没有看一点新知识。那就记录下上周六写爬虫吧，上周六闲来无事，写了个小爬虫。在写这个爬虫的时候也是遇到了很多问题值得我记录下。首先，关于一个请求返回的数据，有些异步请求返回的是json数据，就像腾讯视频的视频评论就是json数据，这里就需要用到json解析数据才能作为Python的字典使用了，开始我一直想用正则表达式把这个jaon提取出来，因为在json数据之前还包含了一串没用的字符串导致这个数据不能直接使用json.loads()解析，正则表达式试了半天，就是无法剔除那个无关信息，还包括括号，正则表达式对于现在的我来说确实有点困难。不得已向前辈请教，他一语道破天机“为什么不试试字符串切片呢？”对啊，这个问题用个简单的切片就完事了，我还在苦苦寻求正则表达式，这也启示了我解决问题的方式。明明有简单的方法，怎么没想到呢；下次再遇到类似的问题先想想除了第一个想到的解决方法之外还有没有其他的方法。</p><p><img src="https://i.loli.net/2018/10/22/5bcdb895eda91.png" alt=""> </p><p>还有就是我在把数据写进数据库的时候，提示错误，key不能包含 . 我又无奈了，还是去请教，大佬说replace可以吧 . 替换成其他字符，我一想replace不是字符串的方法吗，于是马上提出疑问，结果大佬一句话让我哑口无言，key不就是字符串吗？</p><p><img src="https://i.loli.net/2018/10/22/5bcdd0a27da43.png" alt=""> </p><p>是啊，我咋又没想到呢？= =真的是醉了。希望自己吃一堑，长一智，能够记住这些问题吧。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://cdn.pixabay.com/photo/2017/03/12/14/48/terminalia-catappa-2137221__340.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;随便写写&quot;&gt;&lt;a href=&quot;#随便写写&quot; c
      
    
    </summary>
    
      <category term="日志" scheme="http://yoursite.com/categories/%E6%97%A5%E5%BF%97/"/>
    
    
      <category term="学习" scheme="http://yoursite.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生活" scheme="http://yoursite.com/tags/%E7%94%9F%E6%B4%BB/"/>
    
  </entry>
  
  <entry>
    <title>使用Seleniun获取A_JAX数据</title>
    <link href="http://yoursite.com/2018/10/19/a-jax%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E6%96%B9%E5%BC%8F/"/>
    <id>http://yoursite.com/2018/10/19/a-jax获取数据的另一种方式/</id>
    <published>2018-10-19T11:36:13.000Z</published>
    <updated>2018-10-24T06:45:19.103Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2018/10/19/5bc9c3e1b7ebf.png" alt=""> </p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>有一些网站在发起AJAX请求的时候，会带上特殊的字符串用于身份验证。这种字符串称为Token。比如说会带上时间戳，时间戳精确到毫秒。还会跟另一个属性存在某种一一对应的关系，虽然这种对应关系的算法会写在网站的某一个JavaScript文件中，但是要想读懂这种关系需要深厚的JavaScript功底。如果一个网站只需要爬一次，或者对爬取速度没有什么要求，那么可以通过另一种方式来解决这种问题。本文就来介绍这种方法是如何实现的!</p><h1 id="Selenium"><a href="#Selenium" class="headerlink" title="Selenium"></a>Selenium</h1><h2 id="Selenium介绍"><a href="#Selenium介绍" class="headerlink" title="Selenium介绍"></a>Selenium介绍</h2><p>虽然网页的源代码中无法看到被异步加载的内容，但是Chrome浏览器开发者工具的’Elements’选项卡下却可以看到网页上的内容。<br>这就说明Chrom开发者工具Elements选项卡里的HTML代码和网页源代码中的HTML代码是不一样的。在开发者工具中，此时显示的内容是已经加载完成的内容。如果能够获得这个被加载的内容，那么就能绕过手动构造Token的过程，可以直接使用XPath或者正则来获得想要的内容。<br>这种情况下，就需要使用Selenium操作浏览器来解析JavaScript，在爬取被解析以后的代码。Selenium是一个网页自动化测试工具，可以通过代码来操作网页上的各个元素。Selenium是Python中的第三方库，可以实现用Python来操作网页。</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>pip install selenium<br><a href="http://chromedriver.storage.googleapis.com/index.html" target="_blank" rel="noopener">下载ChromDriver</a>，根据自己的系统选择合适的版本：<br><img src="https://i.loli.net/2018/10/19/5bc9c940740eb.png" alt=""><br>下载下来的是一个zip文件，解压完成后得到一个exe文件接下来就是使用了</p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>将上面解压得到的chromedriver.exe与代码放在同一个文件夹中以方便代码直接调用。初始化Selenium，导入selenium库，在指定WebDriver<br><img src="https://i.loli.net/2018/10/19/5bc9ca65d9636.png" alt=""><br>如果chromedriver与代码不在同一个文件夹中可以通过绝对路径来指定，需要注意的是在Windows中路径的分隔符’\’和Python中的转义字符’\’冲突，所以在指明绝对路径的时候可以在路径字符串左引号的左边加一个‘r’符号：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver = webdriver.Chrome(r&apos;D:\user\asd\chromedriver&apos;)</span><br></pre></td></tr></table></figure></p><p>这样Python就能正确处理反斜杠的问题。<br>初始化完成以后下面第7行的代码就是使用selenium打开网页啦。<br>代码运行以后会自动打开一个chrome浏览器窗口，并自动打开这个网址对应的页面。一旦被异步加载的内容出现在了这个窗口中，那么这是使用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">html = driver.page_source</span><br></pre></td></tr></table></figure></p><p>就能得到在开发者工具中出现的HTML代码。如下图所示：<br><img src="https://i.loli.net/2018/10/19/5bc9cce81316f.png" alt=""><br>上图中标明的第6行代码设置了一个5s的延迟，这是由于selenium并不会等待网页加载完成在执行后面的代码。它是向ChromeDriver发送了一个命令，让ChromeDriver打开某个网页。至于网页需要多久打开，多久才能完成异步加载，这些selenium并不管，所以才需要设置一个延迟等待异步加载完成之后再抓取网页信息。这样手动设置延迟的方式很浪费时间资源，并且如果在指定的延迟时间内网页还没有加载出来，那么就获取不到网页信息了。怎么让selenium智能化呢？请看以下代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line">from selenium.webdriver.support.ui import WebDriverWait</span><br><span class="line">from selenium.webdriver.common.by import By</span><br><span class="line">from selenium.webdriver.support import expected_conditions as EC</span><br><span class="line"></span><br><span class="line">driver = webdriver.Chrome(&apos;./chromedriver&apos;)</span><br><span class="line">driver.get(&apos;http://exercise.kingname.info/exercise_advanced_ajax.html&apos;)</span><br><span class="line">try:</span><br><span class="line">    WebDriverWait(driver, 30).until(EC.text_to_be_present_in_element((By.CLASS_NAME, &quot;content&quot;), &apos;通关&apos;))</span><br><span class="line">except:</span><br><span class="line">    print(&apos;网页加载太慢&apos;)</span><br><span class="line"></span><br><span class="line">element = driver.find_element_by_xpath(&apos;//div[@class=&quot;content&quot;]&apos;)</span><br><span class="line">print(element.text)</span><br></pre></td></tr></table></figure></p><p> WebDriverWait(driver, 30).until(EC.text_to_be_present_in_element((By.CLASS_NAME, “content”), ‘通关’))<br>WebDriverWait会阻塞程序的运行，30表示最多等待30s。在这30秒内，每0.5秒检查一次网页，直到expected_conditions（EC）期待条件，这个条件就是text_to_be_present_in_element((By.CLASS_NAME, “content”)，（注释：class为content的元素里面的文本中包含了通关两个子）出现。<br>所以这行代码的完整意思就是等待网页加载，直到class为countent的元素里面包含了”通关”两个汉字。<br>By除了指定class之外，还可以指定很多其他的属性，例如：  </p><ul><li>By.ID  </li><li>By.NAME</li></ul><p>当然，也可以使用XPth:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EC.present_of_element_located((By.XPATH,&apos;//div[@class=&quot;content&quot;]))</span><br></pre></td></tr></table></figure></p><p>需要注意的是：”present_of_element_located”的参数是一个元组，元组第0项为By.XX，第1项为具体内容，”text_to_be_present_in_element”的参数有两个第一个参数为一个元组，元组第0项为By.XX，第1项为具体内容；第二个参数为部分或全部文本，又或者是一段正则表达式。</p><h2 id="获取元素"><a href="#获取元素" class="headerlink" title="获取元素"></a>获取元素</h2><p>在网页中寻找需要的内容，可以使用XPath<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">element = driver.find_element_by_xpath(&apos;//div[@class=&quot;content&quot;]&apos;)</span><br><span class="line">print(element.text)</span><br></pre></td></tr></table></figure></p><p>driver.find_element_by_xpath如果有多个符合条件的返回第一个<br>driver.find_elements_by_xpath以列表形式返回所有的符合条件的element<br>推荐使用driver.find_elements_by_xpath因为driver.find_element_by_xpath返回的是一个Element对象，如果没有符合条件的元素，就会报错，而driver.find_elements_by_xpath返回的是一个Element对象列表，就算没有符合条件的元素也会返回一个空列表，不会报错。<br>因为driver.find_element_by_xpath返回的是一个Element对象,通过它的.text属性才能获取到文本信息，当使用driver.find_elements_by_xpath得到了一个Element对象列表时，可以通过for循环展开这个列表然后在通过.text属性来获取到文本信息。一定要注意，不能直接在XPath的末尾加上text()，会报错。<br><img src="https://cdn.pixabay.com/photo/2016/01/14/06/09/guitar-1139397_960_720.jpg" alt=""></p><hr><p>原创文章，转载注明出处</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2018/10/19/5bc9c3e1b7ebf.png&quot; alt=&quot;&quot;&gt; &lt;/p&gt;
&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h
      
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="学习" scheme="http://yoursite.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="a_jax" scheme="http://yoursite.com/tags/a-jax/"/>
    
  </entry>
  
  <entry>
    <title>test</title>
    <link href="http://yoursite.com/2018/10/19/test/"/>
    <id>http://yoursite.com/2018/10/19/test/</id>
    <published>2018-10-19T04:53:46.000Z</published>
    <updated>2018-10-19T04:58:19.901Z</updated>
    
    <content type="html"><![CDATA[<p>test</p>]]></content>
    
    <summary type="html">
    
      加密文章，输入密码查阅
    
    </summary>
    
      <category term="私密博客" scheme="http://yoursite.com/categories/%E7%A7%81%E5%AF%86%E5%8D%9A%E5%AE%A2/"/>
    
    
  </entry>
  
  <entry>
    <title>hexo文章加密</title>
    <link href="http://yoursite.com/2018/10/19/hexo%E6%96%87%E7%AB%A0%E5%8A%A0%E5%AF%86/"/>
    <id>http://yoursite.com/2018/10/19/hexo文章加密/</id>
    <published>2018-10-19T03:10:41.000Z</published>
    <updated>2018-10-19T13:06:43.247Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://zccblog.cn/images/pictures/nihong.jpg" alt=""></p><h1 id="hexo文章简单加密访问"><a href="#hexo文章简单加密访问" class="headerlink" title="hexo文章简单加密访问"></a>hexo文章简单加密访问</h1><p>方法来源于<a href="https://www.jianshu.com/p/a2330937de6c" target="_blank" rel="noopener">简书</a><br>这里只介绍怎么实现，具体原理访问上面链接</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>找到themes-&gt;next-&gt;layout-&gt;_partials-&gt;head.swig文件。<br>按道理是添加在任何地方都行，但是推荐加在所有的<meta>标签之后，个人建议，仅做参考。以下是原作者加的代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;script&gt;</span><br><span class="line">    (function()&#123;</span><br><span class="line">        if(&apos;&#123;&#123; page.password &#125;&#125;&apos;)&#123;</span><br><span class="line">            if (prompt(&apos;请输入文章密码&apos;) !== &apos;&#123;&#123; page.password &#125;&#125;&apos;)&#123;</span><br><span class="line">                alert(&apos;密码错误！&apos;);</span><br><span class="line">                history.back();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)();</span><br><span class="line">&lt;/script&gt;</span><br></pre></td></tr></table></figure></p><p>再写新博客的时候在顶部加上新属性如下图所示：<br><img src="https://i.loli.net/2018/10/19/5bc94d90d61d8.png" alt=""><br>description是描述<br>password后就是自定义的密码了<br>具体效果如下图所示：<br><img src="/2018/10/19/hexo文章加密/test.gif" alt="test.gif"></p><hr><p>原创文章，转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;http://zccblog.cn/images/pictures/nihong.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;hexo文章简单加密访问&quot;&gt;&lt;a href=&quot;#hexo文章简单加密访问&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
      <category term="HEXO" scheme="http://yoursite.com/categories/HEXO/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>python基础复习</title>
    <link href="http://yoursite.com/2018/10/19/python%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0/"/>
    <id>http://yoursite.com/2018/10/19/python基础复习/</id>
    <published>2018-10-19T02:41:07.000Z</published>
    <updated>2018-10-19T13:06:43.260Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基础复习"><a href="#基础复习" class="headerlink" title="基础复习"></a>基础复习</h2><h3 id="基础类型控制语句"><a href="#基础类型控制语句" class="headerlink" title="基础类型控制语句"></a>基础类型控制语句</h3><p>基础类型：  </p><ul><li>整数型  int</li><li>浮点数  float</li><li>字符串  str</li><li>布尔值  True、Flase</li><li>空值  None</li><li>变量  变量在计算机中不仅可以是数字，还可以是任意数据类型，变量在称剧中就是用一个变量名表示</li></ul><h3 id="字符串详解"><a href="#字符串详解" class="headerlink" title="字符串详解"></a>字符串详解</h3><p>===<br>转义字符:<br>因为一些特殊字符是python中的关键字或一些特殊的概念如换行。<br>所以以特殊字符 \ 开头，构造转义字符。<br>常见的转义字符：<br>\n  换行        \t  制表符<br>\’   单引号     \”  双引号<br>\   反斜杠</p><p> 遍历：<br>for i in ‘abc’:<br>    print(i)<br>‘a’,’b’,’c’</p><p>下标访问：<br>‘hello’[4]→o</p><p>搜索：<br>(了解)’字符串’.count(子字符串)  搜索子串出现次数<br>‘xyaxyaXY’.count(‘xy’)<br>→ 2<br>‘xyaxyaXY’.count(‘xy’, 2)<br>(了解)判断字符串是否以某个字母开头<br>→ 1<br>‘abcd’.startswith(‘a’)<br>→ True<br>‘abcd’.endswith(‘d’)<br>→ True</p><p>字符串.find(子串)  找到返回下标，未找到返回-1<br>‘axyaXY’.find(‘xy’)<br>→ 1<br>‘aaXY’.find(‘xy’)<br>→ -1<br>index()方法与find()类似，区别是未找到的时候报错。</p><p> 替换 ：<br>字符串.replace(老子串，新字符串)<br>‘aaXY’.replace(‘aa’, ‘bb’)<br>‘bbXY’<br>分隔：<br>(了解)partition把一个字符串切成几块并返回，包含子串。<br>‘xyaxyaXY’.partition(‘xy’)<br>(‘’, ‘xy’, ‘axyaXY’)</p><p>字符串.split(子串)，根据子串分成几部分并返回列表，不包含子串。<br>‘xyaxyaXY’.split(‘x’)<br>[‘’, ‘ya’, ‘yaXY’]</p><p>连接：<br>join()用一个字符串连接可迭代对象的各个项。<br>‘-‘.join([‘小明’, ‘hong’, ‘li’])<br>→ ‘小明-hong-li’</p><p> 删除：<br>字符串.strip(要删除的子串)<br>‘今天天气真好\n’.strip(‘\n’)<br>→ ‘今天天气真好’</p><p> 大小写转换：<br>‘aa AA’.lower()<br>→ ‘aa aa’<br>‘aa AA’.upper()<br>→ ‘AA AA’<br>‘hello world’.capitalize()<br>→ ‘Hello world<br>‘aa AA’.swapcase()<br>→ ‘AA aa’</p><p> isxxx判断:<br>判断是否字母<br>‘a’.isalpha()<br>→ True<br>判断是否空格<br>‘ ‘.isspace()<br>→ True<br>判断是否数字<br>‘1’.isdigit()<br>→ True<br>判断是否合法的变量名<br>‘a4’.isidentifier()<br>→ True</p><p> 填充：<br>对齐的时候会用到<br>‘’.center(填充后的字符串总长度，要填充的字符串)<br>‘abc’.center(5, ‘_’)<br>→  ‘<em>abc</em>‘<br>右侧填充<br>‘abc’.ljust(10, ‘_’)<br>→  ‘abc_______’<br>左侧填充<br>‘abc’.rjust(10, ‘_’)<br>→  ‘<strong>___</strong>abc’</p><p>（以上了解end）</p><p>判断变量类型</p><ul><li>type()函数 判断变量类型</li><li><p>isinstance(值， 类型)   如果值属于类型的话返回True</p><p>数据类型转换</p></li></ul><ul><li>int(x)                将x转换为一个整数</li><li>long(x)            将x转换为一个长整数</li><li>float(x ) 将x转换到一个浮点数</li><li>complex(real) 创建一个复数</li><li>str(x ) 将对象 x 转换为字符串</li><li>repr(x ) 将对象 x 转换为表达式字符串</li><li>eval(str ) 用来计算在字符串中的有效Python表达式,并返回一个对象</li><li>tuple(s ) 将序列 s 转换为一个元组</li><li>list(s ) 将序列 s 转换为一个列表</li><li>chr(x ) 将一个整数转换为一个字符</li><li>unichr(x ) 将一个整数转换为Unicode字符</li><li>ord(x ) 将一个字符转换为它的整数值</li><li>hex(x ) 将一个整数转换为一个十六进制字符串</li><li>oct(x ) 将一个整数转换为一个八进制字符串</li></ul><p>控制语句：<br>if else语句<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">if 条件a:</span><br><span class="line">执行1</span><br><span class="line">elif 条件b:</span><br><span class="line">执行2</span><br><span class="line">elif 条件c:</span><br><span class="line">执行3</span><br><span class="line">else：</span><br><span class="line">执行4</span><br></pre></td></tr></table></figure></p><h3 id="循环"><a href="#循环" class="headerlink" title="循环"></a>循环</h3><p>for循环：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">打印三次字符串</span><br><span class="line">for i in range(1,4):</span><br><span class="line">print(&apos;你好  我是你爸&apos;)</span><br></pre></td></tr></table></figure></p><p>while 循环<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">i = 1</span><br><span class="line">while i&lt;4:</span><br><span class="line">print(&apos;你好 我是你爸&apos;)</span><br><span class="line">i+=1</span><br><span class="line">#循环中也可以加入条件控制语句</span><br><span class="line"></span><br><span class="line">i = 0</span><br><span class="line">while i &lt;10:</span><br><span class="line">    i = i+1</span><br><span class="line">    if i %2 == 0:</span><br><span class="line">        continue#当i是偶数时跳过此次循环</span><br><span class="line">    print(i)</span><br><span class="line"></span><br><span class="line">#break语句：用来结束循环</span><br><span class="line">i = 0</span><br><span class="line">while i &lt;10:</span><br><span class="line">i = i+1</span><br><span class="line">print(i)</span><br><span class="line">if i == 6:</span><br><span class="line">break</span><br></pre></td></tr></table></figure></p><h2 id="字典列表元组"><a href="#字典列表元组" class="headerlink" title="字典列表元组"></a>字典列表元组</h2><h3 id="列表list："><a href="#列表list：" class="headerlink" title="列表list："></a>列表list：</h3><p>定义：<br>原来的单值变量无法满足业务需求，需求一个”容器“来装内容<br>列表存储一些列有序（有下标）数据。容器内可以保存整数、布尔、字符串、或其他容器<br>语法：<br>创建一个列表：list1 = [1,’a’]<br>还可以通过类 内置关键字创建 list1 = list()</p><p>添加项：</p><ol><li>append()方法，添加新项到列表的末尾<br>list.append(要添加的数据)</li><li>insert()方法，可以根据索引值插入制定数据。<br>list.insert(索引，要添加的数据)</li><li>for 循环 +append,批量添加项。<br>list1 = [1,2,3,4]<br>for i in range(5, 8):<br> list1.append(i)<br>print(list1)</li><li>两个列表拼接，用加号<br>list1 = [1,2,3]<br>list2 = [4,5,6]<br>list3 = list1 + list2</li></ol><p>删除项：</p><ol><li>pop()<br>list.pop(索引)，根据索引删除列表中的某一个元素，返回删除成功的元素。pop()不传索引参数的时候，默认删除列表最后一项。<br>pop()函数相当于append和insert函数的逆运算。</li><li>remove()<br>list.remove(想要删除的项目值)  根据项item的值value来删除</li><li>clear()清空列表<br>lisr.clear() 清空所有项目，返回空列表。</li><li>del删除列表对象，根据索引删除元素<br>list = [1,2,3,4]<br>del list[0]表示删除的是1</li></ol><p>访问查询：<br>通过索引（下标）<br>list[0]</p><p>修改：<br>修改基于索引的。访问列表下标，然后等号赋值。<br>list [索引] = 新值</p><p> 遍历：<br>列表是一个可迭代对象，通过for循环可以吧一个列表中的元素依次取出<br>如果判断一个对象是可迭代对象：通过collections模块的Itearable类型来判断。<br>from collections import Iterable<br>isinstance(‘abc’,Iterable)#判断字符串abc是否可以迭代<br>True#返回true代表可以迭代<br>list = [1,2,3,4]<br>for i in list:<br>    print(i)#循环遍历出列表所有元素。</p><p>切片：<br>列表[索引开始：索引结束：步进]<br>切割截取出来区间。<br>切片参数为负数是，截取方向向右为正，步进为正式才能截取。步进与截取方向相反时，结果为空<br>索引0可以省略<br>list[0:2] 等同于 list[:2]<br>反向reverse输出列表<br>list[::-1] &gt;&gt;&gt; [5, 4, 3, 2, 1]</p><p>列表生成式（不常用）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">list1 = []</span><br><span class="line">for i in range(1,5)</span><br><span class="line">list1.append(i)</span><br><span class="line">#同样是在列表中加入1,2,3,4也可以用列表生成式写：</span><br><span class="line">[i for i in range(1,5)]</span><br><span class="line">#列表生成式也可以加入条件判断</span><br><span class="line">[i for i in range(1,10) if i%2==0]</span><br><span class="line">输出偶数</span><br></pre></td></tr></table></figure></p><h3 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h3><p>字典也是一种容器，特点：键值对存储<br>没有索引，是无序的。靠键来增删改查</p><p>创建：</p><ol><li>内置类实例化创建<br>dict1 = dict()</li><li>大括号，内容空<br>dict1 = {}</li><li>创建时附初始值<br>dict1 = {‘name’: ‘小明’, ‘age’: 13, ‘sex’: ‘male’}</li></ol><p>查询：</p><ol><li>（常用）dict1[键key]<br>dict1 = {‘name’: ‘小明’, ‘age’: 13, ‘sex’: ‘male’}<br>dict1[‘name’] &gt;&gt; ‘小明’<br>如果键不存在的话，报 KeyError错误。<br>dict1[‘aaa’]  &gt;&gt;&gt; KeyError:aaa</li><li>get(键, 默认值)<br>跟dict[键]非常相似，只不过多了个默认值。当键不存在的时候会报None<br>dict1.get(‘name’) &gt;&gt;&gt; ‘小明’<br>dict1.get(‘aaa’, None) &gt;&gt;&gt; None</li></ol><p>添加：<br>dict[新建] = 新值</p><p>修改：<br>dict[键] = 新值</p><p>遍历：<br>先dict.items() 转换为 大列表套小列表项的形式。<br>再用 for 循环遍历出来。<br>for k,v in 字典.items():<br>    print(k, v)<br>dict.keys()吧字典中的key取出来<br>dict.values()把字典中的value取出来</p><p>删除：</p><ol><li>键访问，值设置为None, 键还在<br>字典[键] = None<br>dict1 = {‘name’: ‘小明’, ‘age’: 13, ‘sex’: ‘male’}<br>dict1[‘name’] = None<br>dict1[‘name’] &gt;&gt;&gt; None</li><li>（常用）字典.pop(键)  根据键删除，返回删除成功的值，会把键和值都删除<br>dict1 = {‘name’: ‘小明’, ‘age’: 13, ‘sex’: ‘male’}<br>dict1.pop(‘name’) &gt;&gt;&gt; ‘小明’<br>dict1 &gt;&gt;&gt; {‘age’: 13, ‘sex’: ‘male’}</li><li>del 字典[键]，  del关键字删除，没有返回值，会把键和值都删除</li><li>字典.clear()   删除所有内容<br>dict1.clear()<br>dict1 &lt;&lt;&lt; {}</li></ol><p>其他的常用方法：<br>dict.items()  返回一个列表，每一项是 键值对<br>dict.keys()    返回一个列表，每一项是字典里的 键<br>dict.values()  返回一个列表，每一项是字典里的 值<br>dict.<strong>contains</strong>(key)  键存在的话返回True，不存在返回False</p><h3 id="元组"><a href="#元组" class="headerlink" title="元组"></a>元组</h3><p>元组跟list非常相似，特点和区别是 “不可修改”。所以元组需要在创建的时候就指定数据。<br>语法是小括号括起来，逗号分隔每一项</p><p>创建:<br>tuple2 = tuple((10, 20, ‘张三’))<br>（常用）tuple1 = (10, 20, ‘张三’)<br>场景:<br>元组由于不可变，适合定义 常量、配置、不需要改变的值。<br>这样在复杂代码中就不用害怕因为bug误修改值。<br>例如定义 中国的所有省份，一个注册登录表单中的下拉框选项</p><p>查询:<br>有索引，通过下标访问<br>tuple[index]<br>tuple2 = (‘河南’, ‘云南’)<br>tuple2[0] &gt;&gt;&gt; ‘河南’</p><p>也支持切片:<br>tuple2[0:1] &gt;&gt;&gt; (‘河南’,)</p><h2 id="注意的一个地方元组有时只有一项的时候，后面仍有一个逗号。"><a href="#注意的一个地方元组有时只有一项的时候，后面仍有一个逗号。" class="headerlink" title="注意的一个地方元组有时只有一项的时候，后面仍有一个逗号。"></a>注意的一个地方元组有时只有一项的时候，后面仍有一个逗号。</h2><p>原创文章，转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;基础复习&quot;&gt;&lt;a href=&quot;#基础复习&quot; class=&quot;headerlink&quot; title=&quot;基础复习&quot;&gt;&lt;/a&gt;基础复习&lt;/h2&gt;&lt;h3 id=&quot;基础类型控制语句&quot;&gt;&lt;a href=&quot;#基础类型控制语句&quot; class=&quot;headerlink&quot; title=&quot;基
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="基础" scheme="http://yoursite.com/tags/%E5%9F%BA%E7%A1%80/"/>
    
      <category term="复习" scheme="http://yoursite.com/tags/%E5%A4%8D%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>分享一种在markdown中插入图片的新方法</title>
    <link href="http://yoursite.com/2018/10/18/%E5%88%86%E4%BA%AB%E4%B8%80%E7%A7%8D%E5%9C%A8markdown%E4%B8%AD%E6%8F%92%E5%85%A5%E5%9B%BE%E7%89%87%E7%9A%84%E6%96%B0%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2018/10/18/分享一种在markdown中插入图片的新方法/</id>
    <published>2018-10-18T07:14:27.000Z</published>
    <updated>2018-10-26T05:10:22.438Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>我在前面的hexo使用笔记中介绍过怎么插入图片，前面那种方法虽然能用，但还是有点麻烦，每次插入图片都要写一段markdown代码。每次创建新博客的时候都会新建一个md文件和一个文件夹，不仅麻烦而且还让博客文件夹变得不简洁。下面我介绍一种新的方法，既不用创建新的存放图片的文件夹，而且还自动生成markdown语法代码，再插入图片的时候直接ctrl+v就完事了，看起来是不是很方便呢？</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>下面进入正文，这种酷炫的新方法来自于<a href="https://github.com/kingname/MarkdownPicPicker/" target="_blank" rel="noopener">大佬的git链接</a><br>MarkdownPicPicker 是一个Markdown写作辅助工具。它能将剪贴板中的图片上传到网络图床中，并将markdown格式的图片链接(![]（&lt;图片地址&gt;))复制到剪贴板中。<br>项目的readme中指明了安装使用方法<br><a href="https://github.com/kingname/MarkdownPicPicker/releases/download/v1.0.0/MarkdownPicPicker_v1.0.0.zip" target="_blank" rel="noopener">下载链接</a><br>下载完成后并解压，解压完成后得到一个MarkdownPicPicker.exe可执行文件和一个pic文件夹。  </p><h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><p>截图之后，比如我使用qq的快捷键截图（截图后点完成，不用保存），然后运行一下MarkdownPicPicker.exe，在你的编辑器中按下ctrl+v,神奇的事情就发生了，你会直接得到一段markdown插入图片的代码。是不是比上次的方法方便多了。<br><img src="https://raw.githubusercontent.com/kingname/MarkdownPicPicker/master/screenshots/MarkdownPicPickerPrew.gif" alt=""><br>如果你只想复制链接，不想让他变成这种形式，那么，你可以在命令行中输入markdownpicpicker.exe -linkonly  </p><p>（当前目录下按住shift点击右键选择在此处打开命令行窗口）</p><h1 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h1><p>如果你觉得上面的方法还不够方便？是不是每次还要用鼠标来运行exe文件觉得很麻烦？<br>没关系，还有一种方法可以优化这些步骤。<br>官方文档说使用AutoHotKey来启动程序可以吧整个流程缩短到两秒钟  </p><h2 id="AutoHotkey"><a href="#AutoHotkey" class="headerlink" title="AutoHotkey"></a>AutoHotkey</h2><p>AutoHotKey是什么东西？我其实也是第一次听说这个什么软件，完全一脸懵比，上网上查了下才知道是干什么用的，在官网上下载之后然后运行出来发现是一份英文帮助文档，我也看不懂说的是什么。后面才知道这是个脚本程序，可以自定义快捷键来执行某个程序或某种命令。　　</p><p>##　怎么使用呢？<br>新建一个文本本件，名字随便起，然后把后缀名改为.ahk<br>然后编辑这个文件输入以下代码：　　<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#b::Run, D:\git第三方包和软件\MarkdownPicPicker_v1.0.0\markdownpicpicker.exe</span><br><span class="line">Return</span><br><span class="line">!b::Run, D:\git第三方包和软件\MarkdownPicPicker_v1.0.0\markdownpicpicker.exe -linkonly</span><br><span class="line">Return</span><br></pre></td></tr></table></figure></p><p>在鼠标右键点击run script即可运行。这里定义了两个快捷键，第一行代码表示按下键盘win+b键执行markdownpicpicker.exe第三行表示按下alt+b执行markdownpicpicker.exe -linkonly这和在命令行中的执行文件是一样的。关于autohotkey还有很多作用，请自行baidu,google。</p><hr><p>原创文章，转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;我在前面的hexo使用笔记中介绍过怎么插入图片，前面那种方法虽然能用，但还是有点麻烦，每次插入图片都要写一段markdown代码。每次创建新
      
    
    </summary>
    
      <category term="markdown" scheme="http://yoursite.com/categories/markdown/"/>
    
    
      <category term="markdown" scheme="http://yoursite.com/tags/markdown/"/>
    
      <category term="图片" scheme="http://yoursite.com/tags/%E5%9B%BE%E7%89%87/"/>
    
  </entry>
  
  <entry>
    <title>python与MongoDB</title>
    <link href="http://yoursite.com/2018/10/18/python%E4%B8%8EMongoDB-1/"/>
    <id>http://yoursite.com/2018/10/18/python与MongoDB-1/</id>
    <published>2018-10-18T02:41:57.000Z</published>
    <updated>2018-10-19T13:06:43.262Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MongoDB"><a href="#MongoDB" class="headerlink" title="MongoDB"></a>MongoDB</h1><h2 id="介绍："><a href="#介绍：" class="headerlink" title="介绍："></a>介绍：</h2><p>MongoDB是一款基于c++开发的开源文档数据库，数据在MongoDB中以Key-Value的形式存储，就像是python的字典一样。使用MongoDB的管理软件Robo3T可以实现数据库的可视化。  </p><h2 id="安装："><a href="#安装：" class="headerlink" title="安装："></a>安装：</h2><p>首先下载并解压MongoDB:<br>访问官网，从官网上下载：<a href="https://www.mongodb.com/download-center/v2/community" target="_blank" rel="noopener">官网下载</a><br><img src="/2018/10/18/python与MongoDB-1/1.png" alt="安装"><br>选择适合自己的版本和操作系统然后Download就可以了<br>解压后把Bin下的文件辅助到新的文件夹（在c盘或者d盘创建MongoDB文件夹）并创建存储数据库的文件夹Data和日志文件夹Log<br>并创建mongod.conf，操作完成后文件目录如下图所示：<br><img src="/2018/10/18/python与MongoDB-1/2.png" alt="目录"><br>接下来就是编辑mongod.conf了<br>使用notepad++或其他除了编辑器(除了记事本)打开mongod.conf然后将以下代码复制进去：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">systemLog:</span><br><span class="line">  destination: file</span><br><span class="line">  path: D:\MongoDB\Log\mongo.log</span><br><span class="line">  logAppend: true</span><br><span class="line">storage:</span><br><span class="line">  dbPath: D:\MongoDB\Data</span><br><span class="line">net:</span><br><span class="line">  bindIp: 127.0.0.1</span><br><span class="line">security:</span><br><span class="line">  authorization: disabled</span><br></pre></td></tr></table></figure></p><p>这里的path和dppath根据设置成自己的路径，比如我的Log和Data是放在D盘MongoDB文件夹下的。<br>然后在安装目录下启动命令行，输入：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mongod.exe --config mongod.conf</span><br></pre></td></tr></table></figure></p><p>即可启动Mongodb(ps:启动后不能关闭这个窗口哦，否则mongodb服务会被关闭，还有一种方法可以将mongodb设置为windows服务启动，这样就不用每次输入命令启动啦，不过这种方法我还没有研究怎么用，等我会使用了在更新。)  </p><h1 id="Robo-3T"><a href="#Robo-3T" class="headerlink" title="Robo 3T"></a>Robo 3T</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Robo 3T是一个快平台的MongoDB管理工具，可以在图形界面中查询或者修改MongoDB  </p><h2 id="下载和安装"><a href="#下载和安装" class="headerlink" title="下载和安装"></a>下载和安装</h2><p><a href="https://robomongo.org/download" target="_blank" rel="noopener">访问官网下载robo3t</a><br>选择Download robo3t<br>等待安装完成后打开  </p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>单机create链接(如下图),如果monggodb在本地计算机上面运行，只需要在Name这一栏填一个名字就可(也可以不填使用默认名字)，单击save即可<br><img src="/2018/10/18/python与MongoDB-1/r3.png" alt="create"><br><img src="/2018/10/18/python与MongoDB-1/r4.png" alt="create"><br>然后点击connect就可以连接MongoDB了<br><img src="/2018/10/18/python与MongoDB-1/r5.png" alt="create"><br>可以看到，数据在MongoDB中是按照库(Database)——集合——文档的层级关系来存储的。文档就像是python中的一个字典，集合相当于一个包含了很多字典的列表；库相当于一个大字典，大字典里面的每一个键值对都对应了一个集合，key为集合的名字，Value就是一个集合。  </p><h1 id="PyMongo"><a href="#PyMongo" class="headerlink" title="PyMongo"></a>PyMongo</h1><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pymongo</span><br></pre></td></tr></table></figure><h2 id="pymongo的使用"><a href="#pymongo的使用" class="headerlink" title="pymongo的使用"></a>pymongo的使用</h2><h3 id="使用pymongo初始化数据库连接"><a href="#使用pymongo初始化数据库连接" class="headerlink" title="使用pymongo初始化数据库连接"></a>使用pymongo初始化数据库连接</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from pymongo import MongoClient</span><br><span class="line">client = MongoClient()</span><br><span class="line">database = client[&apos;库名&apos;]#创建一个库</span><br><span class="line">collection = database[&apos;集合名&apos;]#创建一个集合</span><br><span class="line">#这里的库名和集合名除了可以是字符串还可以是一个变量，当库名或者集合名是一个变量的时候，可以通过循环来批量操作数据库，比如要创建多个集合的时候，可以吧集合名先保存到一个列表中，然后通过循环穿件多个集合。</span><br></pre></td></tr></table></figure><h3 id="插入数据"><a href="#插入数据" class="headerlink" title="插入数据"></a>插入数据</h3><p>插入操作用到的方法为insert(参数)参数就是python的字典。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from pymongo import MongoClient</span><br><span class="line">client = MongoClient()</span><br><span class="line">database = client[&apos;Chapter6&apos;]</span><br><span class="line">collection = database[&apos;spider&apos;]</span><br><span class="line">data = &#123;&apos;id&apos;: 2, &apos;name&apos;: &apos;张三&apos;, &apos;age&apos;: 20, &apos;salary&apos;: 9999&#125;</span><br><span class="line">collection.insert(data)#插入一条数据</span><br><span class="line">#当然也可以一次插入多条数据，先把多条数据保存到一个列表中，然后直接使用insert()即可</span><br><span class="line">more_data = [</span><br><span class="line">    &#123;&apos;id&apos;: 3, &apos;name&apos;: &apos;张四&apos;, &apos;age&apos;: 20, &apos;salary&apos;: 999&#125;,</span><br><span class="line">    &#123;&apos;id&apos;: 4, &apos;name&apos;: &apos;张五&apos;, &apos;age&apos;: 20, &apos;salary&apos;: 99&#125;,</span><br><span class="line">    &#123;&apos;id&apos;: 5, &apos;name&apos;: &apos;张六&apos;, &apos;age&apos;: 20, &apos;salary&apos;: 9&#125;,</span><br><span class="line">    &#123;&apos;id&apos;: 6, &apos;name&apos;: &apos;张七&apos;, &apos;age&apos;: 20, &apos;salary&apos;: 19999&#125;</span><br><span class="line"></span><br><span class="line">]</span><br><span class="line">collection.insert(more_data)</span><br></pre></td></tr></table></figure><h3 id="查找数据"><a href="#查找数据" class="headerlink" title="查找数据"></a>查找数据</h3><p>查找功能对应的方法是</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">find(查询条件，返回字段)#返回所有符合的信息</span><br><span class="line">find_one(查询条件，返回字段)#返回一条符合的信息</span><br></pre></td></tr></table></figure><p>在不写find方法参数的时候，表示获取指定集合中所有字段。<br>返回字段的参数指定返回内容。这个参数也是一个字典，key就是字段的名称，value是0或1,0表示不返回这个字段，1表示返回这个字段。其中_id这个字段比较特殊，必须人工指定它的值为0，这样才不会返回。而对于其他数据，应该统一使用返回，或者不返回。eg:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">collection.find(&#123;&apos;age&apos;:20&#125;,&#123;&apos;_id&apos;:0&#125;)#查询所有age为20的记录，返回除了_id的所有字段</span><br></pre></td></tr></table></figure><p>这里需要知道：find()方法返回的是一个pymongo对象，这个对象可以被for循环展开，展开之后可以得到很多个字典。<br>pymongo也支持逻辑查询：<br>它们对应的关键词如下所示：  </p><ul><li>$gt great than 大于</li><li>$lt less than 小于</li><li>$gte Greater than equal to 大于等于</li><li>$lte less than equal to 小于等于</li><li>$eq equal to 等于</li><li>$ne not equal to 不等于<br>eg:  </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">collection.find(&apos;age&apos;:&#123;&apos;$gt&apos;:19&#125;)#查询age&gt;19的记录</span><br><span class="line">collection.find(&apos;age&apos;:&#123;&apos;$gte&apos;:19,&apos;$lt&apos;:30&#125;)#查询19≤age&lt;30的记录</span><br></pre></td></tr></table></figure><h3 id="对查询结果进行排序"><a href="#对查询结果进行排序" class="headerlink" title="对查询结果进行排序"></a>对查询结果进行排序</h3><p>排序的方法为sort(),这个方法一般和find()配合使用<br>他有两个参数，第一个参数指明以那一项进行排序，第二个参数为1或者-1，1表示升序，-1表示降序。eg:  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">collection.find(&apos;age&apos;:&#123;&apos;$gt&apos;:19&#125;).sort(&apos;age&apos;,-1)#查询所有age大于19并以age按照降序进行排序。</span><br></pre></td></tr></table></figure><h3 id="修改"><a href="#修改" class="headerlink" title="修改"></a>修改</h3><p>修改也有两个方法： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">collection.updata_one(参数1，参数2)#修改一条</span><br><span class="line">collection.updata_many(参数1，参数2)#修改多条</span><br></pre></td></tr></table></figure><p>参数1和2都是字典形式具体使用如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">collection.upadta_many(&#123;&apos;name&apos;:&apos;张三&apos;&#125;,&#123;&apos;$set&apos;:&#123;&apos;age&apos;:30&#125;&#125;)#将姓名为张三的人年龄全部改为30</span><br></pre></td></tr></table></figure><h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><p>删除也有两个方法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">collection.delete_one(&apos;name&apos;:&apos;张三&apos;)#把第一个name是张三的记录删除</span><br><span class="line">collection.delete_many(&apos;name&apos;:&apos;张三&apos;)#把name是张三的记录全部删除</span><br></pre></td></tr></table></figure><p>删除方法只有一个参数，是字典形式。<br>至此,mongodb以及pymongo的使用暂时结束了！</p><hr><p>原创文章，转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;MongoDB&quot;&gt;&lt;a href=&quot;#MongoDB&quot; class=&quot;headerlink&quot; title=&quot;MongoDB&quot;&gt;&lt;/a&gt;MongoDB&lt;/h1&gt;&lt;h2 id=&quot;介绍：&quot;&gt;&lt;a href=&quot;#介绍：&quot; class=&quot;headerlink&quot; title=
      
    
    </summary>
    
      <category term="MongoDB" scheme="http://yoursite.com/categories/MongoDB/"/>
    
    
      <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="MongoDB" scheme="http://yoursite.com/tags/MongoDB/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>一键设置爬虫headers</title>
    <link href="http://yoursite.com/2018/10/18/%E4%B8%80%E9%94%AE%E8%AE%BE%E7%BD%AE%E7%88%AC%E8%99%ABheaders/"/>
    <id>http://yoursite.com/2018/10/18/一键设置爬虫headers/</id>
    <published>2018-10-18T01:34:38.000Z</published>
    <updated>2018-10-19T13:08:31.638Z</updated>
    
    <content type="html"><![CDATA[<p>在写爬虫的时候我们经常需要设置headers属性来让爬虫模拟浏览器从而获得数据。在添加headers属性的时候，需要把浏览器所有的headers属性都写上去：<br><img src="/2018/10/18/一键设置爬虫headers/1.png" alt="headers属性"><br>这么长的headers如果复制下来然后手动把它设置成字典的形式太麻烦、太费时间。那么有没有办法一下把这些属性转为字典形式呢？带着这个疑问我向大佬请教：<br><img src="/2018/10/18/一键设置爬虫headers/2.jpg" alt="kk"><br>大佬不愧是大佬，分分钟解决我的问题好吧。<br>下面介绍下这个方法：<br>安装（输入以下命令）：<br>pip install –upgrade git+<a href="https://github.com/kingname/CrawlerUtility.git" target="_blank" rel="noopener">https://github.com/kingname/CrawlerUtility.git</a><br>使用：<br><img src="/2018/10/18/一键设置爬虫headers/3.jpg" alt="使用"><br>首先引入这个包<br>然后把从浏览器复制下来的headers保存成长字符串。<br>在使用ChromeHeaders2Dict解析一下就完事了<br>是不是很简单！<br>推荐：<a href="https://github.com/kingname" target="_blank" rel="noopener">大佬的github</a></p><hr><p>后记：<br>我使用这个包的时候，是第一次使用别人手动写的第三方包，中间也遇到了很多问题。比如一开始我不知道怎么安装，然后在网上搜了一下怎么安装的资料。我这才知道安装第三方包原来有两种方式，我以前还以为只能手动下载安装呢。下面记录下两种安装方式：<br>一、手动安装</p><ol><li>在github上面下载包</li><li>然后解压该文件</li><li>在该文件夹按住shift+鼠标右键 在此处打开命令行窗口，然后输入python setup.py install</li></ol><p>二、自动安装<br>直接在命令行输入pip install 包<br>eg:pip install pillow<br>其实在每个github项目下都有README文件在这个文件里都会介绍怎么install，比如上面的这个项目：<br><img src="/2018/10/18/一键设置爬虫headers/3.png" alt="安装"><br>然后：<br>问题又来了，在上面这些东西都弄好了之后我在pycharm上面引入的时候pycharm报错，对于我这样一个萌新来说瞬间又懵了。于是我又向大佬请教…这才知道pycharm原来还有一个加载时间，果然一小会过后，这个包就能正常使用了。<br>不得不说，这个大佬人真的很好，身为这么大的一个大佬，对于我这个萌新的问题都很耐心的解答，再次表示感谢。<br>这个大佬就是我最近看的一本爬虫书的作者<br><img src="/2018/10/18/一键设置爬虫headers/4.jpg" alt="爬虫书"><br>技术过硬人又好！！！推荐去看看这本书，写的真不错，我这种死笨死笨的萌新都看的很明白！  </p><hr><p>原创文章，转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在写爬虫的时候我们经常需要设置headers属性来让爬虫模拟浏览器从而获得数据。在添加headers属性的时候，需要把浏览器所有的headers属性都写上去：&lt;br&gt;&lt;img src=&quot;/2018/10/18/一键设置爬虫headers/1.png&quot; alt=&quot;header
      
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="headers" scheme="http://yoursite.com/tags/headers/"/>
    
  </entry>
  
  <entry>
    <title>hexo使用笔记</title>
    <link href="http://yoursite.com/2018/10/17/hexo%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/10/17/hexo使用笔记/</id>
    <published>2018-10-17T13:21:46.000Z</published>
    <updated>2018-10-19T13:06:43.246Z</updated>
    
    <content type="html"><![CDATA[<h2 id="创建新文件"><a href="#创建新文件" class="headerlink" title="创建新文件"></a>创建新文件</h2><p>在D:/个人博客/BLOG/source/_posts文件夹下执行命令<br>hexo new ‘新文件名字’<br><img src="/2018/10/17/hexo使用笔记/hexo1.png" alt="新建文件">  </p><h2 id="在文本中添加图片"><a href="#在文本中添加图片" class="headerlink" title="在文本中添加图片"></a>在文本中添加图片</h2><ol><li>把主页配置文件_config.yml里的post_asset_folder:这个选项设置为true(如过以后不想生成同名的文件夹了改为false即可)</li><li>在hexo目录下执行mup install hexo_asset_image –save,这是下载安装上传本地图片的插件。</li><li>等待安装完成后，再运行上面创建新文件的命令来生成新md文件时，/source/_posts文件夹中除了xxx.md文件还有一个同名的文件夹。</li><li>最后在xxx.md中想引入图片时，先把图片复制到这个文件夹中，然后只需要在xxx.md中按照markdown的格式引入图片：<br><img src="/2018/10/17/hexo使用笔记/hexo2.png" alt="插入图片"><br>ps:!后面没有空格，hexo使用笔记前可加/也可以不加，图片名字一定不要写错。<h2 id="在页面中添加超链接"><a href="#在页面中添加超链接" class="headerlink" title="在页面中添加超链接"></a>在页面中添加超链接</h2><img src="/2018/10/17/hexo使用笔记/hexo3.png" alt="添加链接"><h2 id="关于文章推送的问题"><a href="#关于文章推送的问题" class="headerlink" title="关于文章推送的问题"></a>关于文章推送的问题</h2>之前由于不会弄，导致每次推送时都把git上面的CNAME文件弄丢了，每次推送完之后还要重新创建CNAME文件，这样很麻烦。通过查阅资料知道了把CNAME文件放在本地hexo目录下source的_posts文件夹下就可以解决这个问题了。<h2 id="关于云解析"><a href="#关于云解析" class="headerlink" title="关于云解析"></a>关于云解析</h2>首先要有一个域名，我用的是腾讯云域名：<br><img src="/2018/10/17/hexo使用笔记/云解析.png" alt="域名解析"><br>www和@主机记录的记录值是自己的githubpage的地址<br>然后本地文件中要有一个CNAME文件，这个文件只有一行：<br><img src="/2018/10/17/hexo使用笔记/cname.png" alt="CNAME"><br>这样就行了</li></ol><hr><p>原创文章，转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;创建新文件&quot;&gt;&lt;a href=&quot;#创建新文件&quot; class=&quot;headerlink&quot; title=&quot;创建新文件&quot;&gt;&lt;/a&gt;创建新文件&lt;/h2&gt;&lt;p&gt;在D:/个人博客/BLOG/source/_posts文件夹下执行命令&lt;br&gt;hexo new ‘新文件名字’&lt;br
      
    
    </summary>
    
      <category term="HEXO" scheme="http://yoursite.com/categories/HEXO/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
      <category term="笔记" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>HELL HEXO</title>
    <link href="http://yoursite.com/2018/10/17/HELL-HEXO/"/>
    <id>http://yoursite.com/2018/10/17/HELL-HEXO/</id>
    <published>2018-10-17T04:43:34.000Z</published>
    <updated>2018-10-17T09:36:37.566Z</updated>
    
    <content type="html"><![CDATA[<p>This is my BLOG</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This is my BLOG&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
      <category term="github" scheme="http://yoursite.com/tags/github/"/>
    
      <category term="npm" scheme="http://yoursite.com/tags/npm/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2018/10/17/hello-world/"/>
    <id>http://yoursite.com/2018/10/17/hello-world/</id>
    <published>2018-10-17T03:45:33.021Z</published>
    <updated>2018-10-17T09:33:25.859Z</updated>
    
    <content type="html"><![CDATA[<p>Hello,this is my blog,i’m Yu deqiang. nice to meet you!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Hello,this is my blog,i’m Yu deqiang. nice to meet you!&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
</feed>
